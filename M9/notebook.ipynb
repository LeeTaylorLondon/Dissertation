{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dissertation Set Expansion Paper\n",
        "\n",
        "This notebook contains my coded attempt at reproducing the Word2Vec method from the paper: \n",
        "\n",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999141."
      ],
      "metadata": {
        "id": "iVkM328T7hE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Setting Up Environment and Importing Dependencies"
      ],
      "metadata": {
        "id": "jlDm6TPq8JAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1 Download Files from my Dissertation Git Repo"
      ],
      "metadata": {
        "id": "_usreLNk7Nln"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekKu9dEF2TbZ",
        "outputId": "3eec1c03-59f4-4a72-f65a-a03b35c3042f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Dissertation'...\n",
            "remote: Enumerating objects: 2063, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 2063 (delta 2), reused 13 (delta 2), pack-reused 2048\u001b[K\n",
            "Receiving objects: 100% (2063/2063), 173.61 MiB | 10.84 MiB/s, done.\n",
            "Resolving deltas: 100% (287/287), done.\n",
            "Updating files: 100% (2690/2690), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/LeeTaylorNewcastle/Dissertation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2 Imports & Installations"
      ],
      "metadata": {
        "id": "oeiig8AU81D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4\n",
        "!pip install html5lib\n",
        "!pip install gensim\n",
        "!pip install scikit-learn\n",
        "!pip install tqdm\n",
        "!pip install gdown\n",
        "!pip install tabulate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI6_KzlZIoc0",
        "outputId": "dff83c1e-6f53-40ad-ccbd-1b6d9cb7808b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->bs4) (2.4)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1270 sha256=8a4ef46e94227370e60e5c9f8d218282e6cb7d0c48b8611429bcbc57482cda06\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/2b/cb/099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.9/dist-packages (1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.9/dist-packages (from html5lib) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from html5lib) (0.5.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.9/dist-packages (4.6.6)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown) (3.10.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown) (2.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (0.8.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from typing import List\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tabulate import tabulate\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "import requests\n",
        "import pickle\n",
        "import string\n",
        "import gensim\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import tqdm\n",
        "import shutil\n",
        "import gdown"
      ],
      "metadata": {
        "id": "vfQaiq5i8zaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.3 Mount Google Drive \n",
        "The purpose of this step is to be able to copy files generated by this notebook into a personal folder.  \n",
        "As files generated by this notebook are only available until the session ends. "
      ],
      "metadata": {
        "id": "hqbrFZlOUWpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Create path\n",
        "drive_folder_path = '/content/gdrive/MyDrive/_Mount_Dissertation'\n",
        "os.makedirs(drive_folder_path, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wgEFLeiUdaG",
        "outputId": "f9262ed5-dde9-44bf-fc44-f7739a7c8bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.4 Create folders"
      ],
      "metadata": {
        "id": "LUi0Hh_ZOyR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pkl_dir = 'pkl_files'\n",
        "wcs_dir = 'wcs'\n",
        "\n",
        "\n",
        "dirs_ = [pkl_dir, wcs_dir, 'labelled_', 'model_weights']\n",
        "\n",
        "\n",
        "for dir in dirs_:\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)"
      ],
      "metadata": {
        "id": "wOYj2CngOxNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Acquire and Pre-process Datasets"
      ],
      "metadata": {
        "id": "4gsKIkBe7mwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Web Crawling and Scraping Functions"
      ],
      "metadata": {
        "id": "2w0J_T47_bdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from six import with_metaclass\n",
        "INTERVAL = 3600\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "                  \"(KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\"\n",
        "}\n",
        "\n",
        "def url_to_soup_obj(url: str):\n",
        "    try:\n",
        "        page = requests.get(url, headers=HEADERS)\n",
        "    except:\n",
        "        # print(f\"<ERROR: {url}>\")\n",
        "        return\n",
        "    return bs(page.content, 'html5lib')\n",
        "\n",
        "def extract_links(soup):\n",
        "    links = []\n",
        "    keywords = [\n",
        "        'Help:', 'Main_Page', 'Talk:', 'Category:', \n",
        "        'Wikipedia:', 'Special:', 'Portal:', 'File:',\n",
        "        '#', 'Template:'\n",
        "    ]  \n",
        "\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        if link['href'].startswith('/wiki/') and \\\n",
        "        not any(keyword.lower() in link['href'].lower() for keyword in keywords):\n",
        "            links.append('https://en.wikipedia.org' + link['href'])\n",
        "\n",
        "    return links\n",
        "\n",
        "def extract_elements(html_text, elm='p'):\n",
        "    soup = bs(html_text, 'html.parser')\n",
        "    paragraphs = []\n",
        "    for p in soup.find_all(elm):\n",
        "        paragraphs.append(p.text.strip())\n",
        "    return paragraphs\n",
        "\n",
        "def google_search_wikipedia(search_str: str, debug: bool = True):\n",
        "    # Convert search to google search URL\n",
        "    gsearch_url = f\"https://www.google.com/search?q={'+'.join(search_str.lower().split())}\"\n",
        "    # Generate 'soup' object of google search\n",
        "    gsearch_soup = url_to_soup_obj(gsearch_url) # Todo: error\n",
        "    # Extract URLs\n",
        "    elms = extract_elements(str(gsearch_soup), elm='cite')\n",
        "    href = extract_elements(str(gsearch_soup), elm='a')\n",
        "    # Store 'cite' and 'a' elements in elms list\n",
        "    for item in href:\n",
        "        elms.append(item)\n",
        "    # For-loop extracts URLs into list\n",
        "    urls_ = []\n",
        "    for string in elms:\n",
        "        # Skip blank strings\n",
        "        if string == '':\n",
        "            continue\n",
        "        # String must contain 'https:' but not '...' and 'category'\n",
        "        if string.split()[0].__contains__(\"https:\") and \\\n",
        "                not string.split()[-1].__contains__('...') and \\\n",
        "                not string.lower().__contains__('category'):\n",
        "            # Convert arrows to slashes for URL functionality\n",
        "            string = string.replace(' ', '')\n",
        "            urls_.append(string.replace('›', '/'))\n",
        "    # Return list of URLs\n",
        "    return list(set(urls_))\n",
        "\n",
        "def read_words_from_file(file_path, \n",
        "                         encoding='utf-8-sig',\n",
        "                         rtype='set'):\n",
        "    with open(file_path, 'r', encoding=encoding) as f:\n",
        "        words = f.read().splitlines()\n",
        "        if rtype == 'set':\n",
        "            words = set(words)\n",
        "    return words\n",
        "\n",
        "def remove_code(text):\n",
        "    # Use a regular expression to find any instances of code and scripts\n",
        "    code = re.findall(r'<.*?>', text)\n",
        "    # Remove all instances of code and scripts from the text\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "    # Return the resulting text\n",
        "    return clean_text\n",
        "\n",
        "def extract_text(soup, headers=True, debug=True):\n",
        "    # Find all HTML elements that contain the main content\n",
        "    if headers:\n",
        "        content_elements = soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\",\n",
        "                                          \"h6\", \"a\", \"li\", \"span\", \"strong\", \"em\"])\n",
        "    elif not headers:\n",
        "        content_elements = soup.find_all([\"p\", \"a\", \"li\", \"span\", \"strong\", \"em\"])\n",
        "    # Concatenate the text from all content elements\n",
        "    content = [element.text.strip() for element in content_elements]\n",
        "    for i, v in enumerate(content):\n",
        "        content[i] = v.replace('\\n', '')\n",
        "        content[i] = content[i].replace('  ', '')\n",
        "    # Remove blanks and lines with sentences less than X words\n",
        "    content = [element for element in content if element.strip() != ''] \n",
        "    content = [element for element in content if len(element.split()) > 9]  \n",
        "    # Remove sentences based on the `keywords_to_filter` list \n",
        "    keywords_to_filter = ['site', 'cookie', 'sign in', 'instagram', 'contact us']\n",
        "    content = [element for element in content if not any(\n",
        "        keyword in element.lower() for keyword in keywords_to_filter\n",
        "    )]\n",
        "    # Combine content into a string\n",
        "    content = '\\n'.join(set(content))\n",
        "    content = remove_code(content)\n",
        "    # Return the resulting text\n",
        "    return content\n",
        "\n",
        "def query_to_text(search_str, page_depth_limit=3):\n",
        "    \"\"\" \n",
        "    Given a search query, this function retrieves the top \n",
        "    `page_depth_limit` Wikipedia pages from Google search results, \n",
        "    extracts their readable text, and returns a list\n",
        "    of the extracted text for each page.\n",
        "\n",
        "    Note: this function is called for every word in the set of\n",
        "    words describing personality traits `entity set` from MIT. \n",
        "\n",
        "    Args:\n",
        "        search_str (str): The search query for which to find Wikipedia pages.\n",
        "        page_depth_limit (int, optional): The maximum number of top Wikipedia pages\n",
        "            to extract text from. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of strings containing the extracted text from each\n",
        "            Wikipedia page.\n",
        "    \"\"\"\n",
        "    # Define storage for extracted text\n",
        "    rv = []\n",
        "    \n",
        "    # Extract URLs to scrape from\n",
        "    urls = google_search_wikipedia(search_str=search_str)\n",
        "    urls = urls[:3]\n",
        "    \n",
        "    # Set page limit \n",
        "    page_limit = 10\n",
        "    page_count = 0\n",
        "    \n",
        "    # Prevent checking the same site\n",
        "    explored = []\n",
        "\n",
        "    # Scrape extracted readable text\n",
        "    while len(urls) != 0 and page_count != page_limit:\n",
        "        \n",
        "        # Remove current link \n",
        "        url = urls.pop(0)\n",
        "\n",
        "        # Ignore duplicate websites\n",
        "        explored.append(url.lower())\n",
        "        if url in explored:\n",
        "            continue\n",
        "\n",
        "        # Get `soup` from URL string\n",
        "        soup_obj = url_to_soup_obj(url)\n",
        "        \n",
        "        # Prevent error \n",
        "        if soup_obj is None:\n",
        "            continue\n",
        "        \n",
        "        # Extract Wikipedia links from the page\n",
        "        # `set` to `list` type casting to remove duplicates\n",
        "        wiki_links = list(set(extract_links(soup_obj)))\n",
        "\n",
        "        # Explore wikipedia links from the page\n",
        "        for url_str in wiki_links[:10]:\n",
        "            urls.append(url_str)\n",
        "        \n",
        "        # Add text from website to `rv` (return value) list\n",
        "        rv.append(extract_text(soup_obj))\n",
        "\n",
        "        # Increase page_count to terminate the while loop\n",
        "        page_count += 1\n",
        "        \n",
        "    # Mark EOF\n",
        "    return rv\n",
        "\n",
        "def write_to_file(fn, text):\n",
        "    with open(fn, \"w\", encoding='utf-8-sig') as f:\n",
        "        for elm in text:\n",
        "            f.write(str(elm) + '\\n')\n",
        "\n",
        "def copy_files(src_dir, dst_dir):\n",
        "    # Get a list of all files in the source directory\n",
        "    files = os.listdir(src_dir)\n",
        "\n",
        "    # Copy each file from the source directory to the destination directory\n",
        "    for file_name in files:\n",
        "        # Construct the full file path\n",
        "        src_file = os.path.join(src_dir, file_name)\n",
        "        dst_file = os.path.join(dst_dir, file_name)\n",
        "\n",
        "        # Copy the file\n",
        "        shutil.copy(src_file, dst_file)\n",
        "\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "def main(use_git_clone=False):\n",
        "    \"\"\" This function uses all functions to perform\n",
        "    webpage exploration and text scraping from the resulting \n",
        "    webpage. \"\"\"\n",
        "\n",
        "    # Read list of words to search\n",
        "    entity_set = read_words_from_file(\n",
        "      f'Dissertation/M6/data_prep/entity_set.txt'\n",
        "    )\n",
        "    \n",
        "    # Entity set info\n",
        "    print(\n",
        "        f\"Set of words describing personality traits info.\\n\"\n",
        "        f\"Number of words:  {len(entity_set)}\\n\"\n",
        "        f\"First five words: {list(entity_set)[:5]}\\n\"\n",
        "        f\"Source: http://ideonomy.mit.edu/essays/traits.html\\n\"\n",
        "    )\n",
        "\n",
        "    \"\"\" Parse webapage for text\n",
        "    `counter` counts number of pages downloaded and scraped\n",
        "    `e_counter` counts number of pages failed\n",
        "    `dir_` directory path to store web-scrapings \n",
        "    \"\"\"\n",
        "    counter, e_counter = 0, 0\n",
        "    dir_ = 'wcs'\n",
        "\n",
        "    # Make a dir. for parsed webpages text\n",
        "    if not os.path.exists(dir_):\n",
        "        os.makedirs(dir_)\n",
        "\n",
        "    # Save execution time\n",
        "    if use_git_clone:\n",
        "        copy_files('Dissertation/M8/wcs', 'wcs')\n",
        "\n",
        "    \"\"\" Google search wikipedia for each term from \n",
        "    the MIT entity set and scrape text from each \n",
        "    resulting page \n",
        "    \"\"\"\n",
        "    for word in tqdm.tqdm(list(entity_set)[:]):\n",
        "        # Increase counter (decrease later if failed)\n",
        "        counter += 1\n",
        "        # Form search term to input into Google\n",
        "        search_term = f'wikipedia {word}'\n",
        "        # Saves time if code has already been executed\n",
        "        if os.path.isfile(f\"{dir_}/{search_term}.txt\"):\n",
        "            continue\n",
        "        try:\n",
        "            parsed_webpage_text = query_to_text(search_term)\n",
        "            # Write text from webpage to .txt file inside above dir\n",
        "            write_to_file(f\"{dir_}/{search_term}.txt\", parsed_webpage_text)\n",
        "        except ValueError as e:\n",
        "            print(e)\n",
        "            counter -= 1\n",
        "            e_counter += 1\n",
        "            # print(f\"'{word}' could not be written to a file!\")\n",
        "    print(f\"\\nSuccessfully scraped {counter} terms. {e_counter} terms failed.\")\n",
        "\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "\n",
        "main(use_git_clone=True)"
      ],
      "metadata": {
        "id": "exHqNmAQ7KAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da5e55a0-dfd1-43f4-ed62-de556a4cf70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set of words describing personality traits info.\n",
            "Number of words:  637\n",
            "First five words: ['Stiff', 'Disrespectful', 'Amoral', 'Bland', 'Conciliatory']\n",
            "Source: http://ideonomy.mit.edu/essays/traits.html\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 637/637 [00:00<00:00, 68748.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully scraped 637 terms. 0 terms failed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this code is to collect and store information about personality traits from Wikipedia pages in a structured and organized manner. The information collected is used for further analysis and set expansion.\n",
        "\n",
        "The code starts by reading a list of words that describe personality traits from a file and printing information about the list. Then, for each word in the list, the program performs a Google search to find relevant Wikipedia pages, extracts the readable text from those pages, and writes the extracted text to a separate text file.\n",
        "\n",
        "This code performs web scraping by retrieving the text from Wikipedia pages relevant to words that describe personality traits. This code uses the Google search engine to find Wikipedia pages that match the given words, retrieves the HTML content of those pages, and extracts the readable text from the HTML content. The extracted text is then written to a text file."
      ],
      "metadata": {
        "id": "ROcXGENWL-i-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Create Test Data From Thesaurus.com"
      ],
      "metadata": {
        "id": "qxRw4ik5ghUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_pickle_file(obj, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "def extract_elements(html_text, elm='p'):\n",
        "    \"\"\"\n",
        "    Given HTML text and an HTML element tag, return a list of all text elements within the specified tag.\n",
        "    :param html_text: str, the HTML text\n",
        "    :param elm: str, the HTML element tag to search for (default 'p')\n",
        "    :return: list of str, the text within the specified HTML element tag\n",
        "    \"\"\"\n",
        "    # Use BeautifulSoup to parse the HTML text and find all elements with the specified tag\n",
        "    soup = bs(html_text, 'html.parser')\n",
        "    elements = soup.find_all(elm)\n",
        "    # Extract the text from each element and add it to a list\n",
        "    text_list = [element.text.strip() for element in elements]\n",
        "    # Return the list of text elements\n",
        "    return text_list\n",
        "\n",
        "def extract_word_grid(soup):\n",
        "    \"\"\"\n",
        "    Given a BeautifulSoup object of a webpage, find and return the list of words in the word grid.\n",
        "    :param soup: BeautifulSoup object, the parsed HTML of the webpage\n",
        "    :return: list of str, the words in the word grid\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Find the div with the 'word-grid-container' data-testid attribute\n",
        "        word_grid_div = soup.find('div', {'data-testid': 'word-grid-container'})\n",
        "        # Find all links within the div\n",
        "        syn_list = word_grid_div.find_all('a')\n",
        "        # Extract the text from each link and add it to a list\n",
        "        word_list = [link.text.strip() for link in syn_list]\n",
        "    except:\n",
        "        word_list = []\n",
        "\n",
        "    try:\n",
        "        # Find the div with the 'antonyms' id\n",
        "        antonyms_div = soup.find('div', {'id': 'antonyms'})\n",
        "        # Find all links within the div\n",
        "        ant_list = antonyms_div.find_all('a')\n",
        "        # Extract the text from each link and add it to a list\n",
        "        antonyms_list = [link.text.strip() for link in ant_list]\n",
        "    except:\n",
        "        antonyms_list = []\n",
        "\n",
        "    # Return the list of words in the word grid\n",
        "    return [word_list, antonyms_list]\n",
        "\n",
        "def main(use_git_clone=False):\n",
        "    # Save time\n",
        "    if use_git_clone:\n",
        "        test_dataset_dest = 'Dissertation/M8/related_words_matrix.pkl'\n",
        "        if os.path.exists(test_dataset_dest):\n",
        "            # Copy file from clone to working directory\n",
        "            shutil.copyfile(test_dataset_dest, \n",
        "                            f\"{pkl_dir}/related_words_matrix.pkl\")\n",
        "            print(\"Test Dataset successfully copied from cloned repo!\")\n",
        "            return\n",
        "\n",
        "    # Read list of words to search\n",
        "    entity_set = read_words_from_file(\n",
        "      f'Dissertation/M6/data_prep/entity_set.txt'\n",
        "    )\n",
        "\n",
        "    related_words_matrix = []\n",
        "\n",
        "    # Extract synonyms and antonyms\n",
        "    thesaurus_url = 'https://www.thesaurus.com/browse/'\n",
        "    for word in tqdm.tqdm(list(entity_set)[:]):\n",
        "        # Form and download page from URL\n",
        "        final_url = thesaurus_url + word.lower()\n",
        "        soup = url_to_soup_obj(final_url)\n",
        "        # Extract synonyms and antonyms\n",
        "        related_words = extract_word_grid(soup)\n",
        "        related_words.insert(0, [word.lower()])\n",
        "        related_words_matrix.append(related_words)\n",
        "        # # Check extracted values\n",
        "        # for arr in related_words:\n",
        "        #     print(arr)\n",
        "        # print()\n",
        "\n",
        "    # # Make a dir. for pickle files\n",
        "    # if not os.path.exists(dir_):\n",
        "    #     os.makedirs(dir_)\n",
        "\n",
        "    store_pickle_file(related_words_matrix, f'{pkl_dir}/related_words_matrix.pkl')\n",
        "\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "\n",
        "main(use_git_clone=False)"
      ],
      "metadata": {
        "id": "i9EJxxSogg6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a056fa-9691-4c29-e00f-3557ae1450d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 637/637 [03:22<00:00,  3.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is designed to extract synonyms and antonyms of a given set of words from the website Thesaurus.com. The script performs the following tasks:\n",
        "\n",
        "1. `store_pickle_file(obj, filename)`: This function stores an object (usually the extracted words) as a pickle file with the given filename. Pickle files are used for efficient storage and retrieval of Python objects.\n",
        "\n",
        "2. `extract_elements(html_text, elm='p')`: This function takes HTML text as input and returns a list of text elements within the specified HTML tag (default is 'p'). It uses BeautifulSoup to parse the HTML and extract the text from the specified elements.\n",
        "\n",
        "3. `extract_word_grid(soup)`: Given a BeautifulSoup object representing a parsed webpage, this function finds and returns a list of synonyms and antonyms in the word grid. It searches for specific div elements and extracts the text from the links within them.\n",
        "\n",
        "4. `main(use_git_clone=False)`: This is the main function that drives the script. It performs the following steps:  \n",
        "a. Reads a list of words (the entity set) from a file.  \n",
        "b. Initializes an empty list to store the related words matrix.  \n",
        "c. Loops through each word in the entity set, constructs the URL for the Thesaurus.com page of that word, and downloads the webpage.  \n",
        "d. Parses the webpage and extracts the synonyms and antonyms using the extract_word_grid() function.  \n",
        "e. Appends the extracted related words (synonyms and antonyms) to the related words matrix.  \n",
        "f. Stores the related words matrix as a pickle file for later use.  \n",
        "\n",
        "The purpose of this code is to build a dataset of synonyms and antonyms for a given set of words. "
      ],
      "metadata": {
        "id": "ZKEz1IgYg0wB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Converted Scraped Text into Corpus"
      ],
      "metadata": {
        "id": "VaHv7cvbMy5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pickle_file(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        obj = pickle.load(f)\n",
        "    return obj\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\" \n",
        "    Use a regular expression to match all \n",
        "    characters that are not letters or numbers.\n",
        "    \"\"\" \n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "def smart_replace(big_string_):\n",
        "    \"\"\" Replace fullstops not used to end a sentence. \"\"\"\n",
        "    # Type check\n",
        "    if not isinstance(big_string_, str):\n",
        "        raise TypeError(\"Param: 'big_string' should be a string!\")\n",
        "    # Convert big_string to a list of chars\n",
        "    chars = list(big_string_)\n",
        "    # Remove fullstops not used to end sentences\n",
        "    #   and remove newline chars\n",
        "    new_chars = []\n",
        "    for i, c in enumerate(chars):\n",
        "        # Fullstop used to end sentence\n",
        "        if c == '.' and chars[i+1] == ' ':\n",
        "            new_chars.append(c)\n",
        "        # Fullstop not used to end sentence\n",
        "        elif chars[i-1] != ' ' and c == '.' and chars[i+1] != ' ':\n",
        "            new_chars.append(',')\n",
        "        elif c == '\\n':\n",
        "            new_chars.append(' ')\n",
        "        else:\n",
        "            new_chars.append(c)\n",
        "    new_big_string_ = ''.join(new_chars)\n",
        "    return new_big_string_\n",
        "\n",
        "def extract_sentences(big_string_):\n",
        "    \"\"\" Removing casing and seperate into sentences on fullstop chars. \"\"\"\n",
        "    rv = []\n",
        "    sentences_ = big_string_.split('.')\n",
        "    for sentence in sentences_:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = remove_punctuation(sentence)\n",
        "        if len(sentence.split()) >= 10:\n",
        "            rv.append(sentence.split())\n",
        "    return rv\n",
        "\n",
        "def find_filenames(directory: str):\n",
        "    filenames = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            filenames.append(os.path.join(root, file))\n",
        "    return filenames\n",
        "\n",
        "def main():\n",
        "    # Create list of file names\n",
        "    filenames = find_filenames(\"wcs\")\n",
        "    # Extract sentences from .txt files\n",
        "    all_sentences = []\n",
        "    for fn in filenames:\n",
        "        words = read_words_from_file(fn, rtype='list')\n",
        "        sentences = extract_sentences(' '.join(words))\n",
        "        all_sentences.extend(sentences)\n",
        "    # Create dir for corpus.pkl file\n",
        "    dir_ = 'wcs_pkl'\n",
        "    if not os.path.exists(dir_):\n",
        "        os.makedirs(dir_)\n",
        "    store_pickle_file(all_sentences, f\"{dir_}/corpus.pkl\")\n",
        "    print(\n",
        "        f\"Successfully combined wikipedia data into a corpus stored at:\\n\"\n",
        "        f\"{dir_}/corpus.pkl\")\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "ChdHXnNHM1nZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066354bf-d192-42c5-f915-ca6c729ac2a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully combined wikipedia data into a corpus stored at:\n",
            "wcs_pkl/corpus.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this code is to create a pickle file containing a list of lists, each list containing a sentence from webcrawling and scraping on wikipedia. The code uses two main functions: `main`.\n",
        "\n",
        "The main function uses the `find_filenames` function to search a directory named \"wcs\" and get a list of all the filenames. Then, it reads the contents of each file, removes punctuation, and splits the text into sentences. The resulting list of sentences is stored in a pickle file named \"corpus.pkl\".\n",
        "\n",
        "The other functions in the code are helper functions used by the main functions to process the text data. The `store_pickle_file` and `load_pickle_file` functions are used to store and load the data from the pickle files. The `remove_punctuation` function removes all characters that are not letters or numbers from a given text. The `smart_replace` function replaces full stops not used to end a sentence with a comma. The `extract_sentences` function removes casing and separates text into sentences on full stop characters."
      ],
      "metadata": {
        "id": "gBohhaZPM4sJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Reading XML Books into Python Object"
      ],
      "metadata": {
        "id": "Nm3FFKxDZu2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_as_pickle(obj, filename):\n",
        "    \"\"\"Stores a Python object as a pickle file.\"\"\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "def load_from_pickle(filename):\n",
        "    \"\"\"Loads a Python object from a pickle file.\"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        obj = pickle.load(f)\n",
        "    return obj\n",
        "\n",
        "def list_filenames(directory):\n",
        "    # Each file name in a passed directory is appended to \n",
        "    #  a list and returned by this function\n",
        "    filenames = []\n",
        "    for filename in os.listdir(directory):\n",
        "        filenames.append(f\"{directory}/{filename}\")\n",
        "    return filenames\n",
        "\n",
        "def print_list(arr, limit=None):\n",
        "    # Out number of items contained to user\n",
        "    print(f\"List contains {len(arr)} items.\\n[\")\n",
        "    # Out each item & it's item type i.e. int, str, etc.etc\n",
        "    for item in arr[:limit]:\n",
        "        print(f\"    {str(type(item)).split(' ')[1][1:-2]}: '{item}',\")\n",
        "    # Close content outed from list\n",
        "    print(\"]\")\n",
        "    # State number of items (if user specified) outed\n",
        "    if limit is not None:\n",
        "        print(f\"First {limit} items shown above this line.\\n\")\n",
        "\n",
        "def print_matrix(matrix, m_lim=None, v_lim=None):\n",
        "    # Call `print_list` for each list in the matrix\n",
        "    print(f\"\\nMatrix contains {len(matrix)} list(s).\")\n",
        "    print(\"---\\n<Begin Matrix>\")\n",
        "    for arr in matrix[:m_lim]:\n",
        "        print_list(arr, v_lim)\n",
        "    print(\"<End Matrix>\\n---\\n\")\n",
        "    print(f\"Printed {m_lim} list(s) out of {len(matrix)} list(s).\\n\")\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "def read_xml(fn):\n",
        "    # Load the XML file\n",
        "    tree = ET.parse(fn)\n",
        "    root = tree.getroot()\n",
        "    # Initialize the list\n",
        "    text_list = []\n",
        "    # Iterate through the 's' elements and add the text to the list\n",
        "    for s in root.iter('s'):\n",
        "        text_list.append(s.text)\n",
        "    # Return sentences\n",
        "    return text_list\n",
        "\n",
        "def read_all_books():\n",
        "    dir = \"Dissertation/M8/opus.nlpl.eu.books.php/books_xml\"\n",
        "    # Read the sentences from each XML book\n",
        "    sentences = []\n",
        "    for fn in list_filenames(dir):\n",
        "        sentences.extend(read_xml(fn))\n",
        "    return sentences\n",
        "\n",
        "def main():\n",
        "    # Showcase function list_filenames\n",
        "    print(f\"List containing book file paths info:\")\n",
        "    print_list(list_filenames(\n",
        "        \"Dissertation/M8/opus.nlpl.eu.books.php/books_xml\"\n",
        "        ), \n",
        "        10)\n",
        "\n",
        "    # Read sentences from all books\n",
        "    u_sentences = read_all_books()  # 'u_' = un-processed\n",
        "    print(f\"List containing sentences from all books info:\")\n",
        "    print_list(u_sentences, 10)\n",
        "\n",
        "    # Store sentences\n",
        "    store_as_pickle(u_sentences, 'u_sentences.pkl')\n",
        "\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "ICuzEu07P6ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85aa40c8-2339-412c-8007-9e641d8a6eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List containing book file paths info:\n",
            "List contains 42 items.\n",
            "[\n",
            "    str: 'Dissertation/M8/opus.nlpl.eu.books.php/books_xml/Tolstoy_Leo-Anna_Karenina_vol2.xml',\n",
            "    str: 'Dissertation/M8/opus.nlpl.eu.books.php/books_xml/Zola_Emile-Therese_Raquin.xml',\n",
            "    str: 'Dissertation/M8/opus.nlpl.eu.books.php/books_xml/Defoe_Daniel-Moll_Flanders.xml',\n",
            "    str: 'Dissertation/M8/opus.nlpl.eu.books.php/books_xml/Kafka_Franz-Prozess.xml',\n",
            "    str: 'Dissertation/M8/opus.nlpl.eu.books.php/books_xml/Zola_Emile-Germinal.xml',\n",
            "    str: 'Dissertation/M8/opus.nlpl.eu.books.php/books_xml/Kafka_Franz-Verwandlung.xml',\n",
            "    str: 'Dissertation/M8/opus.nlpl.eu.books.php/books_xml/Cervantes_Miguel-Don_Quijote.xml',\n",
            "    str: 'Dissertation/M8/opus.nlpl.eu.books.php/books_xml/Verne_Jules-Forceurs_de_blocus.xml',\n",
            "    str: 'Dissertation/M8/opus.nlpl.eu.books.php/books_xml/Doyle_Arthur_Conan-Great_Shadow.xml',\n",
            "    str: 'Dissertation/M8/opus.nlpl.eu.books.php/books_xml/Verne_Jules-Ile_mysterieuse.xml',\n",
            "]\n",
            "First 10 items shown above this line.\n",
            "\n",
            "List containing sentences from all books info:\n",
            "List contains 239555 items.\n",
            "[\n",
            "    str: 'Source: http://librosgratis.liblit.com/',\n",
            "    str: 'Anna Karenina',\n",
            "    str: 'Leo Tolstoy',\n",
            "    str: 'VOLUME TWO PART V',\n",
            "    str: 'CHAPTER I',\n",
            "    str: 'THE PRINCESS SHCHERBATSKAYA AT FIRST CONSIDERED it out of the question to have the wedding before Advent, to which there remained but five weeks, but could not help agreeing with Levin that to put it off until after the Fast might involve waiting too long, for Prince Shcherbatsky's old aunt was very ill and likely to die soon, and then the family would be in mourning and the wedding would have to be considerably deferred.',\n",
            "    str: 'Consequently, having decided to divide her daughter's trousseau into two parts, a lesser and a larger, the Princess eventually consented to have the wedding before Advent.',\n",
            "    str: 'She decided that she would have the smaller part of the trousseau got ready at once, and would send on the larger part later; and she was very cross with Levin because he could not give her a serious answer to her question whether he agreed with this arrangement or not.',\n",
            "    str: 'This plan would be all the more convenient because the young couple intended immediately after the wedding to go to the country, where the larger part of the trousseau would not be required.',\n",
            "    str: 'Levin continued in the same condition of delirium as before; imagining that he and his joy were the chief or only purpose of all existence, and that he need not now think or bother about anything, as other people would see to everything for him.',\n",
            "]\n",
            "First 10 items shown above this line.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Overview of output generated by `main()` \n",
        "\n",
        "* The first section of the output is the result of calling `print_list` on the output of `list_filenames`, which lists the file paths of all the book XML files in the specified directory. It first prints a message indicating that it's printing the list of file paths, and then uses `print_list` to output the information. The list contains 42 items (i.e., there are 42 book XML files in the directory), and the print_list function outputs the file paths as a list of strings, with the type of each item being str.\n",
        "* The next line shows that the first 10 items from the list of file paths were printed.\n",
        "* The second section of the output is the result of calling `print_list` on the output of `read_all_books`, which extracts all the sentences from the book XML files and concatenates them into a single list. It first prints a message indicating that it's printing the list of sentences, and then uses print_list to output the information. The list contains 239,555 items (i.e., there are 239,555 sentences in total across all the book XML files), and the `print_list` function outputs each sentence as a `string`, with the type of each item being `str`.\n",
        "* The next line shows that the first 10 items from the list of sentences were printed.\n",
        "\n",
        "### Overview of the functions:\n",
        "\n",
        "* `store_as_pickle()` takes two arguments: the Python object you want to store, and the filename you want to save the object to. The function opens the file in write binary mode, and then uses the pickle.dump function to dump the object to the file.\n",
        "* `load_from_pickle()` takes a single argument, the filename of the pickle file you want to load. The function opens the file in read binary mode, and then uses the pickle.load function to load the object from the file. Finally, the function returns the loaded object.\n",
        "* `list_filenames(directory)` takes a directory path as input and returns a list of all file names in that directory. Each file name is prepended with the input directory path to form a full file path.\n",
        "* `print_list(arr, limit=None)` takes a list (`arr`) as input and prints out information about each item in the list. It first outputs the total number of items in the list, and then iterates over the list and prints out the type of each item (e.g., `int`, `str`) and the item itself. If a `limit` argument is passed, only the first `limit` items in the list will be printed. At the end, if a `limit` argument is passed, it prints a message indicating how many items were printed.\n",
        "* `print_matrix(matrix, lim=None)` takes a matrix (a list of lists) as input and calls `print_list` on each sublist in the matrix. It doesn't return anything, but simply prints out the information for each sublist in the matrix.\n",
        "* `read_xml(fn)` takes a file name (presumably a full file path) as input, reads an XML file from that location using the `ElementTree` module, and extracts all the text contained within the `s` tags in the XML file. It returns a list of all the extracted sentences.\n",
        "* `read_all_books()` reads all the XML files in a specified directory (by calling `list_filenames` and filtering for XML files), and extracts all the sentences from each file by calling `read_xml` on each file. It returns a list of all the extracted sentences.\n",
        "* `main()` is the main function that runs when the script is executed. It first calls `list_filenames` to print out a list of all the book file paths. It then calls `read_all_books` to extract all the sentences from the books, and prints out information about the sentences using `print_list`. Finally, `store_as_pickle` is called to store all of the sentences .pkl file to later be loaded and used."
      ],
      "metadata": {
        "id": "P0z_MBwcYEE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Pre-processing XML Book 'Sentences' "
      ],
      "metadata": {
        "id": "H6bOEBL2YHrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    \"\"\" Removes all punctuation from the given string. \"\"\"\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "def preprocess_sentences(u_sentences_):\n",
        "    \"\"\"\n",
        "    Preprocesses a list of sentences by converting \n",
        "    all characters to lowercase and splitting each sentence\n",
        "    into a list of words where for each word it's punctuation\n",
        "    is removed.\n",
        "    \"\"\"\n",
        "    p_sentences = []\n",
        "    for i, v in enumerate(u_sentences_):\n",
        "        words = v.lower().split()\n",
        "        for i, word in enumerate(words):\n",
        "            words[i] = remove_punctuation(word)\n",
        "        p_sentences.append(words)\n",
        "    return p_sentences\n",
        "\n",
        "def main():\n",
        "    # Load unprocessed sentences\n",
        "    u_sentences = load_from_pickle('u_sentences.pkl')\n",
        "    print_list(u_sentences, 10)\n",
        "\n",
        "    # Process sentences\n",
        "    p_sentences = preprocess_sentences(u_sentences)\n",
        "    print_list(p_sentences, 10)\n",
        "    print_matrix(p_sentences, 2)\n",
        "\n",
        "    # Store sentences\n",
        "    store_as_pickle(p_sentences, 'p_sentences.pkl')\n",
        "    \n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "ZP_jmhRMYEhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c7f1f1-de60-47a1-d9cc-675e001552f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List contains 239555 items.\n",
            "[\n",
            "    str: 'Source: http://librosgratis.liblit.com/',\n",
            "    str: 'Anna Karenina',\n",
            "    str: 'Leo Tolstoy',\n",
            "    str: 'VOLUME TWO PART V',\n",
            "    str: 'CHAPTER I',\n",
            "    str: 'THE PRINCESS SHCHERBATSKAYA AT FIRST CONSIDERED it out of the question to have the wedding before Advent, to which there remained but five weeks, but could not help agreeing with Levin that to put it off until after the Fast might involve waiting too long, for Prince Shcherbatsky's old aunt was very ill and likely to die soon, and then the family would be in mourning and the wedding would have to be considerably deferred.',\n",
            "    str: 'Consequently, having decided to divide her daughter's trousseau into two parts, a lesser and a larger, the Princess eventually consented to have the wedding before Advent.',\n",
            "    str: 'She decided that she would have the smaller part of the trousseau got ready at once, and would send on the larger part later; and she was very cross with Levin because he could not give her a serious answer to her question whether he agreed with this arrangement or not.',\n",
            "    str: 'This plan would be all the more convenient because the young couple intended immediately after the wedding to go to the country, where the larger part of the trousseau would not be required.',\n",
            "    str: 'Levin continued in the same condition of delirium as before; imagining that he and his joy were the chief or only purpose of all existence, and that he need not now think or bother about anything, as other people would see to everything for him.',\n",
            "]\n",
            "First 10 items shown above this line.\n",
            "\n",
            "List contains 239555 items.\n",
            "[\n",
            "    list: '['source', 'httplibrosgratisliblitcom']',\n",
            "    list: '['anna', 'karenina']',\n",
            "    list: '['leo', 'tolstoy']',\n",
            "    list: '['volume', 'two', 'part', 'v']',\n",
            "    list: '['chapter', 'i']',\n",
            "    list: '['the', 'princess', 'shcherbatskaya', 'at', 'first', 'considered', 'it', 'out', 'of', 'the', 'question', 'to', 'have', 'the', 'wedding', 'before', 'advent', 'to', 'which', 'there', 'remained', 'but', 'five', 'weeks', 'but', 'could', 'not', 'help', 'agreeing', 'with', 'levin', 'that', 'to', 'put', 'it', 'off', 'until', 'after', 'the', 'fast', 'might', 'involve', 'waiting', 'too', 'long', 'for', 'prince', 'shcherbatskys', 'old', 'aunt', 'was', 'very', 'ill', 'and', 'likely', 'to', 'die', 'soon', 'and', 'then', 'the', 'family', 'would', 'be', 'in', 'mourning', 'and', 'the', 'wedding', 'would', 'have', 'to', 'be', 'considerably', 'deferred']',\n",
            "    list: '['consequently', 'having', 'decided', 'to', 'divide', 'her', 'daughters', 'trousseau', 'into', 'two', 'parts', 'a', 'lesser', 'and', 'a', 'larger', 'the', 'princess', 'eventually', 'consented', 'to', 'have', 'the', 'wedding', 'before', 'advent']',\n",
            "    list: '['she', 'decided', 'that', 'she', 'would', 'have', 'the', 'smaller', 'part', 'of', 'the', 'trousseau', 'got', 'ready', 'at', 'once', 'and', 'would', 'send', 'on', 'the', 'larger', 'part', 'later', 'and', 'she', 'was', 'very', 'cross', 'with', 'levin', 'because', 'he', 'could', 'not', 'give', 'her', 'a', 'serious', 'answer', 'to', 'her', 'question', 'whether', 'he', 'agreed', 'with', 'this', 'arrangement', 'or', 'not']',\n",
            "    list: '['this', 'plan', 'would', 'be', 'all', 'the', 'more', 'convenient', 'because', 'the', 'young', 'couple', 'intended', 'immediately', 'after', 'the', 'wedding', 'to', 'go', 'to', 'the', 'country', 'where', 'the', 'larger', 'part', 'of', 'the', 'trousseau', 'would', 'not', 'be', 'required']',\n",
            "    list: '['levin', 'continued', 'in', 'the', 'same', 'condition', 'of', 'delirium', 'as', 'before', 'imagining', 'that', 'he', 'and', 'his', 'joy', 'were', 'the', 'chief', 'or', 'only', 'purpose', 'of', 'all', 'existence', 'and', 'that', 'he', 'need', 'not', 'now', 'think', 'or', 'bother', 'about', 'anything', 'as', 'other', 'people', 'would', 'see', 'to', 'everything', 'for', 'him']',\n",
            "]\n",
            "First 10 items shown above this line.\n",
            "\n",
            "\n",
            "Matrix contains 239555 list(s).\n",
            "---\n",
            "<Begin Matrix>\n",
            "List contains 2 items.\n",
            "[\n",
            "    str: 'source',\n",
            "    str: 'httplibrosgratisliblitcom',\n",
            "]\n",
            "List contains 2 items.\n",
            "[\n",
            "    str: 'anna',\n",
            "    str: 'karenina',\n",
            "]\n",
            "<End Matrix>\n",
            "---\n",
            "\n",
            "Printed 2 list(s) out of 239555 list(s).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Overview of Pre-processing\n",
        "\n",
        "I have defined three functions and a main function that work together to preprocess a list of sentences, removing all punctuation and converting all characters to lowercase. Here's an explanation of each function:\n",
        "\n",
        "* `remove_punctuation(text)` takes a string (`text`) as input and removes all punctuation characters from it using the `string` module. It then returns the resulting string without punctuation.\n",
        "* `preprocess_sentences(u_sentences_)` takes a list of strings (`u_sentences_`) representing unprocessed sentences, and preprocesses each sentence by converting all characters to lowercase, splitting each sentence into a list of words, and removing all punctuation characters from each word. It then returns the resulting list of preprocessed sentences, where each sentence is represented as a list of words.\n",
        "* `main()` is the main function that runs when the script is executed. It first loads a list of unprocessed sentences from a pickle file using `load_from_pickle`, and prints information about the list using `print_list`. It then calls `preprocess_sentences` on the list of unprocessed sentences to preprocess them, and prints information about the preprocessed sentences using `print_list` and `print_matrix`. Finally, it stores the preprocessed sentences to a new pickle file using `store_as_pickle`.\n",
        "\n",
        "Overall, my code demonstrates preprocessing in natural language processing (NLP) by removing punctuation and converting all text to lowercase. It uses Python's built-in string module to remove punctuation, and defines a separate function (`preprocess_sentences`) to apply this preprocessing step to a list of sentences. The main function demonstrates how to use the `preprocess_sentences` function and store the preprocessed sentences to a file."
      ],
      "metadata": {
        "id": "QiV2lG59miJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 Combine Datasets"
      ],
      "metadata": {
        "id": "krNrB_U9bLTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_corpus(fns: List[str]=[\"wcs_pkl/corpus.pkl\", \"p_sentences.pkl\"],\n",
        "                   out: bool=True):\n",
        "    \"\"\" Given a list of .pkl files containing lists of lists which\n",
        "    store corpus' this function returns the matrix combination of all\n",
        "    .pkl files. \n",
        "    \"\"\"\n",
        "    # Load first corp and print to user\n",
        "    big_corpus = load_pickle_file(fns[0])\n",
        "    if out: \n",
        "        print_list(big_corpus, 3)\n",
        "    for i, fn in enumerate(fns[1:]):\n",
        "        # Load the current corpus pointed to\n",
        "        corpus_ = load_pickle_file(fn)\n",
        "        # Combine corpus'\n",
        "        big_corpus.extend(corpus_)\n",
        "        # Info for user for each corpus\n",
        "        if out: \n",
        "            print_list(corpus_, 3)\n",
        "    # Info for user for the big corpus\n",
        "    if out: \n",
        "        print_list(big_corpus, 3)\n",
        "    return big_corpus\n",
        "\n",
        "\n",
        "# Run function to demonstrate functionality for later use\n",
        "example_combination = combine_corpus()\n",
        "# Delete `example_combination` as it is never used\n",
        "# and for notebook memory optimization \n",
        "del example_combination "
      ],
      "metadata": {
        "id": "M2jAVwHnbN4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfcda027-0a04-438f-fb4e-83f5364f4507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List contains 815822 items.\n",
            "[\n",
            "    list: '['systematic', 'trading', 'also', 'known', 'as', 'mechanical', 'trading', 'is', 'a', 'way', 'of', 'defining', 'trade', 'goals', 'risk', 'controls', 'and', 'rules', 'that', 'can', 'make', 'investment', 'and', 'trading', 'decisions', 'in', 'a', 'methodical', 'way', 'systematic', 'bias', 'errors', 'that', 'are', 'not', 'determined', 'by', 'chance', 'but', 'are', 'introduced', 'by', 'an', 'inaccuracy', 'involving', 'either', 'the', 'observation', 'or', 'measurement', 'process', 'inherent', 'to', 'the', 'system', 'systematic', 'chaos', 'ninth', 'studio', 'album', 'by', 'american', 'progressive', 'metal', 'band', 'dream', 'theater', 'this', 'page', 'was', 'last', 'edited', 'on', '19', 'november', '2022', 'at', '2123', 'utc']',\n",
            "    list: '['paul', 'bostaph', 'left', 'the', 'band', 'in', '2004', 'and', 'former', 'drummer', 'shaun', 'bannon', 'rejoined', 'having', 'healed', 'from', 'his', 'injuries']',\n",
            "    list: '['he', 'was', 'not', 'in', 'the', 'group', 'for', 'long', 'however', 'as', 'systematic', 'officially', 'disbanded', 'following', 'a', 'concert', 'in', 'april', '2004']',\n",
            "]\n",
            "First 3 items shown above this line.\n",
            "\n",
            "List contains 239555 items.\n",
            "[\n",
            "    list: '['source', 'httplibrosgratisliblitcom']',\n",
            "    list: '['anna', 'karenina']',\n",
            "    list: '['leo', 'tolstoy']',\n",
            "]\n",
            "First 3 items shown above this line.\n",
            "\n",
            "List contains 1055377 items.\n",
            "[\n",
            "    list: '['systematic', 'trading', 'also', 'known', 'as', 'mechanical', 'trading', 'is', 'a', 'way', 'of', 'defining', 'trade', 'goals', 'risk', 'controls', 'and', 'rules', 'that', 'can', 'make', 'investment', 'and', 'trading', 'decisions', 'in', 'a', 'methodical', 'way', 'systematic', 'bias', 'errors', 'that', 'are', 'not', 'determined', 'by', 'chance', 'but', 'are', 'introduced', 'by', 'an', 'inaccuracy', 'involving', 'either', 'the', 'observation', 'or', 'measurement', 'process', 'inherent', 'to', 'the', 'system', 'systematic', 'chaos', 'ninth', 'studio', 'album', 'by', 'american', 'progressive', 'metal', 'band', 'dream', 'theater', 'this', 'page', 'was', 'last', 'edited', 'on', '19', 'november', '2022', 'at', '2123', 'utc']',\n",
            "    list: '['paul', 'bostaph', 'left', 'the', 'band', 'in', '2004', 'and', 'former', 'drummer', 'shaun', 'bannon', 'rejoined', 'having', 'healed', 'from', 'his', 'injuries']',\n",
            "    list: '['he', 'was', 'not', 'in', 'the', 'group', 'for', 'long', 'however', 'as', 'systematic', 'officially', 'disbanded', 'following', 'a', 'concert', 'in', 'april', '2004']',\n",
            "]\n",
            "First 3 items shown above this line.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, combine_corpus, is designed to combine multiple corpora stored in separate pickle files into a single, larger corpus. \n",
        "\n",
        "This function takes a list of pickle files, each containing a corpus, and returns a combined corpus formed by merging all the individual corpora. It also provides an option to print some lines from each corpus for the user's reference."
      ],
      "metadata": {
        "id": "4FhmvXuWbOQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Exploratory Data Analysis\n",
        "\n",
        "This includes basic corpus statistics, frequency analysis, and POS-tagging analysis. "
      ],
      "metadata": {
        "id": "Vi2cFzuaTp5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Basic Corpus Statistics"
      ],
      "metadata": {
        "id": "PlzKq6_svsUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_words(sentences):\n",
        "    \"\"\" Calculate the total number of words. \"\"\"\n",
        "    words = 0\n",
        "    for sentences in sentences:\n",
        "        words += len(sentences)\n",
        "    return words\n",
        "\n",
        "def calc_avg_sen_len(sentences):\n",
        "    \"\"\" Calculate the average length of a sentence. \"\"\"\n",
        "    return calc_words(sentences) / len(sentences)\n",
        "\n",
        "def calc_unique_words(sentences):\n",
        "    \"\"\" Calculate the number of unique words. \"\"\"\n",
        "    all_sentences = []\n",
        "    for sentence in sentences:\n",
        "        all_sentences.extend(sentence)\n",
        "    all_sentences_set = set(all_sentences)\n",
        "    return len(all_sentences_set)\n",
        "\n",
        "def main():\n",
        "    \"\"\" Provide basic statistics to the user. \"\"\"\n",
        "    # Load Processed sentences\n",
        "    # p_sentences = load_from_pickle('p_sentences.pkl')\n",
        "    p_sentences = load_from_pickle('wcs_pkl/corpus.pkl') + load_from_pickle('p_sentences.pkl')\n",
        "\n",
        "    total_words  = calc_words(p_sentences)\n",
        "    avg_length   = calc_avg_sen_len(p_sentences)\n",
        "    unique_words = calc_unique_words(p_sentences)\n",
        "    \n",
        "    print(\n",
        "        f\"Total Number of Sentences: {len(p_sentences)}\\n\"\n",
        "        f\"Average Sentence Length:   {avg_length}\\n\"\n",
        "        f\"Total Number of Words:     {total_words}\\n\"\n",
        "        f\"Number of Unique Words:    {unique_words}\\n\"\n",
        "    )\n",
        "\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "r0TWgoyevsGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08e8844-c296-4580-f440-ab5734258b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Sentences: 1055377\n",
            "Average Sentence Length:   21.492753774243706\n",
            "Total Number of Words:     22682958\n",
            "Number of Unique Words:    399115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines three functions and a main function that work together to calculate some basic statistics on a list of preprocessed sentences. Here's an explanation of each function:\n",
        "\n",
        "* `calc_words(sentences)` takes a list of sentences represented as lists of words (`sentences`) and calculates the total number of words across all sentences. It does this by iterating over each sentence and adding the length of each sentence (i.e., the number of words in the sentence) to a running total. It then returns the total number of words.\n",
        "* `calc_avg_sen_len(sentences)` takes a list of sentences represented as lists of words (`sentences`) and calculates the average length of a sentence. It does this by dividing the total number of words in all sentences by the number of sentences in the list (i.e., `len(sentences)`). It calls the `calc_words` function to calculate the total number of words.\n",
        "* `calc_unique_words(sentences)` takes a list of sentences represented as lists of words (`sentences`) and calculates the number of unique words across all sentences. It does this by creating a new list (`all_sentences`) that concatenates all sentences together into a single list of words, and then converting this list to a set (`all_sentences_set`) to remove duplicates. It then returns the length of the resulting set, which represents the number of unique words across all sentences.\n",
        "* `main()` is the main function that runs when the script is executed. It first loads a list of preprocessed sentences from a pickle file using `load_from_pickle`. It then calculates three basic statistics on the list of preprocessed sentences: the total number of sentences, the average sentence length (in words), and the number of unique words across all sentences. This information is outed to the user."
      ],
      "metadata": {
        "id": "rmMK_P8izE1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Frequency Analysis"
      ],
      "metadata": {
        "id": "YzC6xDqV1tTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matrix_to_dict(sentences):\n",
        "    \"\"\"\n",
        "    Converts a list of sentences represented as lists of words \n",
        "    into a dictionary that maps each unique word to its frequency\n",
        "    count in the sentences.\n",
        "    \"\"\"\n",
        "    single_words = {}\n",
        "    for s in sentences:\n",
        "        for word in s:\n",
        "            if word in single_words:\n",
        "                single_words.update({word: single_words[word] + 1})\n",
        "            else:\n",
        "                single_words.update({word: 1})\n",
        "    return single_words\n",
        "\n",
        "def common_words(dict_: dict, items: int=10):\n",
        "    # Optimization necessary to allow colab to sort dict\n",
        "    #  without deleting lots of entries colab cannot \n",
        "    #  perform `sorted` later on\n",
        "    for key in list(dict_.keys()):\n",
        "        if dict_.get(key) < 10:\n",
        "            dict_.pop(key)\n",
        "    # Sort dict\n",
        "    sorted_dict = dict(sorted(dict_.items(), \n",
        "                              key=lambda item: item[1], \n",
        "                              reverse=True))\n",
        "    # Out common words\n",
        "    print(\n",
        "        f\"Top {items} most common word(s).\\n\"\n",
        "        f\"{'|WORD|':<20} {'|COUNT|':>5}\"\n",
        "    )\n",
        "    for k, v in list(sorted_dict.items())[:items]:\n",
        "        k = f\"'{k}'\"\n",
        "        print(f\"{k:<20} {v:>5}\")\n",
        "    print()\n",
        "\n",
        "def matrix_to_dict_pairs(sentences):\n",
        "    \"\"\"\n",
        "    Converts a list of sentences represented as lists of words into a \n",
        "    dictionary that maps each unique pair of contiguous words to its \n",
        "    frequency count in the sentences.\n",
        "    \"\"\"\n",
        "    word_pairs = {}\n",
        "    for s in sentences:\n",
        "        for i in range(len(s) - 1):\n",
        "            pair = (s[i], s[i+1])\n",
        "            if pair in word_pairs:\n",
        "                word_pairs[pair] += 1\n",
        "            else:\n",
        "                word_pairs[pair] = 1\n",
        "    return word_pairs\n",
        "\n",
        "def common_word_pairs(dict_: dict, items: int=10):\n",
        "    \"\"\"\n",
        "    Prints the most common pairs of contiguous words in the given dictionary, \n",
        "    sorted in descending order of frequency count.\n",
        "    \"\"\"\n",
        "    # Remove entries with frequency count less than 10\n",
        "    # Notebook memory optimization\n",
        "    for pair in list(dict_.keys()):\n",
        "        if dict_.get(pair) < 10:\n",
        "            dict_.pop(pair)\n",
        "\n",
        "    # Sort the dictionary by frequency count in descending order\n",
        "    sorted_dict = dict(sorted(dict_.items(), \n",
        "                              key=lambda item: item[1], \n",
        "                              reverse=True))\n",
        "\n",
        "    # Print out the most common word pairs with their frequency counts\n",
        "    print(f\"Top {items} most common word pairs.\\n{'|UNIGRAM|':<40} {'|COUNT|':>5}\")\n",
        "    for pair, count in list(sorted_dict.items())[:items]:\n",
        "        pair_str = f\"'{pair[0]} {pair[1]}'\"\n",
        "        print(f\"{pair_str:<40} {count:>5}\")\n",
        "    print()\n",
        "\n",
        "def matrix_to_dict_triplets(sentences):\n",
        "    \"\"\"\n",
        "    Converts a list of sentences represented as lists of words into a \n",
        "    dictionary that maps each unique triplet of contiguous words to its \n",
        "    frequency count in the sentences.\n",
        "    \"\"\"\n",
        "    word_triplets = {}\n",
        "    for s in sentences:\n",
        "        for i in range(len(s) - 2):\n",
        "            triplet = (s[i], s[i+1], s[i+2])\n",
        "            if triplet in word_triplets:\n",
        "                word_triplets[triplet] += 1\n",
        "            else:\n",
        "                word_triplets[triplet] = 1\n",
        "    return word_triplets\n",
        "\n",
        "def common_word_triplets(dict_: dict, items: int=10):\n",
        "    \"\"\"\n",
        "    Prints the most common triplets of contiguous words in the given dictionary, \n",
        "    sorted in descending order of frequency count.\n",
        "    \"\"\"\n",
        "    # Remove entries with frequency count less than 10\n",
        "    # Notebook memory optimization\n",
        "    for triplet in list(dict_.keys()):\n",
        "        if dict_.get(triplet) < 10:\n",
        "            dict_.pop(triplet)\n",
        "\n",
        "    # Sort the dictionary by frequency count in descending order\n",
        "    sorted_dict = dict(sorted(dict_.items(), \n",
        "                              key=lambda item: item[1], \n",
        "                              reverse=True))\n",
        "\n",
        "    # Print out the most common word triplets with their frequency counts\n",
        "    print(f\"Top {items} most common word triplets.\\n{'|TRIGRAM|':<60} {'|COUNT|':>5}\")\n",
        "    for triplet, count in list(sorted_dict.items())[:items]:\n",
        "        triplet_str = f\"'{triplet[0]} {triplet[1]} {triplet[2]}'\"\n",
        "        print(f\"{triplet_str:<60} {count:>5}\")\n",
        "    print()\n",
        "\n",
        "def pos_analysis():\n",
        "    ...\n",
        "\n",
        "def main():\n",
        "    \"\"\" Provide frequency statistics to the user. \"\"\"\n",
        "    \n",
        "    # Load sentences matrix\n",
        "    p_sentences = load_from_pickle('wcs_pkl/corpus.pkl') + load_from_pickle('p_sentences.pkl')\n",
        "    \n",
        "    # Create dictionary of word counts {k=<word> : v=<count>}\n",
        "    d_sentences = matrix_to_dict(p_sentences)\n",
        "\n",
        "    # # This line proves the dictionary construction is correct\n",
        "    # print(len(list(d_sentences.keys())))\n",
        "\n",
        "    # Output top n common words (n=10 by default)\n",
        "    common_words(d_sentences)\n",
        "    del d_sentences  # Notebook memory optimization\n",
        "\n",
        "    # Create dict of pair-word counts {k=<w1, w2> : v=<count>}\n",
        "    d_pairs = matrix_to_dict_pairs(p_sentences)\n",
        "\n",
        "    # Output top n common pairs of words (n=10 by default)\n",
        "    common_word_pairs(d_pairs)\n",
        "    del d_pairs  # Notebook memory optimization\n",
        "\n",
        "    # Create dict of unigram counts {k=<w1, w2, w3> : v=<count>}\n",
        "    d_triplets = matrix_to_dict_triplets(p_sentences)\n",
        "    \n",
        "    # Output top n common pairs of words (n=10 by default)\n",
        "    common_word_triplets(d_triplets)\n",
        "    del d_triplets  # Notebook memory optimization\n",
        "\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "-FeiVVCW16HD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "6c5c9ac4-9041-4202-b938-db86b7f96c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7ac44c691584>\u001b[0m in \u001b[0;36m<cell line: 152>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-7ac44c691584>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# Create dictionary of word counts {k=<word> : v=<count>}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0md_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;31m# # This line proves the dictionary construction is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-7ac44c691584>\u001b[0m in \u001b[0;36mmatrix_to_dict\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msingle_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0msingle_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msingle_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0msingle_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code first loads a list of tokenized sentences (called p_sentences) from a pickle file. Then, the script performs several frequency analyses, such as finding the most common words, word pairs (bigrams), and word triplets (trigrams) in the corpus. The `main()` function organizes the execution of these tasks.\n",
        "\n",
        "Here's a brief explanation of each function:\n",
        "\n",
        "* `matrix_to_dict()`: Given a list of tokenized sentences, this function creates a dictionary that maps each unique word to its frequency count.\n",
        "\n",
        "* `common_words()`: Given a dictionary of word frequencies, this function prints the n most common words and their frequencies.\n",
        "\n",
        "* `matrix_to_dict_pairs()`: Given a list of tokenized sentences, this function creates a dictionary that maps each unique pair of contiguous words (bigrams) to their frequency count.\n",
        "\n",
        "* `common_word_pairs()`: Given a dictionary of bigram frequencies, this function prints the n most common bigrams and their frequencies.\n",
        "\n",
        "* `matrix_to_dict_triplets()`: Given a list of tokenized sentences, this function creates a dictionary that maps each unique triplet of contiguous words (trigrams) to their frequency count.\n",
        "\n",
        "* `common_word_triplets()`: Given a dictionary of trigram frequencies, this function prints the n most common trigrams and their frequencies.\n",
        "\n",
        "* `main()`: This function organizes the execution of the tasks mentioned above. It first loads the list of tokenized sentences from a pickle file, then performs the frequency analyses for single words, bigrams, and trigrams. \n"
      ],
      "metadata": {
        "id": "P72IQiAH16cI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 POS-Tagging Analysis\n"
      ],
      "metadata": {
        "id": "0vwWiW0iNIjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk downloads are seperate to prevent \n",
        "# downloading the same files every execution\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "HafnWIMT_3_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tagging(sentence):\n",
        "    \"\"\"\n",
        "    Given a list of words, returns a list of tuples with each tuple containing\n",
        "    a word and its corresponding POS tag.\n",
        "    \"\"\"\n",
        "    pos_tags = nltk.pos_tag(sentence)\n",
        "    _, tags = zip(*pos_tags)\n",
        "    tags = list(tags)\n",
        "    # print(tags)\n",
        "    return pos_tags, tags\n",
        "\n",
        "def pos_matrix(sentences):\n",
        "    \"\"\" Generate matrix of POS-Taggings. \"\"\"\n",
        "    pos_matrix = []\n",
        "    for sentence in sentences[:]:\n",
        "        pos_matrix.append(pos_tagging(sentence)[1])\n",
        "    return pos_matrix\n",
        "\n",
        "def pos_distribution(pos_dict):\n",
        "    total_count = sum(pos_dict.values())\n",
        "    pos_percentages = {}\n",
        "\n",
        "    for pos, count in pos_dict.items():\n",
        "        percentage = (count / total_count) * 100\n",
        "        pos_percentages[pos] = round(percentage, 2)\n",
        "\n",
        "    return pos_percentages\n",
        "\n",
        "def sort_by_value(dictionary, reverse=True):\n",
        "    return sorted(dictionary.items(), key=lambda x: x[1], reverse=reverse)\n",
        "\n",
        "def display_top_n(dictionary, n=10, suffix=''):\n",
        "    # \n",
        "    sorted_dict = sort_by_value(dictionary)\n",
        "    max_key_len = max(len(key) for key, _ in sorted_dict[:n])\n",
        "    # \n",
        "    for i, (key, value) in enumerate(sorted_dict[:n]):\n",
        "        print(f\"{i+1:>{len(str(n)) + 1}}. {key:<{max_key_len}}: {value}{suffix}\")\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "def main():\n",
        "    # try-except saves times if this cell has already\n",
        "    # been executed before in your current runtime session\n",
        "    try:\n",
        "        # Load tags\n",
        "        tags = load_from_pickle(\"tags_dict.pkl\")\n",
        "    except FileNotFoundError:\n",
        "        # Load sentences matrix & Perform POS\n",
        "        p_sentences = load_from_pickle('p_sentences.pkl')\n",
        "        tags = pos_matrix(p_sentences)\n",
        "        # Memory optimization for notebook\n",
        "        del p_sentences  \n",
        "        # Save as file as it is faster to load from file\n",
        "        store_as_pickle(tags, 'tags_dict.pkl')\n",
        "\n",
        "    # Convert POS-taggings to a dictionary for counts\n",
        "    pos_dict = matrix_to_dict(tags)\n",
        "    # Notebook memory optimization\n",
        "    del tags\n",
        "\n",
        "    # Calculate percentages\n",
        "    distribution = pos_distribution(pos_dict)\n",
        "\n",
        "    # Sort dictionaries\n",
        "    sort_by_value(pos_dict)\n",
        "    sort_by_value(distribution)\n",
        "\n",
        "    # print(tags)\n",
        "    print(\"\\nTop 10 POS Tags Counts:\")\n",
        "    display_top_n(pos_dict, 10)\n",
        "\n",
        "    # Percentage distributions\n",
        "    print(\"\\nTop 10 POS Tags Distribution:\")\n",
        "    display_top_n(distribution, 10, '%')\n",
        "\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "GWt6V5n5NPHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code consists of several functions that analyze the part-of-speech (POS) tags in a corpus of text. It performs POS tagging on a list of tokenized sentences, calculates the distribution of POS tags, and displays the top 10 POS tags by count and percentage.\n",
        "\n",
        "Here's a brief explanation of each function:\n",
        "\n",
        "* `pos_tagging()`: Given a list of words (a tokenized sentence), this function returns a list of tuples containing each word and its corresponding POS tag.\n",
        "\n",
        "* `pos_matrix()`: Given a list of tokenized sentences, this function generates a matrix of POS tags by applying the `pos_tagging()` function to each sentence.\n",
        "\n",
        "* `pos_distribution()`: Given a dictionary of POS tags and their counts, this function calculates the percentage distribution of each POS tag.\n",
        "\n",
        "* `sort_by_value()`: Given a dictionary, this function sorts the items by their values (either in ascending or descending order).\n",
        "\n",
        "* `display_top_n()`: Given a dictionary, this function prints the top n items (by value) with an optional suffix (e.g., '%' for percentages).\n",
        "\n",
        "* `main()`: This function organizes the execution of the tasks mentioned above. It first loads the list of POS tags from a pickle file and converts it into a dictionary of POS tag counts. Then, it calculates the percentage distribution of POS tags, sorts the dictionaries by value, and displays the top 10 POS tags by count and percentage.\n"
      ],
      "metadata": {
        "id": "exxhwjg9fCaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training and Expanding Word Sets with Word2Vec Models"
      ],
      "metadata": {
        "id": "UaF_eTQYLc_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Train W2V \n",
        "\n",
        "Different models are generated with by using different window sizes. "
      ],
      "metadata": {
        "id": "q_E3CoQLW4d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_w2v(fn, out=True):\n",
        "    if out:\n",
        "        print(f\"Loading Word2Vec model...\\n\")\n",
        "    rv = None\n",
        "    if fn.__contains__('.bin.gz'):\n",
        "        rv = KeyedVectors.load_word2vec_format(fn, binary=True)\n",
        "    elif fn.__contains__('.model'):\n",
        "        rv = Word2Vec.load(fn)\n",
        "    if out:\n",
        "        print(f\"Loaded Word2Vec model!\\n\")\n",
        "    return rv\n",
        "\n",
        "def train_model(corpus_, window_size):\n",
        "    # Train the word2vec model on the corpus\n",
        "    model_ = gensim.models.Word2Vec(corpus_, window=window_size)  # vector_size=300\n",
        "    return model_\n",
        "\n",
        "def save_model(model_, file_path):\n",
        "    \"\"\" \n",
        "    Save the weights of the trained model\n",
        "\n",
        "    :param model_: pass the model weights\n",
        "    :param file_path: string - location and name to save to \n",
        "    \"\"\"\n",
        "    model_.save(file_path)\n",
        "\n",
        "def train_save_model(corpus_, window_size:int=5):\n",
        "    # Train model\n",
        "    trained_model = train_model(corpus_, window_size=window_size)\n",
        "\n",
        "    # Make dir to save weights\n",
        "    if not os.path.exists(\"model_weights\"):\n",
        "        os.makedirs(\"model_weights\")\n",
        "    \n",
        "    # Save model weights\n",
        "    save_model(\n",
        "        trained_model, \n",
        "        f\"model_weights/trained_model_2c{window_size}w.model\"\n",
        "    )\n",
        "\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "def main(use_git_clone=False):\n",
        "    \"\"\" \n",
        "    Model weights from git-clone are incomplete due \n",
        "        to upload and download format.\n",
        "    \"\"\"\n",
        "    # # Save time\n",
        "    # if use_git_clone:\n",
        "    #     copy_files(\"Dissertation/M8/model_weights\", \"model_weights\")\n",
        "    #     print(\"Successfully copied model weights from cloned repo!\")\n",
        "    #     return\n",
        "\n",
        "    # Save time by downloading weights\n",
        "    if use_git_clone:\n",
        "        url = 'https://drive.google.com/drive/folders/1pZJZXgIcxQ7CrQeSxouFkqbaRQEnNi9X?usp=share_link'\n",
        "        output = 'model_weights'\n",
        "        gdown.download_folder(url, output=output, quiet=False)\n",
        "        return\n",
        "\n",
        "    # Use combined corpus\n",
        "    corpus = combine_corpus(out=False)\n",
        "\n",
        "    window_sizes = [2, 5, 8, 10, 12, 15, 20]\n",
        "\n",
        "    for ws in tqdm.tqdm(window_sizes):  \n",
        "        train_save_model(corpus, ws)\n",
        "\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "\n",
        "main(use_git_clone=True)"
      ],
      "metadata": {
        "id": "lZMZ2LUmW621",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "179737e8-06d7-4bd7-b570-626742aa87d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder list\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1947yGTAIOD6ps9fHaT3r8bX2W8gtzbFs trained_model_2c2w.model\n",
            "Processing file 1oXNj984-bavYKZoffu1xHFTpYUspGWHX trained_model_2c5w.model\n",
            "Processing file 1NPQBj7AQ4zYVf9nQP4JtC13W93y3hRi8 trained_model_2c8w.model\n",
            "Processing file 1RQgozYsM4fRZTrbTFe52D3A7bxWT-zJ4 trained_model_2c10w.model\n",
            "Processing file 1ObpOg1Ce4lFWUJAchq6DTUSi7OwJ2Yn4 trained_model_2c12w.model\n",
            "Processing file 1MxNHHRpvAnpst93FbxFJ_fphvEOsd7kW trained_model_2c15w.model\n",
            "Processing file 1n1dEwCRVDwFYLKExjDYwhdcGonk4y1xX trained_model_2c20w.model\n",
            "Building directory structure completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1947yGTAIOD6ps9fHaT3r8bX2W8gtzbFs\n",
            "To: /content/model_weights/trained_model_2c2w.model\n",
            "100%|██████████| 132M/132M [00:01<00:00, 128MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oXNj984-bavYKZoffu1xHFTpYUspGWHX\n",
            "To: /content/model_weights/trained_model_2c5w.model\n",
            "100%|██████████| 132M/132M [00:01<00:00, 115MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NPQBj7AQ4zYVf9nQP4JtC13W93y3hRi8\n",
            "To: /content/model_weights/trained_model_2c8w.model\n",
            "100%|██████████| 132M/132M [00:01<00:00, 89.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RQgozYsM4fRZTrbTFe52D3A7bxWT-zJ4\n",
            "To: /content/model_weights/trained_model_2c10w.model\n",
            "100%|██████████| 132M/132M [00:00<00:00, 173MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ObpOg1Ce4lFWUJAchq6DTUSi7OwJ2Yn4\n",
            "To: /content/model_weights/trained_model_2c12w.model\n",
            "100%|██████████| 132M/132M [00:00<00:00, 214MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MxNHHRpvAnpst93FbxFJ_fphvEOsd7kW\n",
            "To: /content/model_weights/trained_model_2c15w.model\n",
            "100%|██████████| 132M/132M [00:01<00:00, 109MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n1dEwCRVDwFYLKExjDYwhdcGonk4y1xX\n",
            "To: /content/model_weights/trained_model_2c20w.model\n",
            "100%|██████████| 132M/132M [00:01<00:00, 92.2MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is performing three main tasks:\n",
        "\n",
        "1. Loading a pre-trained Word2Vec model. The function `load_w2v` takes a file name as an argument and loads a Word2Vec model stored in that file. The model is either in binary format ('.bin.gz') or in text format ('.model').\n",
        "\n",
        "2. Training a new Word2Vec model. The `train_model` function takes a corpus as an argument and trains a new Word2Vec model on that corpus.\n",
        "\n",
        "3. Saving the weights of the trained model. The `save_model` function takes a model and a file path as arguments, and saves the weights of the model in the specified file.\n",
        "\n",
        "The main function ties all of these tasks together by first loading the corpus from a pickle file, then training a new Word2Vec model on the corpus, and finally saving the weights of the trained model."
      ],
      "metadata": {
        "id": "QcNSNLBIekls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Perform Set Expansion"
      ],
      "metadata": {
        "id": "uB3Dg8lHeqXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_info(arr):\n",
        "    \"\"\" Out information about the passed list. \"\"\"\n",
        "    if isinstance(arr, list):\n",
        "        print(f\"List contains {len(arr)} items.\")\n",
        "    elif isinstance(arr, set):\n",
        "        print(f\"Set contains {len(arr)} items.\")\n",
        "    print(f\"First 5 items:[\")\n",
        "    for item in list(arr)[:5]:\n",
        "        print(f\"'{item}',\")\n",
        "    print(f\"]\\n\")\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "def write_words_to_file(file_path, arr):\n",
        "    # Open and write to file path passed\n",
        "    with open(file_path, 'w', encoding='utf-8-sig') as f:\n",
        "        string = '\\n'.join(arr)\n",
        "        f.write(string)\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "def expand_set(model_fp='google_w2v_weights/GoogleNews-'\n",
        "                        'vectors-negative300.bin.gz',\n",
        "               entity_set_fp='Dissertation/M6/'\\\n",
        "                        'data_prep/entity_set.txt',\n",
        "               entity_set=None,\n",
        "               out=True,\n",
        "               output_fn='expansion_set_2',\n",
        "               k=3):\n",
        "    \"\"\"\n",
        "    This function loads a specified W2V model, reads and loads an entity set\n",
        "    from a pickle file and convert it to a list.\n",
        "    Then for each word in the entity set it appends the word and most similar\n",
        "    word returned from the W2V model.\n",
        "\n",
        "    :param entity_set: set - the set of words which to expand on.\n",
        "    :param model_fp: str - file path pointing to weights to be loaded.\n",
        "    :param entity_set_fp: (str||None) - file path pointing to set to be loaded.\n",
        "    :param out: bool - if true then the function will print info otherwise nothing\n",
        "    will be outted to the console.\n",
        "    :param output_fn: str - file name of the output_set_expansion to be stored and written to\n",
        "    :return: a list of pairs where each pair contains the original word and\n",
        "    the most similar word generated by the W2V model.\n",
        "    \"\"\"\n",
        "    # Load a pre-trained Word2Vec model\n",
        "    model = load_w2v(fn=model_fp, out=out)\n",
        "\n",
        "    # Set state representing model type\n",
        "    pre_trained = False\n",
        "    if model_fp.__contains__('.model'):\n",
        "        pre_trained = True\n",
        "\n",
        "    # Define the initial entity set\n",
        "    if entity_set_fp is not None:\n",
        "        entity_set = read_words_from_file(entity_set_fp)\n",
        "    if out:\n",
        "        print(f\"Entity set as a Python Set info: \")\n",
        "        list_info(entity_set)\n",
        "\n",
        "    # Convert set to array\n",
        "    entity_arr = list(entity_set)\n",
        "    if out:\n",
        "        print(f\"Entity set as a Python List info: \")\n",
        "        list_info(entity_arr)\n",
        "        \n",
        "    # Array to store expanded terms and counter for errors\n",
        "    expansion_arr = []\n",
        "    e_counter = 0\n",
        "\n",
        "    # Expand the entity set\n",
        "    for word in entity_set:\n",
        "        \n",
        "        # Get a list of similar words for the entity set target word\n",
        "        try:\n",
        "            if not pre_trained:\n",
        "                similar_words_list = model.most_similar(word.lower())\n",
        "            else:\n",
        "                similar_words_list = [word for word in model.wv.most_similar(word.lower())]\n",
        "        except KeyError:\n",
        "            if out:\n",
        "                e_counter += 1\n",
        "                # print(f\"'{word}' not present in W2V vocabulary.\")\n",
        "            continue\n",
        "        \n",
        "        # Start adding similar words\n",
        "        similar_words_added = 0\n",
        "        for i, v in enumerate(similar_words_list):\n",
        "            if similar_words_added == k:\n",
        "                continue\n",
        "            if word.lower() != v[0].lower():\n",
        "                # Add word to expanded set\n",
        "                expansion_arr.append(\n",
        "                    f\"{word}, {similar_words_list[i][0]}, {similar_words_list[i][1]}\"\n",
        "                )\n",
        "                similar_words_added += 1\n",
        "    \n",
        "    # Write expanded set to a text file\n",
        "    write_words_to_file(f\"{output_fn}\", expansion_arr)\n",
        "\n",
        "    # Out function information to user \n",
        "    if out:\n",
        "        print(f\"Expanded list info: \")\n",
        "        list_info(expansion_arr)\n",
        "    if out:\n",
        "        print(\n",
        "            f\"{e_counter} words were not present in the W2V-model's vocabulary.\\n\"\n",
        "            f\"Written expanded set to the following dir: '{output_fn}'\"\n",
        "        )\n",
        "\n",
        "    # Return \n",
        "    return expansion_arr\n",
        "\n",
        "def main():\n",
        "    # Check if the 'expanded_sets' directory exists, if not, create it\n",
        "    parent_dir = f'expanded_sets_'\n",
        "    if not os.path.exists(parent_dir):\n",
        "        os.makedirs(parent_dir)\n",
        "\n",
        "    # Iterate over each file in the 'model_weights' directory\n",
        "    for model_file in tqdm.tqdm(os.listdir('model_weights')[:]):\n",
        "        if model_file.endswith('.model'):\n",
        "            model_fp = os.path.join('model_weights', model_file)\n",
        "\n",
        "            for k in [3, 5, 10, 15]:\n",
        "                # Expand set of words & write to text file in 'expanded_sets' directory\n",
        "                parent_dir_ = os.path.join(parent_dir, str(k))\n",
        "                if not os.path.exists(parent_dir_):\n",
        "                    os.makedirs(parent_dir_)\n",
        "                expand_set(\n",
        "                    model_fp=model_fp,\n",
        "                    output_fn=os.path.join(parent_dir_, \n",
        "                        f\"set_{model_file.split('_')[-1].split('.')[0]}.txt\"\n",
        "                    ),\n",
        "                    out=False,\n",
        "                    k=k\n",
        "                )\n",
        "\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "4dpCt6Vder-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31f39351-3f31-4559-ccd8-a3a6c6068dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [03:13<00:00, 27.58s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this code is to expand a set of words (referred to as the entity set) by finding the most similar words for each word in the entity set using our trained Word2Vec model. \n",
        "\n",
        "The code first loads the Word2Vec model and reads in the entity set from a file. It then converts the set to a list and for each word in the entity set, it finds the most similar words using the pre-trained model. \n",
        "\n",
        "Finally, it writes the results to a file, which is a list of pairs where each pair contains the original word and the most similar word generated by the Word2Vec model."
      ],
      "metadata": {
        "id": "J5ghdtj9e727"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Evaluation"
      ],
      "metadata": {
        "id": "mi9sF0oJSJt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Expanded Set Manual Labelling \n",
        "\n",
        "In my GitHub repository files, which are cloned and downloaded into this project at the start of this notebook, I have manually labelled whether a word generated by W2V is a synonym, antonym, or related for the first 120 outputs. I have added a '1' to the end of the line if the word is a synonym, antonym, or related and a '0' otherwise.\n",
        "\n",
        "This labelling can be found in 'Dissertation/M6/output_set_expansion/labelled/eset_mit_wiki.txt'."
      ],
      "metadata": {
        "id": "hUoJxZvGA1o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/Dissertation/M6/output_set_expansion/labelled/eset_mit_wiki_.txt\n",
        "with open('Dissertation/M6/output_set_expansion/labelled/eset_mit_wiki_.txt',\n",
        "          \"r\") as f:\n",
        "          lines = f.readlines()\n",
        "\n",
        "for line in lines[:5]: \n",
        "    print(line.strip())"
      ],
      "metadata": {
        "id": "927UFwmnA0ai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919d8d29-cbd5-4079-fd0a-c21782ed115d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿Sociable, radians, 0\n",
            "Sociable, modulo, 0\n",
            "Sociable, triplelevel, 0\n",
            "Energetic, dopamine, 1\n",
            "Energetic, legumes, 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1b Expanded Set Automatic Labelling\n",
        "\n",
        "Automate labelling for evaluation"
      ],
      "metadata": {
        "id": "eyT-o3TY9jAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_str = f'{pkl_dir}/related_words_matrix.pkl'\n",
        "\n",
        "def check_test_dataset():\n",
        "    test_dataset = load_pickle_file(test_str)\n",
        "    for vec in test_dataset:\n",
        "        for arr in vec:\n",
        "            print(arr)\n",
        "        print()\n",
        "\n",
        "def convert_test_dataset():\n",
        "    test_dataset = load_pickle_file(test_str)\n",
        "    print(test_dataset)\n",
        "    test_dataset_dict = {}\n",
        "    for vec in test_dataset:\n",
        "        vec[1] = [s.lower().strip() for s in vec[1]]\n",
        "        vec[2] = [s.lower().strip() for s in vec[2]]\n",
        "        test_dataset_dict.update(\n",
        "            { vec[0][0].strip(): vec[1] + vec[2] }\n",
        "        )\n",
        "    store_pickle_file(test_dataset_dict, test_str)\n",
        "    return test_dataset_dict\n",
        "\n",
        "def write_words_to_file(file_path, arr):\n",
        "    # Open and write to file path passed\n",
        "    with open(file_path, 'w', encoding='utf-8-sig') as f:\n",
        "        string = '\\n'.join(arr)\n",
        "        f.write(string)\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "def debug_tds_dict():\n",
        "    # ...\n",
        "    test_dataset_dict = convert_test_dataset()\n",
        "    print('public' in test_dataset_dict['private'])\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "def clean_up():\n",
        "    \"\"\" Remove labelled files. \"\"\"\n",
        "    par_dir = 'expanded_sets_'\n",
        "    for foldername in os.listdir(par_dir):\n",
        "        for fn in os.listdir(os.path.join(par_dir, foldername)):\n",
        "            if fn.__contains__('lab_lab_') or fn.__contains__('lab_'):\n",
        "                os.remove(os.path.join(os.path.join(par_dir, foldername), fn)) \n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "def main(use_gdown=False):\n",
        "    \"\"\" \n",
        "    Read an expanded set from a file given a fp string. \n",
        "    For each line check if expanded_word is a synonym or antonym. \n",
        "    If yes then add a label 1, otherwise 0. \n",
        "\n",
        "    File contents examples: \n",
        "    set_expansion_output.txt = 'target_word, expanded_word\\n'\n",
        "    labelled_SEO.txt = 'target_word, expanded_word, label ∈ {0, 1}\\n'\n",
        "    \"\"\"\n",
        "    if not use_gdown:\n",
        "        # ...\n",
        "        test_dataset_dict = convert_test_dataset()\n",
        "    else:\n",
        "        # Save time by downloading weights\n",
        "        url = 'https://drive.google.com/drive/folders/1zYkKs5Yn3Wl_SjmLMBtGBcv83Dd2RI-u?usp=sharing'\n",
        "        output = f'{pkl_dir}'\n",
        "        gdown.download_folder(url, output=output, quiet=False)\n",
        "        test_dataset_dict = load_pickle_file(f\"{pkl_dir}/related_words_matrix.pkl\")\n",
        "    \n",
        "    # Read entity set\n",
        "    entity_set_fp = f'Dissertation/M6/data_prep/entity_set.txt'\n",
        "    entity_set = read_words_from_file(entity_set_fp)\n",
        "\n",
        "    dir_ = 'expanded_sets_'\n",
        "    errors = 0\n",
        "    for k in [3, 5, 10, 15]:\n",
        "        for fn in tqdm.tqdm(os.listdir(f'{dir_}/{k}')):\n",
        "            # Create file path \n",
        "            fp = dir_ + f'/{k}/' + fn\n",
        "            \n",
        "            # Create expanded set\n",
        "            expanded_set = read_words_from_file(fp)\n",
        "\n",
        "            # ...\n",
        "            labelled_lines = []\n",
        "            for line in list(expanded_set)[:]: # 1634:1635\n",
        "                # Extract words\n",
        "                try: \n",
        "                    tw, ew, sim = target_word, expanded_word, similarity = line.split(',')\n",
        "                except ValueError as e:\n",
        "                    # print(e)\n",
        "                    errors += 1\n",
        "                    continue\n",
        "                # Determine label\n",
        "                label = -1\n",
        "                try:\n",
        "                    if ew.lower().strip() in test_dataset_dict[tw.lower()]:\n",
        "                        label = 1\n",
        "                    else:\n",
        "                        label = 0\n",
        "                except KeyError as e:\n",
        "                    print(e)\n",
        "                # Update `labelled_lines`\n",
        "                labelled_lines.append(f'{tw}, {ew}, {label}, {sim}')\n",
        "\n",
        "            # Write labelled expanded set to text file\n",
        "            write_words_to_file(f'{dir_}/{k}/lab_{fn}', labelled_lines)\n",
        "    \n",
        "    print(f\"\\nNo. of errors: {errors}\")\n",
        "\n",
        "    # Mark EOF\n",
        "    pass\n",
        "\n",
        "\n",
        "# clean_up()\n",
        "main(use_gdown=True)\n",
        "# debug_tds_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_g7Ry1D9okT",
        "outputId": "f9cccd5a-b593-4e18-e729-8248bd4c57a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder list\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1pGADoR2ad0VgNHX7DVjX_o2BgyL7F9Ib related_words_matrix.pkl\n",
            "Building directory structure completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pGADoR2ad0VgNHX7DVjX_o2BgyL7F9Ib\n",
            "To: /content/pkl_files/related_words_matrix.pkl\n",
            "100%|██████████| 263k/263k [00:00<00:00, 54.3MB/s]\n",
            "Download completed\n",
            "100%|██████████| 7/7 [00:00<00:00, 247.68it/s]\n",
            "100%|██████████| 7/7 [00:00<00:00, 175.04it/s]\n",
            "100%|██████████| 7/7 [00:00<00:00, 80.56it/s]\n",
            "100%|██████████| 7/7 [00:00<00:00, 81.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "No. of errors: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is designed to process a dataset of related words, clean it, and then label expanded sets based on whether a given word is a synonym or an antonym. The script consists of several functions that perform specific tasks, which are combined in the main function to complete the overall workflow.\n",
        "\n",
        "1. `check_test_dataset()`: This function loads the test dataset and prints its contents.\n",
        "\n",
        "2. `convert_test_dataset()`: This function converts the test dataset by changing the words to lowercase, stripping any extra spaces, and updating the dataset dictionary with a new key-value pair combining both synonyms and antonyms. It returns the updated dataset dictionary.\n",
        "\n",
        "3. `write_words_to_file(file_path, arr)`: This function writes an array of words to a file specified by the file_path parameter.\n",
        "\n",
        "4. `debug_tds_dict()`: This is a debugging function that prints whether the word \"public\" is present in the test dataset dictionary with the key \"private\".\n",
        "\n",
        "5. `clean_up()`: This function removes labeled files from the \"expanded_sets_\" directory.\n",
        "\n",
        "6. `main(use_gdown=False)`: This is the main function that orchestrates the entire workflow. It loads the test dataset, reads the entity set, and iterates through the expanded sets to label them based on whether a given word is a synonym or an antonym.\n",
        "\n",
        "**Main**:\n",
        "\n",
        "The `main` function starts by checking if the `use_gdown` flag is set to `True`. If not, it calls the `convert_test_dataset()` function to convert the test dataset. If `use_gdown` is True, it downloads the dataset from a Google Drive folder and loads it.\n",
        "\n",
        "Next, the function reads the entity set from a file and initializes the error counter. It iterates through the expanded sets in the directory \"expanded_sets_\" for various values of K (3, 5, 10, 15) and processes each file by labeling the expanded words as either synonyms (label 1) or antonyms (label 0). It then writes the labeled expanded set to a new text file.\n",
        "\n",
        "Finally, it prints the total number of errors encountered during the process."
      ],
      "metadata": {
        "id": "QcBc726I9o2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Evaluation Metrics\n",
        "#### Recall, Precision, F1-Score."
      ],
      "metadata": {
        "id": "rPBRhPxe4Zly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_y(fp=\n",
        "        f\"Dissertation/M6/output_set_expansion/labelled/\"\n",
        "        f\"eset_mit_wiki_.txt\"\n",
        "    ):\n",
        "    # Read lines from file\n",
        "    words = read_words_from_file(\n",
        "        fp, rtype='list'\n",
        "    )\n",
        "    y_pred_ = []\n",
        "    y_true_ = []\n",
        "    # Extract y prediction value from each line\n",
        "    for word in words:\n",
        "        y = word.split()[-2][:-1]\n",
        "        y_pred_.append(int(y))\n",
        "        y_true_.append(float(word.split()[-1]))\n",
        "    # Return y predictions read from file\n",
        "    return y_pred_, y_true_\n",
        "\n",
        "def main(fp='', out=False):\n",
        "    if fp == '':\n",
        "        y_pred, y_true = load_y()\n",
        "    else:\n",
        "        y_pred, y_true = load_y(fp)\n",
        "\n",
        "    # Calculate y_true\n",
        "    for i,v in enumerate(y_true):\n",
        "        if v > 0.6999999:\n",
        "            y_true[i] = 1\n",
        "        else:\n",
        "            y_true[i] = 0\n",
        "\n",
        "    if not(len(y_pred) == len(y_true)):\n",
        "        raise ValueError(\"Lists `y_pred` must equal `y_true`.\")\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "\n",
        "    # Calculate recall\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    if out:\n",
        "        print(f\"Recall: {recall}\")\n",
        "        print(f\"Precision: {precision}\")\n",
        "        print(f\"F1 Score: {f1}\\n\")\n",
        "\n",
        "    return recall, precision, f1\n",
        "\n",
        "\n",
        "# dir_ = 'labelled_'\n",
        "dir_ = 'expanded_sets_'  # Dir. of sets\n",
        "k_values = [3, 5, 10, 15]  # Top K\n",
        "results = [[] for y in range(7)]\n",
        "# wsi = Window Size Index\n",
        "wsi = {2: 0, 5: 1, 8: 2, 10: 3, 12: 4, 15: 5, 20: 6}\n",
        "out = False\n",
        "\n",
        "# Eval. every file for each folder of K\n",
        "for k in k_values:\n",
        "    for fn in os.listdir(f'{dir_}/{k}'):\n",
        "        # Only eval. labelled files\n",
        "        if not fn.__contains__('lab_set_'):\n",
        "            continue\n",
        "        # Calc. and out window size\n",
        "        win_size = fn.split('_')[-1].split('.')[0][2:-1]\n",
        "        if out:\n",
        "            print(f'window_size = {win_size}, Metrics@{k}')\n",
        "        # Perform eval. on e-set located at `fp`\n",
        "        fp = dir_ + f'/{k}/' + fn\n",
        "        for val in main(fp):\n",
        "            results[wsi[int(win_size)]].append(round(val, 4))\n",
        "\n",
        "for i,v in enumerate(results):\n",
        "    v.insert(0, list(wsi.keys())[i])\n",
        "\n",
        "metrics = 'RPF'\n",
        "headers = ''\n",
        "for k in k_values:\n",
        "    for char in metrics:\n",
        "        headers = headers + f'{char}@{k} '\n",
        "    \n",
        "headers = headers.split(' ')\n",
        "headers.insert(0, 'Win. Size')\n",
        "\n",
        "print(tabulate(results, headers, tablefmt=\"github\"))\n"
      ],
      "metadata": {
        "id": "VAMGBztJ4j4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0743b587-387f-4336-cb1b-a53c76511f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   Win. Size |    R@3 |    P@3 |    F@3 |    R@5 |    P@5 |    F@5 |   R@10 |   P@10 |   F@10 |   R@15 |   P@15 |   F@15 |\n",
            "|-------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n",
            "|           2 | 0.0691 | 0.4692 | 0.1204 | 0.0546 | 0.4022 | 0.0962 | 0.0429 | 0.3167 | 0.0756 | 0.0429 | 0.3167 | 0.0756 |\n",
            "|           5 | 0.0818 | 0.352  | 0.1327 | 0.0622 | 0.284  | 0.102  | 0.0438 | 0.1963 | 0.0717 | 0.0438 | 0.1963 | 0.0717 |\n",
            "|           8 | 0.0711 | 0.2627 | 0.1119 | 0.0582 | 0.2035 | 0.0906 | 0.0437 | 0.1466 | 0.0673 | 0.0437 | 0.1466 | 0.0673 |\n",
            "|          10 | 0.0571 | 0.1947 | 0.0884 | 0.0467 | 0.1572 | 0.072  | 0.0355 | 0.1158 | 0.0543 | 0.0355 | 0.1158 | 0.0543 |\n",
            "|          12 | 0.0592 | 0.181  | 0.0892 | 0.0447 | 0.1429 | 0.0681 | 0.0395 | 0.1115 | 0.0583 | 0.0395 | 0.1115 | 0.0583 |\n",
            "|          15 | 0.0486 | 0.1416 | 0.0724 | 0.039  | 0.1154 | 0.0583 | 0.0319 | 0.0931 | 0.0475 | 0.0319 | 0.0931 | 0.0475 |\n",
            "|          20 | 0.0537 | 0.1404 | 0.0777 | 0.0408 | 0.1164 | 0.0604 | 0.0328 | 0.084  | 0.0472 | 0.0328 | 0.084  | 0.0472 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code evaluates the results of the expanded set of words by calculating the precision, recall, and F1 score.\n",
        "\n",
        "The code reads the predicted binary values from a file and stores it in the list `y_pred`. The actual binary values are stored in the list `y_true`, which is created as an array of ones with the same length as `y_pred`.\n",
        "\n",
        "The code then uses the `precision_score`, `recall_score`, and `f1_score` functions from the scikit-learn library to calculate precision, recall, and F1 score respectively. These metrics are commonly used to evaluate the performance of a binary classifier.\n",
        "\n",
        "Finally, the calculated values of precision, recall, and F1 score are printed to the console."
      ],
      "metadata": {
        "id": "XjzIyeaz8R9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Todo List`\n",
        "\n"
      ],
      "metadata": {
        "id": "5qtSBfDfFj1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "❌🟡✅= Incomplete, Started (but not finished), Complete\n",
        "\n",
        "* https://drive.google.com/drive/my-drive\n",
        "* ✅ Either **make the main file public** for N.R or create and send a copy \n",
        "* 🟡 **Finalize current W2V method**\n",
        "* ✅ **Refine the definition of TP, TN, FP, and FN**: Make sure that your definitions for True Positive, True Negative, False Positive, and False Negative are clear and mutually exclusive.  \n",
        "\t* Refined definitions:\n",
        "\t1. True Positive (TP): The word returned by W2V has a similarity percentage higher than the defined threshold, and the word is a known synonym or antonym (appears in test data).\n",
        "\t2. True Negative (TN): The word returned by W2V has a similarity percentage lower than the defined threshold, and the word is not a known synonym or antonym (does not appear in test data).\n",
        "\t3. False Positive (FP): The word returned by W2V has a similarity percentage higher than the defined threshold, but the word is not a known synonym or antonym (does not appear in test data).\n",
        "\t4. False Negative (FN): The word returned by W2V has a similarity percentage lower than the defined threshold, but the word is a known synonym or antonym (appears in test data). \n",
        "* ✅ **Find related papers for evaluation methodology (WRITTEN BELOW)**: To ensure the correctness and validity of your evaluation process, search for similar papers and study their evaluation methodologies. Incorporate any relevant insights into your project. To Clarify the calculation of F1 for top k results: Review the original paper and related papers to understand how the F1 score is calculated for top k results. Make sure to note any discrepancies or inconsistencies between the figures and the results mentioned in the original paper.\n",
        "* 🟡 **Determine the appropriate threshold for low and high percentages** to ensure that each answer corresponds to only one of these classes.\n",
        "\t* To determine the appropriate threshold for low and high percentages, I can: **Analyze the distribution of similarity percentages in my dataset** to identify natural cut-off points. \n",
        "\n",
        "* ❌ **Investigate Dependency-Based word embeddings**: \n",
        "\t* SkipGram https://www.geeksforgeeks.org/implement-your-own-word2vecskip-gram-model-in-python/\n",
        "\t* Generalize SkipGram by replacing BOG contexts with arbitrary contexts\n",
        "\t* Linear Bag-of-Words Contexts\n",
        "\t* Dependency-Based Contexts\n",
        "\n",
        "## Secondary\n",
        "\n",
        "* ❌ Reorganise folders created for .pkl files\n",
        "* ❌ Incorporate test data into EDA section\n",
        "* ❌ Expand book corpora\n",
        "* ❌ Explain W2V model with TSNE\n",
        "* ❌ Adapt Transformers to lexicon construction\n",
        "* ❌ Add .py tests\n",
        "\n",
        "#### Hyper-parameter testing\n",
        "\n",
        "* ❌ size (Vector Size) [100, 200, 300]\n",
        "* ❌ workers (*Research this one)\n",
        "* ❌ negative (*Research this one)\n",
        "* ❌ min_alpha (*Research this one)\n",
        "\n",
        "#### Other Evaluation\n",
        "\n",
        "* ❌ Evaluate different 'seed sets'\n",
        "* ❌ Evaluate Google's model\n",
        "\n"
      ],
      "metadata": {
        "id": "6nstqrjmD6O_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Methodology Justification\n",
        "\n",
        "* In the 2019 paper \"Entity Set Expansion for Detecting Fashion Trends,\" the authors define metrics@K as Recall, Precision, and F-score calculated for top K predicted results, with K taking the values of 5, 10, and 15. The evaluation is based on the percentage of true positive results within the top K results. For unsupervised methods, the top K results are determined by selecting the K most similar words.\n",
        "\n",
        "* In the 2014 paper \"A Search Log Sparseness Oriented Query Expansion Method,\" the authors define metrics@K as Prec@N, specifically Prec@10 and Prec@20. These metrics represent the accuracy of the first N documents (N being 10 or 20) in the search results. \n",
        "\n",
        "* In the 2022 paper \"A Meta Path Based Method for Entity Set Expansion in Knowledge Graph,\" the authors define metrics@K using two popular evaluation criteria: precision-at-k (p@k) and mean average precision (MAP). Precision-at-k (p@k) measures the percentage of correct entities within the top k results, where k is set to 30, 60, and 90 in this study. \n",
        "\n",
        "* In the 2018 paper \"Entity Set Expansion with Semantic Features of Knowledge Graphs,\" the authors define metrics@K using precision-at-k (p@k) as their primary evaluation metric. Precision-at-k (p@k) is the mean of the percentages of relevant entities in the top-k ranked results for all queries. In this study, the authors measure p@5 and p@10, which assess the performance of their approach at two specific cut-off points in the ranked results. \n",
        "\n",
        "* The authors of four papers on entity set expansion define metrics@K differently. In the 2019 paper, metrics are defined as Recall, Precision, and F-score for top K predicted results with K values of 5, 10, and 15. In the 2014 paper, metrics are defined as Prec@N, specifically Prec@10 and Prec@20, representing the accuracy of the first N documents. In the 2022 paper, metrics are defined using precision-at-k (p@k) and mean average precision (MAP) with k values of 30, 60, and 90. Lastly, in the 2018 paper, metrics are defined using precision-at-k (p@k), measuring the mean percentages of relevant entities in top-k ranked results, specifically p@5 and p@10.\n",
        "\n",
        "### Passages from Papers:\n",
        "\n",
        "1. **Entity Set Expansion for Detecting Fashion Trends (2019)**  \n",
        "\"Table II shows the quantitative results for the four scenarios\n",
        ": W2V and DEP, with and without supervision. The columns\n",
        "show percentages for Recall, Precision and F-score at top\n",
        "k predicted results, for k = 5, 10, 15. For the unsupervised\n",
        "methods, top k results are the most similar k words in their\n",
        "respective vector spaces.\"\n",
        "\n",
        "2. **A Search Log Sparseness Oriented Query Expansion\n",
        "Method (2014)**  \n",
        "\"B. Experimental evaluation metrics\n",
        "The experiment mainly uses TREC standard to rank\n",
        "search results. The main evaluation index is MAP (Mean of\n",
        "Average Precision), the accuracy of the first N documents\n",
        "Prec@N (Prec@10 and Prec@20), comparison and analysis\n",
        "the query performance of the query expansion methods.\n",
        "Baseline of the experiment is the search results of vector\n",
        "space model system without any query expansion method.\"\n",
        "\n",
        "3. **A Meta Path Based Method for Entity Set\n",
        "Expansion in Knowledge Graph (2022)**  \n",
        "\"We employ two popular criteria of precision-at-k (p@k) and\n",
        "mean average precision (MAP) to evaluate the performance\n",
        "of our approach. p@k is the percentage of top k results that\n",
        "belong to correct entities. Here, they are p@30, p@60 and\n",
        "p@90. MAP is the mean of the average precision (AP) of the\n",
        "p@30, p@60 and p@90.\"\n",
        "\n",
        "4. **Entity set expansion with semantic features of knowledge graphs (2018)**   \n",
        "5,3, Evaluation metrics\n",
        "We adopt the following metrics [48] for experimental evalua-\n",
        "tion:\n",
        "• Precision@k (shorted as p@k): the mean of the percentages\n",
        "of the relevant entities in the top-k ranked results for all\n",
        "queries, p@5 and p@10 are measured in our study."
      ],
      "metadata": {
        "id": "jhN2BGXOOlAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Done\n",
        "\n",
        "* ✅ Expand online-wikipedia corpora \n",
        "* ✅ Combine corpora of books and wikipedia pages\n",
        "* ✅ Train one final model on this combined corpora\n",
        "* ✅ Import and use `tqdm`\n",
        "* ✅ Test different window sizes.  \n",
        "    ✅ This cannot be done until the dataset is finalized.\n",
        "\n",
        "* ✅ Tested different (Window Size) [2, 5, 8, 10, 12, 15]\n",
        "* ✅ Speed up the notebook. Replace scraping and PP with downloading files from cloned repo. \n",
        "\n",
        "#### Evaluate different Metrics at K i.e. R@K, P@K, F@K.\n",
        "* ✅ K = 3\n",
        "* ✅ K = 5\n",
        "* ✅ K = 10\n",
        "* ✅ K = 15\n",
        "\n",
        "#### Other\n",
        "* ✅ Video presentation"
      ],
      "metadata": {
        "id": "xRfCq4jp7nI1"
      }
    }
  ]
}