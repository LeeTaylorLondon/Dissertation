We could not find the above page on our servers.
Alternatively, you can visit the Main Page or read more information about this type of error.
Inserting or deleting an element at the end of the array (constant amortized time)
Python's list datatype implementation is a dynamic array the growth pattern of which is:0, 4, 8, 16, 24, 32, 40, 52, 64, 76, ...[29]
Brodnik, Andrej; Carlsson, Svante; Sedgewick, Robert; Munro, JI; Demaine, ED (1999), Resizable Arrays in Optimal Time and Space (PDF) (Technical Report CS-99-09), Department of Computer Science, University of Waterloo
^ a b c Chris Okasaki (1995). "Purely Functional Random-Access Lists". Proceedings of the Seventh International Conference on Functional Programming Languages and Computer Architecture: 86–95. doi:10.1145/224164.224187.
A fixed-size array will suffice in applications where the maximum logical size is fixed (e.g. by specification), or can be calculated before the array is allocated. A dynamic array might be preferred if:
Iterating over the elements in order (linear time, good cache performance)
A balanced tree can store a list while providing all operations of both dynamic arrays and linked lists reasonably efficiently, but both insertion at the end and iteration over the list are slower than for a dynamic array, in theory and in practice, due to non-contiguous storage and tree traversal/manipulation overhead.
Naïve resizable arrays -- also called"the worst implementation" of resizable arrays -- keep the allocated size of the array exactly big enough for all the data it contains, perhaps by calling realloc for each and every item added to the array. Naïve resizable arrays are the simplest way of implementing a resizeable array in C. They don't waste any memory, but appending to the end of the array always takes Θ(n) time.[19][22][23][24][25]Linearly growing arrays pre-allocate ("waste") Θ(1) space every time they re-size the array, making them many times faster than naïve resizable arrays -- appending to the end of the array still takes Θ(n) time but with a much smaller constant.Naïve resizable arrays and linearly growing arrays may be useful when a space-constrained application needs lots of small resizable arrays;they are also commonly used as an educational example leading to exponentially growing dynamic arrays.[26]
Common Lisp provides a rudimentary support for resizable vectors by allowing to configure the built-in array type as adjustable and the location of insertion by the fill-pointer.
.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Lambert, Kenneth Alfred (2009), "Physical size and logical size", Fundamentals of Python: From First Programs Through Data Structures, Cengage Learning, p. 510, ISBN 978-1423902188
Brais, Hadi. "Dissecting the C++ STL Vector: Part 3 - Capacity & Size". Micromysteries. Retrieved 2015-08-05.
Resizable Arrays in Optimal Time and Space (Technical Report CS-99-09)
// (copy the contents to the new memory location here)
^ Brodnik, Andrej; Carlsson, Svante; Sedgewick, Robert; Munro, JI; Demaine, ED (1999), Resizable Arrays in Optimal Time and Space (PDF) (Technical Report CS-99-09), Department of Computer Science, University of Waterloo
Ada's Ada.Containers.Vectors generic package provides dynamic array implementation for a given subtype.
^ a b Goodrich, Michael T.; Tamassia, Roberto (2002), "1.5.2 Analyzing an Extendable Array Implementation", Algorithm Design: Foundations, Analysis and Internet Examples, Wiley, pp. 39–41.
A dynamic array is not the same thing as a dynamically allocated array or variable-length array, either of which is an array whose size is fixed when the array is allocated, although a dynamic array may use such a fixed-size array as a back end.[1]
Compared to linked lists, dynamic arrays have faster indexing (constant time versus linear time) and typically faster iteration due to improved locality of reference; however, dynamic arrays require linear time to insert or delete at an arbitrary location, since all following elements must be moved, while linked lists can do this in constant time. This disadvantage is mitigated by the gap buffer and tiered vector variants discussed under Variants below. Also, in a highly fragmented memory region, it may be expensive or impossible to find contiguous space for a large dynamic array, whereas linked lists do not require the whole data structure to be stored contiguously.
"vector growth factor of 1.5". comp.lang.c++.moderated. Google Groups. Archived from the original on 2011-01-22. Retrieved 2015-08-05.
Goodrich, Michael T.; Tamassia, Roberto (2002), "1.5.2 Analyzing an Extendable Array Implementation", Algorithm Design: Foundations, Analysis and Internet Examples, Wiley, pp. 39–41.
Several cross-platform frameworks provide dynamic array implementations for C, including CFArray and CFMutableArray in Core Foundation, and GArray and GPtrArray in GLib.
^ Number crunching: Why you should never, ever, EVER use linked-list in your code again at kjellkod.wordpress.com
^ a b Sitarski, Edward (September 1996), "HATs: Hashed array trees", Algorithm Alley, Dr. Dobb's Journal, 21 (11)
In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed. It is supplied with standard libraries in many modern mainstream programming languages. Dynamic arrays overcome a limit of static arrays, which have a fixed capacity that needs to be specified at allocation.
^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Lambert, Kenneth Alfred (2009), "Physical size and logical size", Fundamentals of Python: From First Programs Through Data Structures, Cengage Learning, p. 510, ISBN 978-1423902188
Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001) [1990]. "17.4 Dynamic tables". Introduction to Algorithms (2nd ed.). MIT Press and McGraw-Hill. pp. 416–424. ISBN 0-262-03293-7.
CollectionSpy — A Java profiler with explicit support for debugging ArrayList- and Vector-related issues.
See, for example, the source code of java.util.ArrayList class from OpenJDK 6.
The dynamic array has performance similar to an array, with the addition of new operations to add and remove elements:
Dynamic arrays are a common example when teaching amortized analysis.[3][4]
Bagwell, Phil (2002), Fast Functional Lists, Hash-Lists, Deques and Variable Length Arrays, EPFL
Inserting or deleting an element in the middle of the array (linear time)
Gap buffers are similar to dynamic arrays but allow efficient insertion and deletion operations clustered near the same arbitrary location. Some deque implementations use array deques, which allow amortized constant time insertion/removal at both ends, instead of just one end.
This page was last edited on 17 February 2023, at 12:23 (UTC).
^ Bagwell, Phil (2002), Fast Functional Lists, Hash-Lists, Deques and Variable Length Arrays, EPFL
the maximum logical size is unknown, or difficult to calculate, before the array is allocated
the amortized cost of resizing a dynamic array does not significantly affect performance or responsiveness
To avoid incurring the cost of resizing many times, dynamic arrays resize by a large amount, such as doubling in size, and use the reserved space for future expansion. The operation of adding an element to the end might work as follows:
^ "vector growth factor of 1.5". comp.lang.c++.moderated. Google Groups. Archived from the original on 2011-01-22. Retrieved 2015-08-05.
Day 1 Keynote - Bjarne Stroustrup: C++11 Style at GoingNative 2012 on channel9.msdn.com from minute 45 or foil 44
^ Brodnik, Andrej; Carlsson, Svante; Sedgewick, Robert; Munro, JI; Demaine, ED (1999), Resizable Arrays in Optimal Time and Space (Technical Report CS-99-09) (PDF), Department of Computer Science, University of Waterloo
In a 1999 paper,[20] Brodnik et al. describe a tiered dynamic array data structure, which wastes only n1/2 space for n elements at any point in time, and they prove a lower bound showing that any dynamic array must waste this much space if the operations are to remain amortized constant time. Additionally, they present a variant where growing and shrinking the buffer has not only amortized but worst-case constant time.
Getting or setting the value at a particular index (constant time)
Goodrich[18] presented a dynamic array algorithm called tiered vectors that provides O(n1/k) performance for insertions and deletions from anywhere in the array, and O(k) get and set, where k ≥ 2 is a constant parameter.
Bagwell (2002)[21] presented the VList algorithm, which can be adapted to implement a dynamic array.
it is considered that a maximum logical size given by a specification is likely to change
^ a b c "C++ STL vector: definition, growth factor, member functions". Archived from the original on 2015-08-06. Retrieved 2015-08-05.
Hashed array tree (HAT) is a dynamic array algorithm published by Sitarski in 1996.[19] Hashed array tree wastes order n1/2 amount of storage space, where n is the number of elements in the array. The algorithm has O(1) amortized performance when appending a series of objects to the end of a hashed array tree.
C++'s std::vector and Rust's std::vec::Vec are implementations of dynamic arrays, as are the ArrayList[27] classes supplied with the Java API and the .NET Framework.[28]
The generic List class supplied with version 2.0 of the .NET Framework is also implemented with dynamic arrays. Smalltalk's OrderedCollection is a dynamic array with dynamic start and end-index, making the removal of the first element also O(1).
^ Goodrich, Michael T.; Kloss II, John G. (1999), "Tiered Vectors: Efficient Dynamic Arrays for Rank-Based Sequences", Workshop on Algorithms and Data Structures, Lecture Notes in Computer Science, 1663: 205–216, doi:10.1007/3-540-48447-7_21, ISBN 978-3-540-66279-2
Open Data Structures - Chapter 2 - Array-Based Lists, Pat Morin
As n elements are inserted, the capacities form a geometric progression. Expanding the array by any constant proportion a ensures that inserting n elements takes O(n) time overall, meaning that each insertion takes amortized constant time. Many dynamic arrays also deallocate some of the underlying storage if its size drops below a certain threshold, such as 30% of the capacity. This threshold must be strictly smaller than 1/a in order to provide hysteresis (provide a stable band to avoid repeatedly growing and shrinking) and support mixed sequences of insertions and removals with amortized constant cost.
"C++ STL vector: definition, growth factor, member functions". Archived from the original on 2015-08-06. Retrieved 2015-08-05.
"Dissecting the C++ STL Vector: Part 3 - Capacity & Size"
Goodrich, Michael T.; Kloss II, John G. (1999), "Tiered Vectors: Efficient Dynamic Arrays for Rank-Based Sequences", Workshop on Algorithms and Data Structures, Lecture Notes in Computer Science, 1663: 205–216, doi:10.1007/3-540-48447-7_21, ISBN 978-3-540-66279-2
Sitarski, Edward (September 1996), "HATs: Hashed array trees", Algorithm Alley, Dr. Dobb's Journal, 21 (11)
The growth factor for the dynamic array depends on several factors including a space-time trade-off and algorithms used in the memory allocator itself. For growth factor a, the average time per insertion operation is about a/(a−1), while the number of wasted cells is bounded above by (a−1)n[citation needed]. If memory allocator uses a first-fit allocation algorithm, then growth factor values such as a=2 can cause dynamic array expansion to run out of memory even though a significant amount of memory may still be available.[5] There have been various discussions on ideal growth factor values, including proposals for the golden ratio as well as the value 1.5.[6] Many textbooks, however, use a = 2 for simplicity and analysis purposes.[3][4]
Delphi and D implement dynamic arrays at the language's core.
Number crunching: Why you should never, ever, EVER use linked-list in your code again at kjellkod.wordpress.com
^ Brais, Hadi. "Dissecting the C++ STL Vector: Part 3 - Capacity & Size". Micromysteries. Retrieved 2015-08-05.
^ a b See, for example, the source code of java.util.ArrayList class from OpenJDK 6.
^ Day 1 Keynote - Bjarne Stroustrup: C++11 Style at GoingNative 2012 on channel9.msdn.com from minute 45 or foil 44
A simple dynamic array can be constructed by allocating an array of fixed-size, typically larger than the number of elements immediately required. The elements of the dynamic array are stored contiguously at the start of the underlying array, and the remaining positions towards the end of the underlying array are reserved, or unused. Elements can be added at the end of a dynamic array in constant time by using the reserved space, until this space is completely consumed. When all space is consumed, and an additional element is to be added, then the underlying fixed-size array needs to be increased in size. Typically resizing is expensive because it involves allocating a new underlying array and copying each element from the original array. Elements can be removed from the end of a dynamic array in constant time, as no resizing is required. The number of elements used by the dynamic array contents is its logical size or size, while the size of the underlying array is called the dynamic array's capacity or physical size, which is the maximum possible size without relocating data.[2]
Number crunching: Why you should never, ever, EVER use linked-list in your code again
Many scripting languages such as Perl and Ruby offer dynamic arrays as a built-in primitive data type.
Brodnik, Andrej; Carlsson, Svante; Sedgewick, Robert; Munro, JI; Demaine, ED (1999), Resizable Arrays in Optimal Time and Space (Technical Report CS-99-09) (PDF), Department of Computer Science, University of Waterloo
about a/(a−1), while the number of wasted cells is bounded above by (a−1)n
Chris Okasaki (1995). "Purely Functional Random-Access Lists". Proceedings of the Seventh International Conference on Functional Programming Languages and Computer Architecture: 86–95. doi:10.1145/224164.224187.
Dynamic arrays benefit from many of the advantages of arrays, including good locality of reference and data cache utilization, compactness (low memory use), and random access. They usually have only a small fixed additional overhead for storing information about the size and capacity. This makes dynamic arrays an attractive tool for building cache-friendly data structures. However, in languages like Python or Java that enforce reference semantics, the dynamic array generally will not store the actual data, but rather it will store references to the data that resides in other areas of memory. In this case, accessing items in the array sequentially will actually involve accessing multiple non-contiguous areas of memory, so the many advantages of the cache-friendliness of this data structure are lost.
^ a b Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001) [1990]. "17.4 Dynamic tables". Introduction to Algorithms (2nd ed.). MIT Press and McGraw-Hill. pp. 416–424. ISBN 0-262-03293-7.
If a page was recently created here, it may not be visible yet because of a delay in updating the database; wait a few minutes or try the purge function.
Look for pages within Wikipedia that link to this title
Look for pages within Wikipedia that link to this title.
If the page has been deleted, check the deletion log, and see Why was the page I created deleted?.
Titles on Wikipedia are case sensitive except for the first character; please check alternative capitalizations and consider adding a redirect here to the correct title.
You need to log in or create an account to create this page.
Each non-root node can be treated as the root node of its own subtree, which includes that node and all its descendants.[a][1]
Donald Knuth. The Art of Computer Programming: Fundamental Algorithms, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4 . Section 2.3: Trees, pp. 308–423.
CRAN package data.tree – implementation of a tree data structure in the R programming language
^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Weisstein, Eric W. "Subtree". MathWorld.
This page was last edited on 27 February 2023, at 23:37 (UTC).
An internal node (also known as an inner node, inode for short, or branch node) is any node of a tree that has child nodes. Similarly, an external node (also known as an outer node, leaf node, or terminal node) is any node that does not have child nodes.
Adding a new item at a certain position on the tree
Binary trees are a commonly used type, which constrain the number of children for each parent to at most two. When the order of the children is specified, this data structure corresponds to anordered tree in graph theory. A value or pointer to other data may be associated with every node in the tree, or sometimes only with the leaf nodes, which have no children.
Trees as used in computing are similar to but can be different from mathematical constructs of trees in graph theory, trees in set theory, and trees in descriptive set theory.
A binary tree can be implemented as a list of lists: the head of a list (the value of the first term) is the left child (subtree), while the tail (the list of second and subsequent terms) is the right child (subtree). This can be modified to allow values as well, as in Lisp S-expressions, where the head (value of first term) is the value of the node, the head of the tail (value of second term) is the left child, and the tail of the tail (list of third and subsequent terms) is the right child.
^ This is different from the formal definition of subtree used in graph theory, which is a subgraph that forms a tree – it need not include all descendants. For example, the root node by itself is a subtree in the graph theory sense, but not in the data structure sense (unless there are no descendants).
File systems for:Directory structure used to organize subdirectories and files (symbolic links create non-tree graphs, as do multiple hard links to the same file or directory)The mechanism used to allocate and link blocks of data on the storage device
Inheritance of DNA among species by evolution, of source code by software projects (e.g. Linux distribution timeline), of designs in various types of cars, etc.
The abstract data type can be represented in a number of ways, including a list of parents with pointers to children, a list of children with pointers to parents, or a list of nodes and a separate list of parent-child relations (a specific type of adjacency list). Representations might also be more complicated, for example using indexes or ancestor lists for performance.
Susanna S. Epp (Aug 2010). Discrete Mathematics with Applications. Pacific Grove, CA: Brooks/Cole Publishing Co. p. 694. ISBN 978-0-495-39132-6.
Natural language processing:Parse treesModeling utterances in a generative grammarDialogue tree for generating conversations
This is different from the formal definition of subtree used in graph theory, which is a subgraph that forms a tree – it need not include all descendants. For example, the root node by itself is a subtree in the graph theory sense, but not in the data structure sense (unless there are no descendants).
A binary search tree is a type of binary tree
Directory structure used to organize subdirectories and files (symbolic links create non-tree graphs, as do multiple hard links to the same file or directory)
A node is a structure which may contain data and connections to other nodes, sometimes called edges or links. Each node in a tree has zero or more child nodes, which are below it in the tree (by convention, trees are drawn with descendants going downwards). A node that has a child is called the child's parent node (or superior). All nodes have exactly one parent, except the topmost root node, which has none. A node might have many ancestor nodes, such as the parent's parent.Child nodes with the same parent are sibling nodes. Typically siblings have an order, with the first one conventionally drawn on the left. Some definitions allow a tree to have no nodes at all, in which case it is called empty.
WormWeb.org: Interactive Visualization of the C. elegans Cell Tree – Visualize the entire cell lineage tree of the nematode C. elegans (javascript)
A rooted tree with the "away from root" direction (a more narrow term is an "arborescence"), meaning:A directed graph,whose underlying undirected graph is a tree (any two vertices are connected by exactly one simple path),with a distinguished root (one vertex is designated as the root),which determines the direction on the edges (arrows point away from the root; given an edge, the node that the edge points from is called the parent and the node that the edge points to is called the child), together with:
Data Trees as a Means of Presenting Complex Data Analysis
Tree structures are often used for mapping the relationships between things, such as:
an ordering on the child nodes of a given node, and
There are many different ways to represent trees. In working memory, nodes are typically dynamically allocated records with pointers to their children, their parents, or both, as well as any associated data.If of a fixed size, the nodes might be stored in a list. Nodes and relationships between nodes might be stored in a separate special type of adjacency list. In relational databases, nodes are typically represented as table rows, with indexed row IDs facilitating pointers between parents and children.
Allowing empty trees makes some definitions simpler, some more complicated: a rooted tree must be non-empty, hence if empty trees are allowed the above definition instead becomes "an empty tree or a rooted tree such that ...". On the other hand, empty trees simplify defining fixed branching factor: with empty trees allowed, a binary tree is a tree such that every node has exactly two children, each of which is a tree (possibly empty). The complete sets of operations on the tree must include the fork operation.[clarification needed]
The mechanism used to allocate and link blocks of data on the storage device
Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed.
In computer science, a tree is a widely used abstract data type that represents a hierarchical tree structure with a set of connected nodes. Each node in the tree can be connected to many children (depending on the type of tree), but must be connected to exactly one parent, except for the root node, which has no parent. These constraints mean there are no cycles or "loops" (no node can be its own ancestor), and also that each child can be treated like the root node of its own subtree, making recursion a useful technique for tree traversal. In contrast to linear data structures, many trees cannot be represented by relationships between neighboring nodes in a single straight line.
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 10.4: Representing rooted trees, pp. 214–217. Chapters 12–14 (Binary Search Trees, Red–Black Trees, Augmenting Data Structures), pp. 253–320.
^ Susanna S. Epp (Aug 2010). Discrete Mathematics with Applications. Pacific Grove, CA: Brooks/Cole Publishing Co. p. 694. ISBN 978-0-495-39132-6.
Nodes can also be stored as items in an array, with relationships between them determined by their positions in the array (as in a binary heap).
Trees are commonly used to represent or manipulate hierarchical data in applications such as:
.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Weisstein, Eric W. "Subtree". MathWorld.
Donald Knuth (1997). "Section 2.3.4.2: Oriented trees". The Art of Computer Programming. Vol. 1: Fundamental Algorithms (Third ed.). Addison-Wesley. p. 373.
Trees can be used to represent and manipulate various mathematical structures, such as:
Components and subcomponents which can be visualized in an exploded-view drawing
^ L. Afanasiev; P. Blackburn; I. Dimitriou; B. Gaiffe; E. Goris; M. Marx; M. de Rijke (2005). "PDL for ordered trees" (PDF). Journal of Applied Non-Classical Logics. 15 (2): 115–135. doi:10.3166/jancl.15.115-135. S2CID 1979330.
4Common operations											Toggle Common operations subsection																					4.1Traversal and search methods
Hierarchical taxonomies such as the Dewey Decimal Classification with sections of increasing specificity.
Ordered trees can be naturally encoded by finite sequences, for example with natural numbers.[4]
Subroutine calls used to identify which subroutines in a program call other subroutines non recursively
Viewed as a whole, a tree data structure is an ordered tree, generally with values attached to each node. Concretely, it is (if required to be non-empty):
Search trees store data in a way that makes an efficient search algorithm possible via tree traversalA binary search tree is a type of binary tree
JSON and YAML documents can be thought of as trees, but are typically represented by nested lists and dictionaries.
In terms of type theory, a tree is an inductive type defined by the constructors nil (empty forest) and node (tree with root node with given value and children).
Data Trees as a Means of Presenting Complex Data Analysis by Sally Knipe on August 8, 2013
L. Afanasiev; P. Blackburn; I. Dimitriou; B. Gaiffe; E. Goris; M. Marx; M. de Rijke (2005). "PDL for ordered trees" (PDF). Journal of Applied Non-Classical Logics. 15 (2): 115–135. doi:10.3166/jancl.15.115-135. S2CID 1979330.
^ Donald Knuth (1997). "Section 2.3.4.2: Oriented trees". The Art of Computer Programming. Vol. 1: Fundamental Algorithms (Third ed.). Addison-Wesley. p. 373.
Often trees have a fixed (more properly, bounded) branching factor (outdegree), particularly always having two child nodes (possibly empty, hence at most two non-empty child nodes), hence a "binary tree".
which determines the direction on the edges (arrows point away from the root; given an edge, the node that the edge points from is called the parent and the node that the edge points to is called the child), together with:
with a distinguished root (one vertex is designated as the root),
As an abstract data type, the abstract tree type T with values of some type E is defined, using the abstract forest type F (list of trees), by the functions:
Class hierarchy or "inheritance tree" showing the relationships among classes in object-oriented programming; multiple inheritance produces non-tree graphs
The height of a node is the length of the longest downward path to a leaf from that node. The height of the root is the height of the tree. The depth of a node is the length of the path to its root (i.e., its root path). When using zero-based counting, the root node has depth zero, leaf nodes have height zero, and a tree with only a single node (hence both a root and leaf) has depth and height zero. Conventionally, an empty tree (tree with no nodes, if such are allowed) has height −1.
Paths through an arbitrary node-and-edge graph (including multigraphs), by making multiple nodes in the tree for each graph node used in multiple paths
Document Object Models ("DOM tree") of XML and HTML documents
whose underlying undirected graph is a tree (any two vertices are connected by exactly one simple path),
Storing deque contents in a circular buffer, and only resizing when the buffer becomes full. This decreases the frequency of resizings.
Both the basic and most common list types in computing, queues and stacks can be considered specializations of deques, and can be implemented using deques.
Please help improve it to make it understandable to non-experts, without removing the technical details.
^ Adam L. Buchsbaum and Robert E. Tarjan. Confluently persistent deques via data structural bootstrapping. Journal of Algorithms, 18(3):513–547, May 1995. (pp. 58, 101, 125)
Blumofe, Robert D.; Leiserson, Charles E. (1999). "Scheduling multithreaded computations by work stealing" (PDF). J ACM. 46 (5): 720–748. doi:10.1145/324133.324234. S2CID 5428476.
^ a b Okasaki, Chris (September 1996). Purely Functional Data Structures (PDF) (Ph.D. thesis). Carnegie Mellon University. CMU-CS-96-177.
^ Donald Knuth. The Art of Computer Programming, Volume 1: Fundamental Algorithms, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4. Section 2.2.1: Stacks, Queues, and Deques, pp. 238–243.
Rust's std::collections includes VecDeque which implements a double-ended queue using a growable ring buffer.
Python 2.4 introduced the collections module with support for deque objects. It is implemented using a doubly linked list of fixed-length subarrays.
It remains to explain how to define a method balance that rebalance the deque if insert' or tail broke the invariant. The method insert and tail can be defined by first applying insert' and tail' and then applying balance.
4.1Purely functional implementation																								4.1.1Real-time deques via lazy rebuilding and scheduling																											4.1.2Implementation without laziness
In computer science, a double-ended queue (abbreviated to deque, pronounced deck, like "cheque"[1]) is an abstract data type that generalizes a queue, for which elements can be added to or removed from either the front (head) or back (tail).[2] It is also often called a head-tail linked list, though properly this refers to a specific data structure implementation of a deque (see below).
There are at least two common ways to efficiently implement a deque: with a modified dynamic array or with a doubly linked list.
where rotateRev(front, middle, rear) is a function that returns the front, followed by the middle reversed, followed by the rear. This function is also defined using laziness to ensure that it can be computed step by step, with one step executed during each insert' and tail' and taking a constant time. This function uses the invariant that |rear|-2|front| is 2 or 3.
C++'s Standard Template Library provides the class templates std::deque and std::list, for the multiple array and linked list implementations, respectively.
GHC's Data.Sequence module implements an efficient, functional deque structure in Haskell. The implementation uses 2–3 finger trees annotated with sizes. There are other (fast) possibilities to implement purely functional (thus also persistent) double queues (most using heavily lazy evaluation).[3][4] Kaplan and Tarjan were the first to implement optimal confluently persistent catenable deques.[5] Their implementation was strictly purely functional in the sense that it did not use lazy evaluation. Okasaki simplified the data structure by using lazy evaluation with a bootstrapped data structure and degrading the performance bounds from worst-case to amortized. Kaplan, Okasaki, and Tarjan produced a simpler, non-bootstrapped, amortized version that can be implemented either using lazy evaluation or more efficiently using mutation in a broader but still restricted fashion. Mihaesau and Tarjan created a simpler (but still highly complex) strictly purely functional implementation of catenable deques, and also a much simpler implementation of strictly purely functional non-catenable deques, both of which have optimal worst-case bounds.
An input-restricted deque is one where deletion can be made from both ends, but insertion can be made at one end only.
Type-safe open source deque implementation at Comprehensive C Archive Network
^ Jesse Liberty; Siddhartha Rao; Bradley Jones. C++ in One Hour a Day, Sams Teach Yourself, Sixth Edition. Sams Publishing, 2009. .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}ISBN 0-672-32941-7. Lesson 18: STL Dynamic Array Classes, pp. 486.
Let us recall that, for a list l, |l| denotes its length, that NIL represents an empty list and CONS(h, t) represents the list whose head is h and whose tail is t. The functions drop(i, l) and take(i, l) return the list l without its first i elements, and the first i elements of l, respectively. Or, if |l| < i, they return the empty list and l respectively.
Code Project: An In-Depth Study of the STL Deque Container
Please consider expanding the lead to provide an accessible overview of all important aspects of the article.
The dynamic array approach uses a variant of a dynamic array that can grow from both ends, sometimes called array deques. These array deques have all the properties of a dynamic array, such as constant-time random access, good locality of reference, and inefficient insertion/removal in the middle, with the addition of amortized constant-time insertion/removal at both ends, instead of just one end. Three common implementations include:
where rotateDrop(front, i, rear)) return the concatenation of front and of drop(i, rear). That isfront' = rotateDrop(front, ceil_half_len, rear) put into front' the content of front and thecontent of rear that is not already in rear'. Since dropping n elements takes O(n){\displaystyle O(n)} time, we use laziness to ensure that elements are dropped two by two, with two drops being done during each tail' and each insert' operation.
As of Java 6, Java's Collections Framework provides a new Deque interface that provides the functionality of insertion and removal at both ends. It is implemented by classes such as ArrayDeque (also new in Java 6) and LinkedList, providing the dynamic array and linked list implementations, respectively. However, the ArrayDeque, contrary to its name, does not support random access.
Deque is sometimes written dequeue, but this use is generally deprecated in technical literature or technical writing because dequeue is also a verb meaning "to remove from a queue". Nevertheless, several libraries and some writers, such as Aho, Hopcroft, and Ullman in their textbook Data Structures and Algorithms, spell it dequeue. John Mitchell, author of Concepts in Programming Languages, also uses this terminology.
Haim Kaplan and Robert E. Tarjan. Purely functional representations of catenable sorted lists. In ACM Symposium on Theory of Computing, pages 202–211, May1996. (pp. 4, 82, 84, 124)
^ Haim Kaplan and Robert E. Tarjan. Purely functional representations of catenable sorted lists. In ACM Symposium on Theory of Computing, pages 202–211, May1996. (pp. 4, 82, 84, 124)
This differs from the queue abstract data type or first in first out list (FIFO), where elements can only be added to one end and removed from the other. This general data class has some possible sub-types:
A double-ended queue is represented as a sextuple (len_front, front, tail_front, len_rear, rear, tail_rear) where front is a linked list which contains the front of the queue of length len_front. Similarly, rear is a linked list which represents the reverse of the rear of the queue, of length len_rear. Furthermore, it is assured that |front| ≤ 2|rear|+1 and |rear| ≤ 2|front|+1 - intuitively, it means that both the front and the rear contains between a third minus one and two thirds plus one of the elements. Finally, tail_front and tail_rear are tails of front and of rear, they allow scheduling the moment where some lazy operations are forced. Note that, when a double-ended queue contains n elements in the front list and n elements in the rear list, then the inequality invariant remains satisfied after i insertions and d deletions when (i+d) ≤ n/2. That is, at most n/2 operations can happen between each rebalancing.
As of PHP 5.3, PHP's SPL extension contains the 'SplDoublyLinkedList' class that can be used to implement Deque datastructures. Previously to make a Deque structure the array functions array_shift/unshift/pop/push had to be used instead.
Jesse Liberty; Siddhartha Rao; Bradley Jones. C++ in One Hour a Day, Sams Teach Yourself, Sixth Edition. Sams Publishing, 2009. .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}ISBN 0-672-32941-7. Lesson 18: STL Dynamic Array Classes, pp. 486.
In a doubly-linked list implementation and assuming no allocation/deallocation overhead, the time complexity of all deque operations is O(1). Additionally, the time complexity of insertion or deletion in the middle, given an iterator, is O(1); however, the time complexity of random access by index is O(n).
One example where a deque can be used is the work stealing algorithm.[6] This algorithm implements task scheduling for several processors. A separate deque with threads to be executed is maintained for each processor. To execute the next thread, the processor gets the first element from the deque (using the "remove first element" deque operation). If the current thread forks, it is put back to the front of the deque ("insert element at front") and a new thread is executed. When one of the processors finishes execution of its own threads (i.e. its deque is empty), it can "steal" a thread from another processor: it gets the last element from the deque of another processor ("remove last element") and executes it. The work stealing algorithm is used by Intel's Threading Building Blocks (TBB) library for parallel programming.
Adam L. Buchsbaum and Robert E. Tarjan. Confluently persistent deques via data structural bootstrapping. Journal of Algorithms, 18(3):513–547, May 1995. (pp. 58, 101, 125)
This page was last edited on 6 August 2022, at 18:02 (UTC).
Ada's containers provides the generic packages Ada.Containers.Vectors and Ada.Containers.Doubly_Linked_Lists, for the dynamic array and linked list implementations, respectively.
Okasaki, Chris (September 1996). Purely Functional Data Structures (PDF) (Ph.D. thesis). Carnegie Mellon University. CMU-CS-96-177.
Note that, without the lazy part of the implementation, this would be a non-persistent implementation of queue in O(1) amortized time. In this case, the lists tail_front and tail_rear could be removed from the representation of the double-ended queue.
Let us first give an implementation of the various operations that affect the front of the deque - cons, head and tail. Those implementation do not necessarily respect the invariant. In a second time we'll explain how to modify a deque which does not satisfy the invariant into one which satisfy it. However, they use the invariant, in that if the front is empty then the rear has at most one element. The operations affecting the rear of the list are defined similarly by symmetry.
4Implementations											Toggle Implementations subsection																					4.1Purely functional implementation																								4.1.1Real-time deques via lazy rebuilding and scheduling																											4.1.2Implementation without laziness
Storing contents in multiple smaller arrays, allocating additional arrays at the beginning or end as needed. Indexing is implemented by keeping a dynamic array containing pointers to each of the smaller arrays.
^ Blumofe, Robert D.; Leiserson, Charles E. (1999). "Scheduling multithreaded computations by work stealing" (PDF). J ACM. 46 (5): 720–748. doi:10.1145/324133.324234. S2CID 5428476.
Allocating deque contents from the center of the underlying array, and resizing the underlying array when either end is reached. This approach may require more frequent resizings and waste more space, particularly when elements are only inserted at one end.
Double-ended queues can also be implemented as a purely functional data structure.[3]: 115  Two versions of the implementation exist. The first one, called 'real-time deque, is presented below. It allows the queue to be persistent with operations in O(1) worst-case time, but requires lazy lists with memoization. The second one, with no lazy lists nor memoization is presented at the end of the sections. Its amortized time is O(1) if the persistency is not used; but the worst-time complexity of an operation is O(n) where n is the number of elements in the double-ended queue.
In a growing array, the amortized time complexity of all deque operations is O(1). Additionally, the time complexity of random access by index is O(1); but the time complexity of insertion or deletion in the middle is O(n).
An output-restricted deque is one where insertion can be made at both ends, but deletion can be made from one end only.
The basic operations on a deque are enqueue and dequeue on either end. Also generally implemented are peek operations, which return the value at that end without dequeuing it.
Donald Knuth. The Art of Computer Programming, Volume 1: Fundamental Algorithms, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4. Section 2.2.1: Stacks, Queues, and Deques, pp. 238–243.
Javascript's Array prototype & Perl's arrays have native support for both removing (shift and pop) and adding (unshift and push) elements on both ends.
Other libraries provide low-level functions and implementations of data structures, including:
Microsoft Foundation Class Library (MFC) – An object-oriented C++ wrapper library to the C-based Windows API which also includes some data structures and other convenience functionality
The Apache Portable Runtime and Apple Core Foundation have a large functional overlap with GLib, and provide many similar OS-portable threading, network and data structure implementations in C.
Boost – provides some functions for C++, such as threading primitives, similar to what GLib does for C
GLib began as part of the GTK+ project, now named GTK. However, before releasing GTK+ version 2, the project's developers decided to separate code from GTK+ that was not for graphical user interfaces (GUIs), thus creating GLib as a separate software bundle. GLib was released as a separate library so other developers, those not using the GUI-related parts of GTK+, could use the non-GUI parts of the library without the overhead of depending on the full GUI library.
This page was last edited on 28 February 2023, at 09:08 (UTC).
The GLib package consisted of five libraries, but they were all merged into one library, since then named simply GLib, and are no longer sustained as standalone libraries. The original libraries were:
GLib implements functions that provide threads, thread programming and related facilities such as primitive variable access, mutexes, asynchronous queues, secure memory pools, message passing and logging, hook functions (callback registering) and timers. GLib also includes message passing facilities such as byte order conversion and I/O channels.
Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: "GLib" – news · newspapers · books · scholar · JSTOR
^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Krause, Andrew (2007). Foundations of GTK+ Development. Expert's Voice in Open Source. Apress. p. 5. ISBN 978-1-59059-793-4. Retrieved 3 April 2013. [GLib] provides a cross-platform interface that allows your code to be run on any of its supported operating systems with little to no rewriting of code!
Standard Template Library (STL) – C++ library for data structures and algorithms
GLib provides advanced data structures, such as memory chunks, doubly and singly linked lists, hash tables, dynamic strings and string utilities, such as a lexical scanner, string chunks (groups of strings), dynamic arrays, balanced binary trees, N-ary trees, quarks (a two-way association of a string and a unique integer identifier), keyed data lists, relations, and tuples. Caches provide memory management.
.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Krause, Andrew (2007). Foundations of GTK+ Development. Expert's Voice in Open Source. Apress. p. 5. ISBN 978-1-59059-793-4. Retrieved 3 April 2013. [GLib] provides a cross-platform interface that allows your code to be run on any of its supported operating systems with little to no rewriting of code!
Glib is undergoing active development. For a current overview see https://gitlab.gnome.org/GNOME/glib/-/blob/main/NEWS. The table below documents major patch notes from 1998 to 2022.
Since GLib is a cross-platform library, applications using it to interface with the operating system are usually portable across different operating systems without major changes.[2]
Of these, three continue to reside in distinct subdirectories of the source tree, and so can be thought of as discrete components: GLib, GObject, and GIO. These can be thought of as a software stack: GObject relies on GLib, and GIO provides higher-level functionality that uses both.
GLibis a bundle of three (formerly five) low-level system libraries written in C and developed mainly by GNOME. GLib's code was separated from GTK, so it can be used by software other than GNOME and has been developed in parallel ever since.
Find sources: "GLib" – news · newspapers · books · scholar · JSTOR
Batista, Facundo (17 October 2003). "PEP 327 – Decimal Data Type". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 4 June 2020. Retrieved 24 November 2008.
^ "PEP 0465 – A dedicated infix operator for matrix multiplication". python.org. Archived from the original on 4 June 2020. Retrieved 1 January 2016.
^ van Rossum, Guido; Eby, Phillip J. (10 May 2005). "PEP 342 – Coroutines via Enhanced Generators". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 29 May 2020. Retrieved 19 February 2012.
Triple-quoted (beginning and ending with three single or double quote marks), which may span multiple lines and function like here documents in shells, Perl, and Ruby.
"Transcrypt: Anatomy of a Python to JavaScript Compiler". InfoQ. Archived from the original on 5 December 2020. Retrieved 20 January 2021.
"mypy – Optional Static Typing for Python". Archived from the original on 6 June 2020. Retrieved 28 January 2017.
Pythran compiles a subset of Python 3 to C++ (C++11).[146][147][148]
^ "Why must 'self' be used explicitly in method definitions and calls?". Design and History FAQ. Python Software Foundation. Archived from the original on 24 October 2012. Retrieved 19 February 2012.
^ "Python 3.5.1 Release and Changelog". python.org. Archived from the original on 14 May 2020. Retrieved 1 January 2016.
"Core Security". Core Security. Archived from the original on 9 June 2020. Retrieved 10 April 2020.
.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}"General Python FAQ — Python 3.9.2 documentation". docs.python.org. Archived from the original on 24 October 2012. Retrieved 28 March 2021.
"division". python.org. Archived from the original on 20 July 2006. Retrieved 30 July 2014.
"Perl and Python influences in JavaScript". www.2ality.com. 24 February 2013. Archived from the original on 26 December 2018. Retrieved 15 May 2015.
"CVE – CVE-2021-23336". cve.mitre.org. Archived from the original on 24 February 2021. Retrieved 26 February 2021.
^ Langa, Łukasz (19 February 2021). "Python Insider: Python 3.9.2 and 3.8.8 are now available". Python Insider. Archived from the original on 25 February 2021. Retrieved 26 February 2021.
^ Eby, Phillip J. (7 December 2003). "PEP 333 – Python Web Server Gateway Interface v1.0". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 14 June 2020. Retrieved 19 February 2012.
"Documentation Tools". Python.org. Archived from the original on 11 November 2020. Retrieved 22 March 2021.
^ Dean, Jeff; Monga, Rajat;et al. (9 November 2015). "TensorFlow: Large-scale machine learning on heterogeneous systems" (PDF). TensorFlow.org. Google Research. Archived (PDF) from the original on 20 November 2015. Retrieved 10 November 2015.
^ "PEP 373 – Python 2.7 Release Schedule". Python.org. Archived from the original on 13 January 2020. Retrieved 22 September 2019.
^ Hughes, Owen (24 May 2021). "Programming languages: Why Python 4.0 might never arrive, according to its creator". TechRepublic. Retrieved 16 May 2022.
^ a b c "itertools — Functions creating iterators for efficient looping — Python 3.7.1 documentation". docs.python.org. Archived from the original on 14 June 2020. Retrieved 22 November 2016. This module implements a number of iterator building blocks inspired by constructs from APL, Haskell, and SML.
Most Python implementations (including CPython) include a read–eval–print loop (REPL), permitting them to function as a command line interpreter for which users enter statements sequentially and receive results immediately.
"Code Style — The Hitchhiker's Guide to Python". docs.python-guide.org. Archived from the original on 27 January 2021. Retrieved 20 January 2021.
^ "Python 2.5 internals" (PDF). Archived (PDF) from the original on 6 August 2012. Retrieved 19 April 2011.
"Google's Go: A New Programming Language That's Python Meets C++"
Performance comparison of various Python implementations on a non-numerical (combinatorial) workload was presented at EuroSciPy '13.[156] Python's performance compared to other programming languages is also benchmarked by The Computer Language Benchmarks Game.[157]
"How we use Python at Spotify". Spotify Labs. 20 March 2013. Archived from the original on 10 June 2020. Retrieved 25 July 2018.
^ "PEP 373 – Python 2.7 Release Schedule". python.org. Archived from the original on 19 May 2020. Retrieved 9 January 2017.
^ Oliphant, Travis (2007). "Python for Scientific Computing". Computing in Science and Engineering. 9 (3): 10–20. Bibcode:2007CSE.....9c..10O. CiteSeerX 10.1.1.474.6460. doi:10.1109/MCSE.2007.58. S2CID 206457124. Archived from the original on 15 June 2020. Retrieved 10 April 2015.
"Python 3.11.2, Python 3.10.10 and 3.12.0 alpha 5 are available". 8 February 2023. Retrieved 8 February 2023.
Guelton, Serge; Brunet, Pierrick; Amini, Mehdi; Merlini, Adrien; Corbillon, Xavier; Raynaud, Alan (16 March 2015). "Pythran: enabling static optimization of scientific Python programs". Computational Science & Discovery. IOP Publishing. 8 (1): 014001. Bibcode:2015CS&D....8a4001G. doi:10.1088/1749-4680/8/1/014001. ISSN 1749-4699.
"Frequently asked questions". Godot Engine documentation. Archived from the original on 28 April 2021. Retrieved 10 May 2021.
"Nim language draws from best of Python, Rust, Go, and Lisp"
Langa, Łukasz (16 March 2022). "Python Insider: Python 3.10.3, 3.9.11, 3.8.13, and 3.7.13 are now available with security content". Python Insider. Retrieved 19 April 2022.
^ File extension .pyo was removed in Python 3.5. See PEP 0488 Archived 1 June 2020 at the Wayback Machine
^ "CVE – CVE-2021-23336". cve.mitre.org. Archived from the original on 24 February 2021. Retrieved 26 February 2021.
Its core philosophy is summarized in the document The Zen of Python (PEP 20), which includes aphorisms such as:[73]
van Rossum, Guido (9 February 2006). "Language Design Is Not Just Solving Puzzles". Artima forums. Artima. Archived from the original on 17 January 2020. Retrieved 21 March 2007.
Python consistently ranks as one of the most popular programming languages.[38][39][40][41]
"Build a Rapid Web Development Environment for Python Server Pages and Oracle"
^ "Starlark Language". Archived from the original on 15 June 2020. Retrieved 25 May 2019.
^ "Organizations Using Python". Python Software Foundation. Archived from the original on 21 August 2018. Retrieved 15 January 2009.
"Jython FAQ". www.jython.org. Archived from the original on 22 April 2021. Retrieved 22 April 2021.
"Nuitka Home | Nuitka Home". nuitka.net. Archived from the original on 30 May 2020. Retrieved 18 August 2017.
Fairchild, Carlie (12 July 2018). "Guido van Rossum Stepping Down from Role as Python's Benevolent Dictator For Life". Linux Journal. Archived from the original on 13 July 2018. Retrieved 13 July 2018.
Many operating systems include Python as a standard component. It ships with most Linux distributions,[205] AmigaOS 4 (using Python 2.7), FreeBSD (as a package), NetBSD, and OpenBSD (as a package) and can be used from the command line (terminal). Many Linux distributions use installers written in Python: Ubuntu uses the Ubiquity installer, while Red Hat Linux and Fedora Linux use the Anaconda installer. Gentoo Linux uses Python in its package management system, Portage.
Strings in Python can be concatenated by "adding" them (with the same operator as for adding integers and floats), e.g. "spam" + "eggs" returns "spameggs". If strings contain numbers, they are added as strings rather than integers, e.g. "2" + "2" returns "22".
^ "GitHub – IronLanguages/ironpython3: Implementation of Python 3.x for .NET Framework that is built on top of the Dynamic Language Runtime". GitHub. Archived from the original on 28 September 2021.
File extension .pyo was removed in Python 3.5. See PEP 0488 Archived 1 June 2020 at the Wayback Machine
"speed comparison between CPython and Pypy". Speed.pypy.org. Archived from the original on 10 May 2021. Retrieved 3 December 2012.
"PEP 498 – Literal String Interpolation". python.org. Archived from the original on 15 June 2020. Retrieved 8 March 2017.
The pass statement, serving as a NOP, syntactically needed to create an empty code block
"4.0 New Features and Fixes". LibreOffice.org. The Document Foundation. 2013. Archived from the original on 9 February 2014. Retrieved 25 February 2013.
"Python eats away at R: Top Software for Analytics, Data Science, Machine Learning in 2018: Trends and Analysis"
"Who is using scikit-learn? — scikit-learn 0.20.1 documentation". scikit-learn.org. Archived from the original on 6 May 2020. Retrieved 30 November 2018.
^ "PEP 602 – Annual Release Cycle for Python". Python.org. Archived from the original on 14 June 2020. Retrieved 6 November 2019.
Kuchling, Andrew M. (22 December 2006). "Interview with Guido van Rossum (July 1998)". amk.ca. Archived from the original on 1 May 2007. Retrieved 12 March 2012. I'd spent a summer at DEC's Systems Research Center, which introduced me to Modula-2+; the Modula-3 final report was being written there at about the same time. What I learned there later showed up in Python's exception handling, modules, and the fact that methods explicitly contain 'self' in their parameter list. String slicing came from Algol-68 and Icon.
Python is used extensively in the information security industry, including in exploit development.[206][207]
^ "Python Buildbot". Python Developer's Guide. Python Software Foundation. Archived from the original on 5 June 2020. Retrieved 24 September 2011.
"10 Reasons Python Rocks for Research (And a Few Reasons it Doesn't) – Hoyt Koepke". www.stat.washington.edu. Archived from the original on 31 May 2020. Retrieved 3 February 2019.
"Starlark Language". Archived from the original on 15 June 2020. Retrieved 25 May 2019.
^ "PEP 7 – Style Guide for C Code | peps.python.org". peps.python.org. Retrieved 28 April 2022.
"8. Errors and Exceptions — Python 3.12.0a0 documentation". docs.python.org. Retrieved 9 May 2022.
^ Kernighan, Brian W.; Ritchie, Dennis M. (1988). The C Programming Language (2nd ed.). p. 206.
The return statement, used to return a value from a function
^ "CPython byte code". Docs.python.org. Archived from the original on 5 June 2020. Retrieved 16 February 2016.
Jouppi, Norm. "Google supercharges machine learning tasks with TPU custom chip". Google Cloud Platform Blog. Archived from the original on 18 May 2016. Retrieved 19 May 2016.
"Swift Evolution Process". Swift Programming Language Evolution repository on GitHub. 18 February 2020. Archived from the original on 27 April 2020. Retrieved 27 April 2020.
^ "IronPython.net /". ironpython.net. Archived from the original on 17 April 2021.
^ "The State of Developer Ecosystem in 2020 Infographic". JetBrains: Developer Tools for Professionals and Teams. Archived from the original on 1 March 2021. Retrieved 5 March 2021.
"Whetting Your Appetite". The Python Tutorial. Python Software Foundation. Archived from the original on 26 October 2012. Retrieved 20 February 2012.
Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a "batteries included" language due to its comprehensive standard library.[34][35]
Go is designed for the "speed of working in a dynamic language like Python"[214] and shares the same syntax for slicing arrays.
"2to3 – Automated Python 2 to 3 code translation". docs.python.org. Archived from the original on 4 June 2020. Retrieved 2 February 2021.
Python has a type of expression called a list comprehension, as well as a more general expression called a generator expression.[71]
van Rossum, Guido (22 April 2009). "Tail Recursion Elimination". Neopythonic.blogspot.be. Archived from the original on 19 May 2018. Retrieved 3 December 2012.
"Python : the holy grail of programming". CERN Bulletin. CERN Publications (31/2006). 31 July 2006. Archived from the original on 15 January 2013. Retrieved 11 February 2012.
Borderies, Olivier (24 January 2019). "Pythran: Python at C++ speed !". Medium. Archived from the original on 25 March 2020. Retrieved 25 March 2020.
In Python, == compares by value. Python's is operator may be used to compare object identities (comparison by reference), and comparisons may be chained—for example, a <= b <= c.
Python uses duck typing and has typed objects but untyped variable names. Type constraints are not checked at compile time; rather, operations on an object may fail, signifying that it is not of a suitable type. Despite being dynamically typed, Python is strongly typed, forbidding operations that are not well-defined (for example, adding a number to a string) rather than silently attempting to make sense of them.
^ "Contracts for Python". Archived from the original on 15 June 2020. Retrieved 24 September 2011.
Lutz, Mark (2009). Learning Python: Powerful Object-Oriented Programming. O'Reilly Media, Inc. p. 17. ISBN 9781449379322. Archived from the original on 17 July 2017. Retrieved 9 May 2017.
Taft, Darryl K. (5 March 2007). "Python Slithers into Systems". eWeek.com. Ziff Davis Holdings. Archived from the original on 13 August 2021. Retrieved 24 September 2011.
^ a b Venners, Bill (13 January 2003). "The Making of Python". Artima Developer. Artima. Archived from the original on 1 September 2016. Retrieved 22 March 2007.
"Python for Artificial Intelligence". Wiki.python.org. 19 July 2012. Archived from the original on 1 November 2012. Retrieved 3 December 2012.
^ Beazley, David M. (2009). Python Essential Reference (4th ed.). p. 66. ISBN 9780672329784.
Paine, Jocelyn, ed. (August 2005). "AI in Python". AI Expert Newsletter. Amzi!. Archived from the original on 26 March 2012. Retrieved 11 February 2012.
^ Langa, Łukasz (16 March 2022). "Python Insider: Python 3.10.3, 3.9.11, 3.8.13, and 3.7.13 are now available with security content". Python Insider. Retrieved 19 April 2022.
^ Guttag, John V. (12 August 2016). Introduction to Computation and Programming Using Python: With Application to Understanding Data. MIT Press. ISBN 978-0-262-52962-4.
"PEP 466 – Network Security Enhancements for Python 2.7.x". python.org. Archived from the original on 4 June 2020. Retrieved 9 January 2017.
^ "Documentation Tools". Python.org. Archived from the original on 11 November 2020. Retrieved 22 March 2021.
^ The Cain Gang Ltd. "Python Metaclasses: Who? Why? When?" (PDF). Archived from the original (PDF) on 30 May 2009. Retrieved 27 June 2009.
"PEP 373 – Python 2.7 Release Schedule". python.org. Archived from the original on 19 May 2020. Retrieved 9 January 2017.
^ Holth, Moore (30 March 2014). "PEP 0441 – Improving Python ZIP Application Support". Archived from the original on 26 December 2018. Retrieved 12 November 2015.
^ Strachan, James (29 August 2003). "Groovy – the birth of a new dynamic language for the Java platform". Archived from the original on 5 April 2007. Retrieved 11 June 2007.
^ Lattner, Chris (3 June 2014). "Chris Lattner's Homepage". Chris Lattner. Archived from the original on 22 December 2015. Retrieved 3 June 2014. I started work on the Swift Programming Language in July of 2010. I implemented much of the basic language structure, with only a few people knowing of its existence. A few other (amazing) people started contributing in earnest late in 2011, and it became a major focus for the Apple Developer Tools group in July 2013 [...] drawing ideas from Objective-C, Rust, Haskell, Ruby, Python, C#, CLU, and far too many others to list.
^ "Perl and Python influences in JavaScript". www.2ality.com. 24 February 2013. Archived from the original on 26 December 2018. Retrieved 15 May 2015.
"Plans for optimizing Python". Google Project Hosting. 15 December 2009. Archived from the original on 11 April 2016. Retrieved 24 September 2011.
Shafer, Daniel G. (17 January 2003). "Python Streamlines Space Shuttle Mission Design". Python Software Foundation. Archived from the original on 5 June 2020. Retrieved 24 November 2008.
Lutz, Mark (2013). Learning Python (5th ed.). O'Reilly Media. ISBN 978-0-596-15806-4.
^ "1. Extending Python with C or C++ — Python 3.9.1 documentation". docs.python.org. Archived from the original on 23 June 2020. Retrieved 14 February 2021.
^ Francisco, Thomas Claburn in San. "Google's Grumpy code makes Python Go". www.theregister.com. Archived from the original on 7 March 2021. Retrieved 20 January 2021.
Kincaid, Jason (10 November 2009). "Google's Go: A New Programming Language That's Python Meets C++". TechCrunch. Archived from the original on 18 January 2010. Retrieved 29 January 2010.
Lattner, Chris (3 June 2014). "Chris Lattner's Homepage". Chris Lattner. Archived from the original on 25 December 2018. Retrieved 3 June 2014. The Swift language is the product of tireless effort from a team of language experts, documentation gurus, compiler optimization ninjas, and an incredibly important internal dogfooding group who provided feedback to help refine and battle-test ideas. Of course, it also greatly benefited from the experiences hard-won by many other languages in the field, drawing ideas from Objective-C, Rust, Haskell, Ruby, Python, C#, CLU, and far too many others to list.
"Proposals: iterators and generators [ES4 Wiki]". wiki.ecmascript.org. Archived from the original on 20 October 2007. Retrieved 24 November 2008.
"An empirical comparison of C, C++, Java, Perl, Python, Rexx, and Tcl"
"A Python Book: Beginning Python, Advanced Python, and Python Exercises"
"round", The Python standard library, release 2.7, §2: Built-in functions, archived from the original on 27 October 2012, retrieved 14 August 2011
corbet (24 October 2022). "Python 3.11 released [LWN.net]". lwn.net. Retrieved 15 November 2022.
"15. Floating Point Arithmetic: Issues and Limitations — Python 3.8.3 documentation"
van Rossum, Guido (29 August 2000). "SETL (was: Lukewarm about range literals)". Python-Dev (Mailing list). Archived from the original on 14 July 2018. Retrieved 13 March 2011.
^ "round", The Python standard library, release 2.7, §2: Built-in functions, archived from the original on 27 October 2012, retrieved 14 August 2011
^ CCP porkbelly (24 August 2010). "Stackless Python 2.7". EVE Community Dev Blogs. CCP Games. Archived from the original on 11 January 2014. Retrieved 11 January 2014. As you may know, EVE has at its core the programming language known as Stackless Python.
Python uses whitespace indentation, rather than curly brackets or keywords, to delimit blocks. An increase in indentation comes after certain statements; a decrease in indentation signifies the end of the current block.[82] Thus, the program's visual structure accurately represents its semantic structure.[83] This feature is sometimes termed the off-side rule. Some other languages use indentation this way; but in most, indentation has no semantic meaning. The recommended indent size is four spaces.[84]
Caudill, Barry (20 September 2005). "Modding Sid Meier's Civilization IV". Sid Meier's Civilization IV Developer Blog. Firaxis Games. Archived from the original on 2 December 2010. we created three levels of tools ... The next level offers Python and XML support, letting modders with more experience manipulate the game world and everything in it.
Guttag, John V. (12 August 2016). Introduction to Computation and Programming Using Python: With Application to Understanding Data. MIT Press. ISBN 978-0-262-52962-4.
Summerfield, Mark (2009). Programming in Python 3 (2nd ed.). Addison-Wesley Professional. ISBN 978-0-321-68056-3.
^ Martelli, Alex; Ravenscroft, Anna; Ascher, David (2005). Python Cookbook, 2nd Edition. O'Reilly Media. p. 230. ISBN 978-0-596-00797-3. Archived from the original on 23 February 2020. Retrieved 14 November 2015.
Team, The PyPy (28 December 2019). "Download and Install". PyPy. Retrieved 8 January 2022.
^ "How we use Python at Spotify". Spotify Labs. 20 March 2013. Archived from the original on 10 June 2020. Retrieved 25 July 2018.
^ "Brython". brython.info. Archived from the original on 3 August 2018. Retrieved 21 January 2021.
^ "Stack Overflow Developer Survey 2022". Stack Overflow. Retrieved 12 August 2022.
Python uses dynamic typing and a combination of reference counting and a cycle-detecting garbage collector for memory management.[70] It uses dynamic name resolution (late binding), which binds method and variable names during program execution.
Python is commonly used in artificial intelligence projects and machine learning projects with the help of libraries like TensorFlow, Keras, Pytorch, and scikit-learn.[194][195][196][197] As a scripting language with a modular architecture, simple syntax, and rich text processing tools, Python is often used for natural language processing.[198]
Rounding towards negative infinity, though different from most languages, adds consistency. For instance, it means that the equation (a + b)//b == a//b + 1 is always true. It also means that the equation b*(a//b) + a%b == a is valid for both positive and negative values of a. However, maintaining the validity of this equation means that while the result of a%b is, as expected, in the half-open interval [0, b), where b is a positive integer, it has to lie in the interval (b, 0] when b is negative.[109]
'Type a number, and its factorial will be printed: '
^ Borderies, Olivier (24 January 2019). "Pythran: Python at C++ speed !". Medium. Archived from the original on 25 March 2020. Retrieved 25 March 2020.
^ "PyPy compatibility". Pypy.org. Archived from the original on 6 June 2020. Retrieved 3 December 2012.
RPython can be compiled to C, and is used to build the PyPy interpreter of Python.
"pprint — Data pretty printer — Python 3.11.0 documentation". docs.python.org. Archived from the original on 22 January 2021. Retrieved 5 November 2022. stuff = ['spam', 'eggs', 'lumberjack', 'knights', 'ni']
The prefix Py- is used to show that something is related to Python. Examples of the use of this prefix in names of Python applications or libraries include Pygame, a binding of SDL to Python (commonly used to create games); PyQt and PyGTK, which bind Qt and GTK to Python respectively; and PyPy, a Python implementation originally written in Python.
Yegulalp, Serdar (29 October 2020). "Pyston returns from the dead to speed Python". InfoWorld. Archived from the original on 27 January 2021. Retrieved 26 January 2021.
Most of the Sugar software for the One Laptop per Child XO, developed at Sugar Labs since 2008, is written in Python.[208] The Raspberry Pi single-board computer project has adopted Python as its main user-programming language.
Python has a "string format" operator % that functions analogously to printf format strings in C—e.g. "spam=%s eggs=%d" % ("blah", 2) evaluates to "spam=blah eggs=2". In Python 2.6+ and 3+, this was supplemented by the format() method of the str class, e.g. "spam={0} eggs={1}".format("blah", 2). Python 3.6 added "f-strings": spam = "blah"; eggs = 2; f'spam={spam} eggs={eggs}'.[98]
^ Batista, Facundo. "PEP 0327 – Decimal Data Type". Python.org. Archived from the original on 4 June 2020. Retrieved 26 September 2015.
^ Batista, Facundo (17 October 2003). "PEP 327 – Decimal Data Type". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 4 June 2020. Retrieved 24 November 2008.
Rossum, Guido Van (20 January 2009). "The History of Python: A Brief Timeline of Python". The History of Python. Archived from the original on 5 June 2020. Retrieved 5 March 2021.
"Brython". brython.info. Archived from the original on 3 August 2018. Retrieved 21 January 2021.
"Application-level Stackless features — PyPy 2.0.2 documentation". Doc.pypy.org. Archived from the original on 4 June 2020. Retrieved 17 July 2013.
^ "Projects". opensource.google. Archived from the original on 24 April 2020. Retrieved 25 March 2020.
Jython compiles Python 2.7 to Java bytecode, allowing the use of the Java libraries from a Python program.[155]
^ "The Genie Programming Language Tutorial". Archived from the original on 1 June 2020. Retrieved 28 February 2020.
"PyDBC: method preconditions, method postconditions and class invariants for Python". Archived from the original on 23 November 2019. Retrieved 24 September 2011.
"PEP 13 – Python Language Governance". Python.org. Archived from the original on 27 May 2021. Retrieved 25 August 2021.
^ "Why Python's Integer Division Floors". 24 August 2010. Archived from the original on 5 June 2020. Retrieved 25 August 2010.
Piotrowski, Przemyslaw (July 2006). "Build a Rapid Web Development Environment for Python Server Pages and Oracle". Oracle Technology Network. Oracle. Archived from the original on 2 April 2019. Retrieved 12 March 2012.
Zadka, Moshe; van Rossum, Guido (11 March 2001). "PEP 238 – Changing the Division Operator". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 28 May 2020. Retrieved 23 October 2013.
Yegulalp, Serdar (16 January 2017). "Nim language draws from best of Python, Rust, Go, and Lisp". InfoWorld. Archived from the original on 13 October 2018. Retrieved 7 June 2020. Nim's syntax is strongly reminiscent of Python's, as it uses indented code blocks and some of the same syntax (such as the way if/elif/then/else blocks are constructed).
^ Yegulalp, Serdar (29 October 2020). "Pyston returns from the dead to speed Python". InfoWorld. Archived from the original on 27 January 2021. Retrieved 26 January 2021.
van Rossum, Guido; Eby, Phillip J. (10 May 2005). "PEP 342 – Coroutines via Enhanced Generators". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 29 May 2020. Retrieved 19 February 2012.
Python allows programmers to define their own types using classes, most often used for object-oriented programming. New instances of classes are constructed by calling the class (for example, SpamClass() or EggsClass()), and the classes are instances of the metaclass type (itself an instance of itself), allowing metaprogramming and reflection.
Many alpha, beta, and release-candidates are also released as previews and for testing before final releases. Although there is a rough schedule for each release, they are often delayed if the code is not ready. Python's development team monitors the state of the code by running the large unit test suite during development.[168]
^ "The Computer Language Benchmarks Game". Archived from the original on 14 June 2020. Retrieved 30 April 2020.
"Python 0.9.1 part 01/21". alt.sources archives. Archived from the original on 11 August 2021. Retrieved 11 August 2021.
^ "CVE – CVE-2021-3177". cve.mitre.org. Archived from the original on 27 February 2021. Retrieved 26 February 2021.
Backward-incompatible versions, where code is expected to break and needs to be manually ported. The first part of the version number is incremented. These releases happen infrequently—version 3.0 was released 8 years after 2.0. According to Guido van Rossum, a version 4.0 is very unlikely to ever happen.[163]
Peters, Tim (19 August 2004). "PEP 20 – The Zen of Python". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 26 December 2018. Retrieved 24 November 2008.
The @ infix operator. It is intended to be used by libraries such as NumPy for matrix multiplication.[92][93]
"round", The Python standard library, release 3.2, §2: Built-in functions, archived from the original on 25 October 2012, retrieved 14 August 2011
^ "Python Developer's Guide — Python Developer's Guide". devguide.python.org. Archived from the original on 9 November 2020. Retrieved 17 December 2019.
^ Kincaid, Jason (10 November 2009). "Google's Go: A New Programming Language That's Python Meets C++". TechCrunch. Archived from the original on 18 January 2010. Retrieved 29 January 2010.
"PyPy compatibility". Pypy.org. Archived from the original on 6 June 2020. Retrieved 3 December 2012.
The +, -, and * operators for mathematical addition, subtraction, and multiplication are similar to other languages, but the behavior of division differs. There are two types of divisions in Python: floor division (or integer division) // and floating-point/division.[91] Python uses the ** operator for exponentiation.
Stackless Python is a significant fork of CPython that implements microthreads; it does not use the call stack in the same way, thus allowing massively concurrent programs. PyPy also has a stackless version.[136]
"Is Python a good language for beginning programmers?". General Python FAQ. Python Software Foundation. Archived from the original on 24 October 2012. Retrieved 21 March 2007.
^ "Why is Python a dynamic language and also a strongly typed language – Python Wiki". wiki.python.org. Archived from the original on 14 March 2021. Retrieved 27 January 2021.
"In Python, should I use else after a return in an if block?"
Python has array index and array slicing expressions in lists, denoted as a[key], a[start:stop] or a[start:stop:step]. Indexes are zero-based, and negative indexes are relative to the end. Slices take elements from the start index up to, but not including, the stop index. The third slice parameter called step or stride, allows elements to be skipped and reversed. Slice indexes may be omitted—for example, a[:] returns a copy of the entire list. Each element of a slice is a shallow copy.
^ "15. Floating Point Arithmetic: Issues and Limitations — Python 3.8.3 documentation". docs.python.org. Archived from the original on 6 June 2020. Retrieved 6 June 2020. Almost all machines today (November 2000) use IEEE-754 floating point arithmetic, and almost all platforms map Python floats to IEEE-754 "double precision".
^ "Ada 83 Reference Manual (raise statement)". Archived from the original on 22 October 2019. Retrieved 7 January 2020.
As well as standard desktop integrated development environments, there are Web browser-based IDEs, including SageMath, for developing science- and math-related programs; PythonAnywhere, a browser-based IDE and hosting environment; and Canopy IDE, a commercial IDE emphasizing scientific computing.[122]
"PEP 7 – Style Guide for C Code | peps.python.org"
Hughes, Owen (24 May 2021). "Programming languages: Why Python 4.0 might never arrive, according to its creator". TechRepublic. Retrieved 16 May 2022.
In Python terms, / is true division (or simply division), and // is floor division. / before version 3.0 is classic division.[108]
"Python Buildbot". Python Developer's Guide. Python Software Foundation. Archived from the original on 5 June 2020. Retrieved 24 September 2011.
^ "In Python, should I use else after a return in an if block?". Stack Overflow. Stack Exchange. 17 February 2011. Archived from the original on 20 June 2019. Retrieved 6 May 2011.
Python 3.10 deprecated wstr (to be removed in Python 3.12; meaning Python extensions[169] need to be modified by then),[170] and added pattern matching to the language.[171]
Python uses arbitrary-precision arithmetic for all integer operations. The Decimal type/class in the decimal module provides decimal floating-point numbers to a pre-defined arbitrary precision and several rounding modes.[114] The Fraction class in the fractions module provides arbitrary precision for rational numbers.[115]
"6.5 itertools – Functions creating iterators for efficient looping". Docs.python.org. Archived from the original on 14 June 2020. Retrieved 22 November 2016.
"Transcrypt – Python in the browser". transcrypt.org. Archived from the original on 19 August 2018. Retrieved 22 December 2020.
^ "Transcrypt – Python in the browser". transcrypt.org. Archived from the original on 19 August 2018. Retrieved 22 December 2020.
"Tornado: Facebook's Real-Time Web Framework for Python – Facebook for Developers"
"Python 3.5.1 Release and Changelog". python.org. Archived from the original on 14 May 2020. Retrieved 1 January 2016.
The try statement, which allows exceptions raised in its attached code block to be caught and handled by except clauses (or new syntax except* in Python 3.11 for exception groups[85]); it also ensures that clean-up code in a finally block is always run regardless of how the block exits
"Python Insider: Python 3.10.3, 3.9.11, 3.8.13, and 3.7.13 are now available with security content"
Python provides a round function for rounding a float to the nearest integer. For tie-breaking, Python 3 uses round to even: round(1.5) and round(2.5) both produce 2.[110] Versions before 3 used round-away-from-zero: round(0.5) is 1.0, round(-0.5) is −1.0.[111]
Langa, Łukasz (19 February 2021). "Python Insider: Python 3.9.2 and 3.8.8 are now available". Python Insider. Archived from the original on 25 February 2021. Retrieved 26 February 2021.
^ van Rossum, Guido (1993). "An Introduction to Python for UNIX/C Programmers". Proceedings of the NLUUG Najaarsconferentie (Dutch UNIX Users Group). CiteSeerX 10.1.1.38.2023. even though the design of C is far from ideal, its influence on Python is considerable.
Gustafsson, Per; Niskanen, Raimo (29 January 2007). "EEP 1: EEP Purpose and Guidelines". erlang.org. Archived from the original on 15 June 2020. Retrieved 19 April 2011.
^ "The Python Language Reference, section 3.3. New-style and classic classes, for release 2.7.1". Archived from the original on 26 October 2012. Retrieved 12 January 2011.
^ a b Warsaw, Barry; Hylton, Jeremy; Goodger, David (13 June 2000). "PEP 1 – PEP Purpose and Guidelines". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 6 June 2020. Retrieved 19 April 2011.
"What is Sugar?". Sugar Labs. Archived from the original on 9 January 2009. Retrieved 11 February 2012.
Current Python (i.e. since 3.0) changed / to always be floating-point division, e.g. 5/2 == 2.5.
Hamilton, Naomi (5 August 2008). "The A-Z of Programming Languages: Python". Computerworld. Archived from the original on 29 December 2008. Retrieved 31 March 2010.
^ "Core Security". Core Security. Archived from the original on 9 June 2020. Retrieved 10 April 2020.
^ Kuhlman, Dave. "A Python Book: Beginning Python, Advanced Python, and Python Exercises". Section 1.1. Archived from the original (PDF) on 23 June 2012.
There are several compilers to high-level object languages, with either unrestricted Python, a restricted subset of Python, or a language similar to Python as the source language:
PyPy is a fast, compliant interpreter of Python 2.7 and 3.8.[133][134] Its just-in-time compiler often brings a significant speed improvement over CPython but some libraries written in C cannot be used with it.[135]
^ Science education with SageMath, Innovative Computing in Science Education, archived from the original on 15 June 2020, retrieved 22 April 2019
Practical JRuby on Rails Web 2.0 Projects: bringing Ruby on Rails to the Java platform
"Why must 'self' be used explicitly in method definitions and calls?"
^ Fortenberry, Tim (17 January 2003). "Industrial Light & Magic Runs on Python". Python Software Foundation. Archived from the original on 6 June 2020. Retrieved 11 February 2012.
Python does not support tail call optimization or first-class continuations, and, according to Van Rossum, it never will.[87][88] However, better support for coroutine-like functionality is provided by extending Python's generators.[89] Before 2.5, generators were lazy iterators; data was passed unidirectionally out of the generator. From Python 2.5 on, it is possible to pass data back into a generator function; and from version 3.3, it can be passed through multiple stack levels.[90]
^ Enthought, Canopy. "Canopy". www.enthought.com. Archived from the original on 15 July 2017. Retrieved 20 August 2016.
"PEP 634 – Structural Pattern Matching: Specification". Python.org. Archived from the original on 6 May 2021. Retrieved 14 February 2021.
7Implementations											Toggle Implementations subsection																					7.1Reference implementation																											7.2Other implementations																											7.3Unsupported implementations																											7.4Cross-compilers to other languages																											7.5Performance
^ van Rossum, Guido (5 June 2001). "PEP 7 – Style Guide for C Code". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 1 June 2020. Retrieved 24 November 2008.
Cobra uses indentation and a similar syntax, and its Acknowledgements document lists Python first among languages that influenced it.[211]
Groovy was motivated by the desire to bring the Python design philosophy to Java.[215]
The with statement, which encloses a code block within a context manager (for example, acquiring a lock before it is run, then releasing the lock; or opening and closing a file), allowing resource-acquisition-is-initialization (RAII)-like behavior and replacing a common try/finally idiom[86]
"4. Building C and C++ Extensions — Python 3.9.2 documentation". docs.python.org. Archived from the original on 3 March 2021. Retrieved 1 March 2021.
IronPythonallows running Python 2.7 programs (and an alpha, released in 2021, is also available for "Python 3.4, although features and behaviors from later versions may be included"[153]) on the .NET Common Language Runtime.[154]
Hettinger, Raymond (30 January 2002). "PEP 289 – Generator Expressions". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 14 June 2020. Retrieved 19 February 2012.
Bini, Ola (2007). Practical JRuby on Rails Web 2.0 Projects: bringing Ruby on Rails to the Java platform. Berkeley: APress. p. 3. ISBN 978-1-59059-881-8.
"IronPython.net /". ironpython.net. Archived from the original on 17 April 2021.
Python 2.0 was released on 16 October 2000, with many major new features such as list comprehensions, cycle-detecting garbage collection, reference counting, and Unicode support.[48] Python 3.0, released on 3 December 2008, with many of its major features backported to Python 2.6.x[49] and 2.7.x. Releases of Python 3 include the 2to3 utility, which automates the translation of Python 2 code to Python 3.[50]
Major or "feature" releases are largely compatible with the previous version but introduce new features. The second part of the version number is incremented. Starting with Python 3.9, these releases are expected to happen annually.[164][165] Each major version is supported by bug fixes for several years after its release.[166]
Python can also be used to create games, with libraries such as Pygame, which can make 2D games.
Sweigart, Al (2020). Beyond the Basic Stuff with Python: Best Practices for Writing Clean Code. No Starch Press. p. 322. ISBN 978-1-59327-966-0. Archived from the original on 13 August 2021. Retrieved 7 July 2021.
"CoffeeScript". coffeescript.org. Archived from the original on 12 June 2020. Retrieved 3 July 2018.
Ring Team (4 December 2017). "Ring and other languages". ring-lang.net. ring-lang. Archived from the original on 25 December 2018. Retrieved 4 December 2017.
"Organizations Using Python". Python Software Foundation. Archived from the original on 21 August 2018. Retrieved 15 January 2009.
^ "PEP 634 – Structural Pattern Matching: Specification". Python.org. Archived from the original on 6 May 2021. Retrieved 14 February 2021.
GDScript, a scripting language very similar to Python, built-in to the Godot game engine.[213]
^ "6.5 itertools – Functions creating iterators for efficient looping". Docs.python.org. Archived from the original on 14 June 2020. Retrieved 22 November 2016.
"PEP 373 – Python 2.7 Release Schedule". Python.org. Archived from the original on 13 January 2020. Retrieved 22 September 2019.
^ Guelton, Serge; Brunet, Pierrick; Amini, Mehdi; Merlini, Adrien; Corbillon, Xavier; Raynaud, Alan (16 March 2015). "Pythran: enabling static optimization of scientific Python programs". Computational Science & Discovery. IOP Publishing. 8 (1): 014001. Bibcode:2015CS&D....8a4001G. doi:10.1088/1749-4680/8/1/014001. ISSN 1749-4699.
Oliphant, Travis (2007). "Python for Scientific Computing". Computing in Science and Engineering. 9 (3): 10–20. Bibcode:2007CSE.....9c..10O. CiteSeerX 10.1.1.474.6460. doi:10.1109/MCSE.2007.58. S2CID 206457124. Archived from the original on 15 June 2020. Retrieved 10 April 2015.
^ "Application-level Stackless features — PyPy 2.0.2 documentation". Doc.pypy.org. Archived from the original on 4 June 2020. Retrieved 17 July 2013.
^ Team, The PyPy (28 December 2019). "Download and Install". PyPy. Retrieved 8 January 2022.
"PyDBC: method preconditions, method postconditions and class invariants for Python"
^ "10 Reasons Python Rocks for Research (And a Few Reasons it Doesn't) – Hoyt Koepke". www.stat.washington.edu. Archived from the original on 31 May 2020. Retrieved 3 February 2019.
Python has been successfully embedded in many software products as a scripting language, including in finite element method software such as Abaqus, 3D parametric modelers like FreeCAD, 3D animation packages such as 3ds Max, Blender, Cinema 4D, Lightwave, Houdini, Maya, modo, MotionBuilder, Softimage, the visual effects compositor Nuke, 2D imaging programs like GIMP,[199] Inkscape, Scribus and Paint Shop Pro,[200] and musical notation programs like scorewriter and capella. GNU Debugger uses Python as a pretty printer to show complex structures such as C++ containers. Esri promotes Python as the best choice for writing scripts in ArcGIS.[201] It has also been used in several video games,[202][203] and has been adopted as first of the three available programming languages in Google App Engine, the other two being Java and Go.[204]
Piatetsky, Gregory. "Python eats away at R: Top Software for Analytics, Data Science, Machine Learning in 2018: Trends and Analysis". KDnuggets. KDnuggets. Archived from the original on 15 November 2019. Retrieved 30 May 2018.
The continue statement, which skips the rest of the current iteration and continues with the next
The assignment statement (=) binds a name as a reference to a separate, dynamically allocated object. Variables may subsequently be rebound at any time to any object. In Python, a variable name is a generic reference holder without a fixed data type; however, it always refers to some object with a type. This is called dynamic typing—in contrast to statically-typed languages, where each variable may contain only a value of a certain type.
^ Kuchling, A. M. "Functional Programming HOWTO". Python v2.7.2 documentation. Python Software Foundation. Archived from the original on 24 October 2012. Retrieved 9 February 2012. List comprehensions and generator expressions [...] are a concise notation for such operations, borrowed from the functional programming language Haskell.
^ Bini, Ola (2007). Practical JRuby on Rails Web 2.0 Projects: bringing Ruby on Rails to the Java platform. Berkeley: APress. p. 3. ISBN 978-1-59059-881-8.
^ Kuchling, A. M.; Zadka, Moshe (16 October 2000). "What's New in Python 2.0". Python Software Foundation. Archived from the original on 23 October 2012. Retrieved 11 February 2012.
^ "4.0 New Features and Fixes". LibreOffice.org. The Document Foundation. 2013. Archived from the original on 9 February 2014. Retrieved 25 February 2013.
"Myths about indentation in Python". Secnetix.de. Archived from the original on 18 February 2018. Retrieved 19 April 2011.
"3.3. Special method names". The Python Language Reference. Python Software Foundation. Archived from the original on 15 December 2018. Retrieved 27 June 2009.
^ "Built-in Types". Archived from the original on 14 June 2020. Retrieved 3 October 2019.
Python has the usual symbols for arithmetic operators (+, -, *, /), the floor division operator // and the modulo operation % (where the remainder can be negative,e.g. 4 % -3 == -2). It also has ** for exponentiation, e.g. 5**3 == 125 and 9**0.5 == 3.0, and a matrix‑multiplication operator @ .[107] These operators work like in traditional math; with the same precedence rules, the operators infix (+ and - can also be unary to represent positive and negative numbers respectively).
^ "Who is using scikit-learn? — scikit-learn 0.20.1 documentation". scikit-learn.org. Archived from the original on 6 May 2020. Retrieved 30 November 2018.
Before version 3.0, Python had two kinds of classes (both using the same syntax): old-style and new-style,[101] current Python versions only support the semantics new style.
^ "Highlights: Python 2.5". Python.org. Archived from the original on 4 August 2019. Retrieved 20 March 2018.
Zadka, Moshe; van Rossum, Guido (11 March 2001). "PEP 237 – Unifying Long Integers and Integers". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 28 May 2020. Retrieved 24 September 2011.
^ "4. Building C and C++ Extensions — Python 3.9.2 documentation". docs.python.org. Archived from the original on 3 March 2021. Retrieved 1 March 2021.
Libraries such as NumPy, SciPy, and Matplotlib allow the effective use of Python in scientific computing,[190][191] with specialized libraries such as Biopython and Astropy providing domain-specific functionality. SageMath is a computer algebra system with a notebook interface programmable in Python: its library covers many aspects of mathematics, including algebra, combinatorics, numerical mathematics, number theory, and calculus.[192] OpenCV has Python bindings with a rich set of features for computer vision and image processing.[193]
"Red Hat Customer Portal – Access to 24x7 support and knowledge". access.redhat.com. Archived from the original on 6 March 2021. Retrieved 26 February 2021.
"Download Python". Python.org. Archived from the original on 8 August 2018. Retrieved 24 May 2021.
Kuhlman, Dave. "A Python Book: Beginning Python, Advanced Python, and Python Exercises". Section 1.1. Archived from the original (PDF) on 23 June 2012.
"4. Building C and C++ Extensions — Python 3.9.2 documentation"
"history [vmspython]". www.vmspython.org. Archived from the original on 2 December 2020. Retrieved 4 December 2020.
"What's New in Python 2.6 — Python v2.6.9 documentation". docs.python.org. Archived from the original on 23 December 2019. Retrieved 26 September 2015.
"GitHub – IronLanguages/ironpython3: Implementation of Python 3.x for .NET Framework that is built on top of the Dynamic Language Runtime"
^ "round", The Python standard library, release 3.2, §2: Built-in functions, archived from the original on 25 October 2012, retrieved 14 August 2011
Downey, Allen B. (May 2012). Think Python: How to Think Like a Computer Scientist (version 1.6.6 ed.). ISBN 978-0-521-72596-5.
In 2022, Python 3.10.4 and 3.9.12 were expedited[59] and 3.8.13, and 3.7.13, because of many security issues.[60] When Python 3.9.13 was released in May 2022, it was announced that the 3.9 series (joining the older series 3.8 and 3.7) would only receive security fixes in the future.[61] On September 7, 2022, four new releases were made due to a potential denial-of-service attack: 3.10.7, 3.9.14, 3.8.14, and 3.7.14.[62][63]
^ "Type hinting for Python". LWN.net. 24 December 2014. Archived from the original on 20 June 2019. Retrieved 5 May 2015.
"1. Extending Python with C or C++ — Python 3.9.1 documentation". docs.python.org. Archived from the original on 23 June 2020. Retrieved 14 February 2021.
Python makes a distinction between lists and tuples. Lists are written as [1, 2, 3], are mutable, and cannot be used as the keys of dictionaries (dictionary keys must be immutable in Python). Tuples, written as (1, 2, 3), are immutable and thus can be used as keys of dictionaries, provided all of the tuple's elements are immutable. The + operator can be used to concatenate two tuples, which does not directly modify their contents, but produces a new tuple containing the elements of both. Thus, given the variable t initially equal to (1, 2, 3), executing t = t + (4, 5) first evaluates t + (4, 5), which yields (1, 2, 3, 4, 5), which is then assigned back to t—thereby effectively "modifying the contents" of t while conforming to the immutable nature of tuple objects. Parentheses are optional for tuples in unambiguous contexts.[96]
"Natural Language Toolkit — NLTK 3.5b1 documentation". www.nltk.org. Archived from the original on 13 June 2020. Retrieved 10 April 2020.
Python 2.7's end-of-life was initially set for 2015, then postponed to 2020 out of concern that a large body of existing code could not easily be forward-ported to Python 3.[51][52]No further security patches or other improvements will be released for it.[53][54] Currently only 3.7 and later are supported. In 2021, Python 3.9.2 and 3.8.8 were expedited[55] as all versions of Python (including 2.7[56]) had security issues leading to possible remote code execution[57] and web cache poisoning.[58]
^ Langa, Łukasz (17 May 2022). "Python Insider: Python 3.9.13 is now available". Python Insider. Retrieved 21 May 2022.
Some parts of the standard library are covered by specifications—for example, the Web Server Gateway Interface (WSGI) implementation wsgiref follows PEP 333[120]—but most are specified by their code, internal documentation, and test suites. However, because most of the standard library is cross-platform Python code, only a few modules need altering or rewriting for variant implementations.
"jasc psp9components". Archived from the original on 19 March 2008.
^ "platform — Access to underlying platform's identifying data — Python 3.10.4 documentation". docs.python.org. Retrieved 17 May 2022.
"re — Regular expression operations — Python 3.10.6 documentation". docs.python.org. Retrieved 6 September 2022. This module provides regular expression matching operations similar to those found in Perl.
^ a b "Whetting Your Appetite". The Python Tutorial. Python Software Foundation. Archived from the original on 26 October 2012. Retrieved 20 February 2012.
^ "More Control Flow Tools". Python 3 documentation. Python Software Foundation. Archived from the original on 4 June 2016. Retrieved 24 July 2015. By popular demand, a few features commonly found in functional programming languages like Lisp have been added to Python. With the lambda keyword, small anonymous functions can be created.
^ "Python on the Nokia N900". Stochastic Geometry. 29 April 2010. Archived from the original on 20 June 2019. Retrieved 9 July 2015.
^ "Pythran — Pythran 0.9.5 documentation". pythran.readthedocs.io. Archived from the original on 19 February 2020. Retrieved 25 March 2020.
^ Gustafsson, Per; Niskanen, Raimo (29 January 2007). "EEP 1: EEP Purpose and Guidelines". erlang.org. Archived from the original on 15 June 2020. Retrieved 19 April 2011.
Enthought, Canopy. "Canopy". www.enthought.com. Archived from the original on 15 July 2017. Retrieved 20 August 2016.
"5.3. Tuples and Sequences — Python 3.7.1rc2 documentation". python.org. Archived from the original on 10 June 2020. Retrieved 17 October 2018.
"Python Insider: Python releases 3.10.7, 3.9.14, 3.8.14, and 3.7.14 are now available". pythoninsider.blogspot.com. 7 September 2022. Retrieved 16 September 2022.
Batista, Facundo. "PEP 0327 – Decimal Data Type". Python.org. Archived from the original on 4 June 2020. Retrieved 26 September 2015.
^ "Python Setup and Usage". Python Software Foundation. Archived from the original on 17 June 2020. Retrieved 10 January 2020.
"Stack Overflow Developer Survey 2022". Stack Overflow. Retrieved 12 August 2022.
"PEP 602 – Annual Release Cycle for Python". Python.org. Archived from the original on 14 June 2020. Retrieved 6 November 2019.
"1. Extending Python with C or C++ — Python 3.9.1 documentation"
^ "PEP 465 – A dedicated infix operator for matrix multiplication". python.org. Archived from the original on 29 May 2020. Retrieved 3 July 2018.
"itertools — Functions creating iterators for efficient looping — Python 3.7.1 documentation"
^ Murri, Riccardo (2013). Performance of Python runtimes on a non-numeric scientific code. European Conference on Python in Science (EuroSciPy). arXiv:1404.6388. Bibcode:2014arXiv1404.6388M.
^ "PEP 623 – Remove wstr from Unicode". Python.org. Archived from the original on 5 March 2021. Retrieved 14 February 2021.
Ramalho, Luciano (May 2022). Fluent Python (2nd ed.). O'Reilly Media. ISBN 978-1-4920-5632-4.
Google began a project named Unladen Swallow in 2009, with the aim of speeding up the Python interpreter fivefold by using the LLVM, and of improving its multithreading ability to scale to thousands of cores,[140] while ordinary implementations suffer from the global interpreter lock.
"Why Python's Integer Division Floors". 24 August 2010. Archived from the original on 5 June 2020. Retrieved 25 August 2010.
"The Python Language Reference, section 3.3. New-style and classic classes, for release 2.7.1". Archived from the original on 26 October 2012. Retrieved 12 January 2011.
^ Cannon, Brett. "Guido, Some Guys, and a Mailing List: How Python is Developed". python.org. Python Software Foundation. Archived from the original on 1 June 2009. Retrieved 27 June 2009.
^ "About getting started with writing geoprocessing scripts". ArcGIS Desktop Help 9.2. Environmental Systems Research Institute. 17 November 2006. Archived from the original on 5 June 2020. Retrieved 11 February 2012.
^ Kupries, Andreas; Fellows, Donal K. (14 September 2000). "TIP #3: TIP Format". tcl.tk. Tcl Developer Xchange. Archived from the original on 13 July 2017. Retrieved 24 November 2008.
Cannon, Brett. "Guido, Some Guys, and a Mailing List: How Python is Developed". python.org. Python Software Foundation. Archived from the original on 1 June 2009. Retrieved 27 June 2009.
"Download Python". Python.org. Archived from the original on 8 December 2020. Retrieved 13 December 2020.
^ "CoffeeScript". coffeescript.org. Archived from the original on 12 June 2020. Retrieved 3 July 2018.
Tools that can generate documentation for Python API include pydoc (available as part of the standard library), Sphinx, Pdoc and its forks, Doxygen and Graphviz, among others.[172]
"Python-for-EV3". LEGO Education. Archived from the original on 7 June 2020. Retrieved 17 April 2019.
^ "An Interview with Guido van Rossum". Oreilly.com. Archived from the original on 16 July 2014. Retrieved 24 November 2008.
"Python Language Guide (v1.0)". Google Documents List Data API v1.0. Archived from the original on 15 July 2010.
Due to Python's extensive mathematics library, and the third-party library NumPy that further extends the native capabilities, it is frequently used as a scientific scripting language to aid in problems such as numerical data processing and manipulation.[116][117]
Esterbrook, Charles. "Acknowledgements". cobra-language.com. Cobra Language. Archived from the original on 8 February 2008. Retrieved 7 April 2010.
"platform — Access to underlying platform's identifying data — Python 3.10.4 documentation"
Rauschmayer, Axel. "Chapter 3: The Nature of JavaScript; Influences". O'Reilly, Speaking JavaScript. Archived from the original on 26 December 2018. Retrieved 15 May 2015.
"google/grumpy". 10 April 2020. Archived from the original on 15 April 2020. Retrieved 25 March 2020 – via GitHub.
^ "Python Insider: Python releases 3.10.7, 3.9.14, 3.8.14, and 3.7.14 are now available". pythoninsider.blogspot.com. 7 September 2022. Retrieved 16 September 2022.
"Contracts for Python". Archived from the original on 15 June 2020. Retrieved 24 September 2011.
^ "jasc psp9components". Archived from the original on 19 March 2008.
^ Taft, Darryl K. (5 March 2007). "Python Slithers into Systems". eWeek.com. Ziff Davis Holdings. Archived from the original on 13 August 2021. Retrieved 24 September 2011.
Lundh, Fredrik. "Call By Object". effbot.org. Archived from the original on 23 November 2019. Retrieved 21 November 2017. replace "CLU" with "Python", "record" with "instance", and "procedure" with "function or method", and you get a pretty accurate description of Python's object model.
Swift, a programming language developed by Apple, has some Python-inspired syntax.[218]
An empirical study found that scripting languages, such as Python, are more productive than conventional languages, such as C and Java, for programming problems involving string manipulation and search in a dictionary, and determined that memory consumption was often "better than Java and not much worse than C or C++".[178]
^ "Tornado: Facebook's Real-Time Web Framework for Python – Facebook for Developers". Facebook for Developers. Archived from the original on 19 February 2019. Retrieved 19 June 2018.
^ "Nuitka Home | Nuitka Home". nuitka.net. Archived from the original on 30 May 2020. Retrieved 18 August 2017.
^ Shell, Scott (17 June 2014). "An introduction to Python for scientific computing" (PDF). Archived (PDF) from the original on 4 February 2019. Retrieved 3 February 2019.
^ "Python Language Guide (v1.0)". Google Documents List Data API v1.0. Archived from the original on 15 July 2010.
Bugfix releases,[167] which introduce no new features, occur about every 3 months and are made when a sufficient number of bugs have been fixed upstream since the last release. Security vulnerabilities are also patched in these releases. The third and final part of the version number is incremented.[167]
"Programming languages: Why Python 4.0 might never arrive, according to its creator"
^ Schemenauer, Neil; Peters, Tim; Hetland, Magnus Lie (18 May 2001). "PEP 255 – Simple Generators". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 5 June 2020. Retrieved 9 February 2012.
^ van Rossum, Guido (5 April 2006). "PEP 3000 – Python 3000". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 3 March 2016. Retrieved 27 June 2009.
"CVE – CVE-2021-3177". cve.mitre.org. Archived from the original on 27 February 2021. Retrieved 26 February 2021.
Simionato, Michele. "The Python 2.3 Method Resolution Order". Python Software Foundation. Archived from the original on 20 August 2020. Retrieved 29 July 2014. The C3 method itself has nothing to do with Python, since it was invented by people working on Dylan and it is described in a paper intended for lispers
"PEP 465 – A dedicated infix operator for matrix multiplication". python.org. Archived from the original on 29 May 2020. Retrieved 3 July 2018.
^ "Swift Evolution Process". Swift Programming Language Evolution repository on GitHub. 18 February 2020. Archived from the original on 27 April 2020. Retrieved 27 April 2020.
^ "3.3. Special method names". The Python Language Reference. Python Software Foundation. Archived from the original on 15 December 2018. Retrieved 27 June 2009.
^ "test — Regression tests package for Python — Python 3.7.13 documentation". docs.python.org. Retrieved 17 May 2022.
"PEP 483 – The Theory of Type Hints". Python.org. Archived from the original on 14 June 2020. Retrieved 14 June 2018.
Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0.[36] Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.[37]
^ "Is Python a good language for beginning programmers?". General Python FAQ. Python Software Foundation. Archived from the original on 24 October 2012. Retrieved 21 March 2007.
^ "4. Built-in Types — Python 3.6.3rc1 documentation". python.org. Archived from the original on 14 June 2020. Retrieved 1 October 2017.
^ "Plans for optimizing Python". Google Project Hosting. 15 December 2009. Archived from the original on 11 April 2016. Retrieved 24 September 2011.
In Python, a distinction between expressions and statements is rigidly enforced, in contrast to languages such as Common Lisp, Scheme, or Ruby. This leads to duplicating some functionality. For example:
Other shells, including IDLE and IPython, add further abilities such as improved auto-completion, session state retention, and syntax highlighting.
^ Clark, Robert (26 April 2019). "How to be Pythonic and why you should care". Medium. Archived from the original on 13 August 2021. Retrieved 20 January 2021.
^ "division". python.org. Archived from the original on 20 July 2006. Retrieved 30 July 2014.
As of November 2022,[update] Python 3.11.0 is the current stable release. Notable changes from 3.10 include increased program execution speed and improved error reporting.[64]
^ "Python 0.9.1 part 01/21". alt.sources archives. Archived from the original on 11 August 2021. Retrieved 11 August 2021.
^ "Download Python for Other Platforms". Python.org. Archived from the original on 27 November 2020. Retrieved 4 December 2020.
^ Rauschmayer, Axel. "Chapter 3: The Nature of JavaScript; Influences". O'Reilly, Speaking JavaScript. Archived from the original on 26 December 2018. Retrieved 15 May 2015.
"Python Developer's Guide — Python Developer's Guide". devguide.python.org. Archived from the original on 9 November 2020. Retrieved 17 December 2019.
^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}"General Python FAQ — Python 3.9.2 documentation". docs.python.org. Archived from the original on 24 October 2012. Retrieved 28 March 2021.
^ a b Kuchling, Andrew M. (22 December 2006). "Interview with Guido van Rossum (July 1998)". amk.ca. Archived from the original on 1 May 2007. Retrieved 12 March 2012. I'd spent a summer at DEC's Systems Research Center, which introduced me to Modula-2+; the Modula-3 final report was being written there at about the same time. What I learned there later showed up in Python's exception handling, modules, and the fact that methods explicitly contain 'self' in their parameter list. String slicing came from Algol-68 and Icon.
Kernighan, Brian W.; Ritchie, Dennis M. (1988). The C Programming Language (2nd ed.). p. 206.
^ Fehily, Chris (2002). Python. Peachpit Press. p. xv. ISBN 9780201748840. Archived from the original on 17 July 2017. Retrieved 9 May 2017.
MyHDL is a Python-based hardware description language (HDL), that converts MyHDL code to Verilog or VHDL code.
Clark, Robert (26 April 2019). "How to be Pythonic and why you should care". Medium. Archived from the original on 13 August 2021. Retrieved 20 January 2021.
^ corbet (24 October 2022). "Python 3.11 released [LWN.net]". lwn.net. Retrieved 15 November 2022.
Brython,[142] Transcrypt[143][144] and Pyjs (latest release in 2012) compile Python to JavaScript.
Martelli, Alex; Ravenscroft, Anna; Ascher, David (2005). Python Cookbook, 2nd Edition. O'Reilly Media. p. 230. ISBN 978-0-596-00797-3. Archived from the original on 23 February 2020. Retrieved 14 November 2015.
^ "8. Errors and Exceptions — Python 3.12.0a0 documentation". docs.python.org. Retrieved 9 May 2022.
^ Langa, Łukasz (24 March 2022). "Python Insider: Python 3.10.4 and 3.9.12 are now available out of schedule". Python Insider. Retrieved 19 April 2022.
"PEP 0465 – A dedicated infix operator for matrix multiplication"
"Groovy – the birth of a new dynamic language for the Java platform"
"Python on the Nokia N900". Stochastic Geometry. 29 April 2010. Archived from the original on 20 June 2019. Retrieved 9 July 2015.
"PEP 206 – Python Advanced Library". Python.org. Archived from the original on 5 May 2021. Retrieved 11 October 2021.
This page was last edited on 13 March 2023, at 19:11 (UTC).
"Python 2.5 internals" (PDF). Archived (PDF) from the original on 6 August 2012. Retrieved 19 April 2011.
Python allows boolean expressions with multiple equality relations in a manner that is consistent with general use in mathematics. For example, the expression a < b < c tests whether a is less than b and b is less than c.[112] C-derived languages interpret this expression differently: in C, the expression would first evaluate a < b, resulting in 0 or 1, and that result would then be compared with c.[113]
The yield statement, which returns a value from a generator function (and also an operator); used to implement coroutines
"TIOBE Index". TIOBE. Retrieved 3 January 2023. The TIOBE Programming Community index is an indicator of the popularity of programming languages Updated as required.
^ "mypy – Optional Static Typing for Python". Archived from the original on 6 June 2020. Retrieved 28 January 2017.
"10 Reasons Python Rocks for Research (And a Few Reasons it Doesn't) – Hoyt Koepke"
"What's New in Python 3.8". Archived from the original on 8 June 2020. Retrieved 14 October 2019.
"Guido van Rossum Stepping Down from Role as Python's Benevolent Dictator For Life"
Langa, Łukasz (17 May 2022). "Python Insider: Python 3.9.13 is now available". Python Insider. Retrieved 21 May 2022.
"Quotes about Python". Python Software Foundation. Archived from the original on 3 June 2020. Retrieved 8 January 2012.
^ "Frequently asked questions". Godot Engine documentation. Archived from the original on 28 April 2021. Retrieved 10 May 2021.
^ Blake, Troy (18 January 2021). "TIOBE Index for January 2021". Technology News and Information by SeniorDBA. Archived from the original on 21 March 2021. Retrieved 26 February 2021.
^ "Python 3.12.0 alpha 6 released". 7 March 2023. Retrieved 8 March 2023.
The class statement, which executes a block of code and attaches its local namespace to a class, for use in object-oriented programming
"15. Floating Point Arithmetic: Issues and Limitations — Python 3.8.3 documentation". docs.python.org. Archived from the original on 6 June 2020. Retrieved 6 June 2020. Almost all machines today (November 2000) use IEEE-754 floating point arithmetic, and almost all platforms map Python floats to IEEE-754 "double precision".
van Rossum, Guido (5 June 2001). "PEP 7 – Style Guide for C Code". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 1 June 2020. Retrieved 24 November 2008.
"Pythran — Pythran 0.9.5 documentation". pythran.readthedocs.io. Archived from the original on 19 February 2020. Retrieved 25 March 2020.
^ Caudill, Barry (20 September 2005). "Modding Sid Meier's Civilization IV". Sid Meier's Civilization IV Developer Blog. Firaxis Games. Archived from the original on 2 December 2010. we created three levels of tools ... The next level offers Python and XML support, letting modders with more experience manipulate the game world and everything in it.
Norwitz, Neal (8 April 2002). "[Python-Dev] Release Schedules (was Stability & change)". Archived from the original on 15 December 2018. Retrieved 27 June 2009.
^ "OpenCV: OpenCV-Python Tutorials". docs.opencv.org. Archived from the original on 23 September 2020. Retrieved 14 September 2020.
"Python Insider: Python 3.10.4 and 3.9.12 are now available out of schedule"
^ Ring Team (4 December 2017). "Ring and other languages". ring-lang.net. ring-lang. Archived from the original on 25 December 2018. Retrieved 4 December 2017.
Francisco, Thomas Claburn in San. "Google's Grumpy code makes Python Go". www.theregister.com. Archived from the original on 7 March 2021. Retrieved 20 January 2021.
Python was conceived in the late 1980s[42] by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands as a successor to the ABC programming language, which was inspired by SETL,[43] capable of exception handling and interfacing with the Amoeba operating system.[13] Its implementation began in December 1989.[44] Van Rossum shouldered sole responsibility for the project, as the lead developer, until 12 July 2018, when he announced his "permanent vacation" from his responsibilities as Python's "benevolent dictator for life", a title the Python community bestowed upon him to reflect his long-term commitment as the project's chief decision-maker.[45] In January 2019, active Python core developers elected a five-member Steering Council to lead the project.[46][47]
Python's large standard library[118] provides tools suited to many tasks and is commonly cited as one of its greatest strengths. For Internet-facing applications, many standard formats and protocols such as MIME and HTTP are supported. It includes modules for creating graphical user interfaces, connecting to relational databases, generating pseudorandom numbers, arithmetic with arbitrary-precision decimals,[119] manipulating regular expressions, and unit testing.
Python's developers aim for it to be fun to use. This is reflected in its name—a tribute to the British comedy group Monty Python[76]—and in occasionally playful approaches to tutorials and reference materials, such as the use of the terms "spam" and "eggs" (a reference to a Monty Python sketch) in examples, instead of the often-used "foo" and "bar".[77][78]
"Changing the Python release cadence [LWN.net]". lwn.net. Archived from the original on 6 November 2019. Retrieved 6 November 2019.
Julia was designed to be "as usable for general programming as Python".[29]
^ "PyDBC: method preconditions, method postconditions and class invariants for Python". Archived from the original on 23 November 2019. Retrieved 24 September 2011.
Venners, Bill (13 January 2003). "The Making of Python". Artima Developer. Artima. Archived from the original on 1 September 2016. Retrieved 22 March 2007.
"Python Culture". ebeab. 21 January 2014. Archived from the original on 30 January 2014.
"PEP 0465 – A dedicated infix operator for matrix multiplication". python.org. Archived from the original on 4 June 2020. Retrieved 1 January 2016.
^ a b "Classes". The Python Tutorial. Python Software Foundation. Archived from the original on 23 October 2012. Retrieved 20 February 2012. It is a mixture of the class mechanisms found in C++ and Modula-3
van Rossum, Guido; Hettinger, Raymond (7 February 2003). "PEP 308 – Conditional Expressions". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 13 March 2016. Retrieved 13 July 2011.
"Why was Python created in the first place?". General Python FAQ. Python Software Foundation. Archived from the original on 24 October 2012. Retrieved 22 March 2007. I had extensive experience with implementing an interpreted language in the ABC group at CWI, and from working with this group I had learned a lot about language design. This is the origin of many Python features, including the use of indentation for statement grouping and the inclusion of very high-level data types (although the details are all different in Python).
^ van Rossum, Guido (20 January 2009). "A Brief Timeline of Python". The History of Python. Archived from the original on 5 June 2020. Retrieved 20 January 2009.
^ "Transcrypt: Anatomy of a Python to JavaScript Compiler". InfoQ. Archived from the original on 5 December 2020. Retrieved 20 January 2021.
Please consider expanding the lead to provide an accessible overview of all important aspects of the article.
^ Piatetsky, Gregory. "Python eats away at R: Top Software for Analytics, Data Science, Machine Learning in 2018: Trends and Analysis". KDnuggets. KDnuggets. Archived from the original on 15 November 2019. Retrieved 30 May 2018.
Beyond the Basic Stuff with Python: Best Practices for Writing Clean Code
Enhancement of the language corresponds with the development of the CPython reference implementation. The mailing list python-dev is the primary forum for the language's development. Specific issues were originally discussed in the Roundup bug tracker hosted at by the foundation.[160] In 2022, all issues and discussions were migrated to GitHub.[161] Development originally took place on a self-hosted source-code repository running Mercurial, until Python moved to GitHub in January 2017.[162]
^ Fairchild, Carlie (12 July 2018). "Guido van Rossum Stepping Down from Role as Python's Benevolent Dictator For Life". Linux Journal. Archived from the original on 13 July 2018. Retrieved 13 July 2018.
^ Lutz, Mark (2009). Learning Python: Powerful Object-Oriented Programming. O'Reilly Media, Inc. p. 17. ISBN 9781449379322. Archived from the original on 17 July 2017. Retrieved 9 May 2017.
^ "5.3. Tuples and Sequences — Python 3.7.1rc2 documentation". python.org. Archived from the original on 10 June 2020. Retrieved 17 October 2018.
^ Lundh, Fredrik. "Call By Object". effbot.org. Archived from the original on 23 November 2019. Retrieved 21 November 2017. replace "CLU" with "Python", "record" with "instance", and "procedure" with "function or method", and you get a pretty accurate description of Python's object model.
"Why is Python a dynamic language and also a strongly typed language – Python Wiki". wiki.python.org. Archived from the original on 14 March 2021. Retrieved 27 January 2021.
Blake, Troy (18 January 2021). "TIOBE Index for January 2021". Technology News and Information by SeniorDBA. Archived from the original on 21 March 2021. Retrieved 26 February 2021.
^ "Jython FAQ". www.jython.org. Archived from the original on 22 April 2021. Retrieved 22 April 2021.
Dean, Jeff; Monga, Rajat;et al. (9 November 2015). "TensorFlow: Large-scale machine learning on heterogeneous systems" (PDF). TensorFlow.org. Google Research. Archived (PDF) from the original on 20 November 2015. Retrieved 10 November 2015.
"About getting started with writing geoprocessing scripts". ArcGIS Desktop Help 9.2. Environmental Systems Research Institute. 17 November 2006. Archived from the original on 5 June 2020. Retrieved 11 February 2012.
Science education with SageMath, Innovative Computing in Science Education, archived from the original on 15 June 2020, retrieved 22 April 2019
Other just-in-time Python compilers have been developed, but are now unsupported:
Python is meant to be an easily readable language. Its formatting is visually uncluttered and often uses English keywords where other languages use punctuation. Unlike many other languages, it does not use curly brackets to delimit blocks, and semicolons after statements are allowed but rarely used. It has fewer syntactic exceptions and special cases than C or Pascal.[81]
Pyston is a variant of the Python runtime that uses just-in-time compilation to speed up the execution of Python programs.[138]
^ Lattner, Chris (3 June 2014). "Chris Lattner's Homepage". Chris Lattner. Archived from the original on 25 December 2018. Retrieved 3 June 2014. The Swift language is the product of tireless effort from a team of language experts, documentation gurus, compiler optimization ninjas, and an incredibly important internal dogfooding group who provided feedback to help refine and battle-test ideas. Of course, it also greatly benefited from the experiences hard-won by many other languages in the field, drawing ideas from Objective-C, Rust, Haskell, Ruby, Python, C#, CLU, and far too many others to list.
Prechelt, Lutz (14 March 2000). "An empirical comparison of C, C++, Java, Perl, Python, Rexx, and Tcl" (PDF). Archived (PDF) from the original on 3 January 2020. Retrieved 30 August 2013.
"Why must 'self' be used explicitly in method definitions and calls?". Design and History FAQ. Python Software Foundation. Archived from the original on 24 October 2012. Retrieved 19 February 2012.
GitHub – reddit-archive/reddit: historical code from reddit.com., The Reddit Archives, archived from the original on 1 June 2020, retrieved 20 March 2019
The floor division // operator was introduced. So 7//3 == 2, -7//3 == -3, 7.5//3 == 2.0 and -7.5//3 == -3.0. Adding from __future__ import division causes a module used in Python 2.7 to use Python 3.0 rules for division (see above).
"Projects". opensource.google. Archived from the original on 24 April 2020. Retrieved 25 March 2020.
"Red Hat Customer Portal – Access to 24x7 support and knowledge"
The major academic conference on Python is PyCon. There are also special Python mentoring programs, such as Pyladies.
^ "history [vmspython]". www.vmspython.org. Archived from the original on 2 December 2020. Retrieved 4 December 2020.
"Why is it called Python?". General Python FAQ. Docs.python.org. Archived from the original on 24 October 2012. Retrieved 3 January 2023.
^ Zadka, Moshe; van Rossum, Guido (11 March 2001). "PEP 237 – Unifying Long Integers and Integers". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 28 May 2020. Retrieved 24 September 2011.
"PYPL PopularitY of Programming Language index". pypl.github.io. Archived from the original on 14 March 2017. Retrieved 26 March 2021.
Since 2003, Python has consistently ranked in the top ten most popular programming languages in the TIOBE Programming Community Index where as of December 2022[update] it was the most popular language (ahead of C, C++, and Java).[40] It was selected Programming Language of the Year (for "the highest rise in ratings in a year") in 2007, 2010, 2018, and 2020 (the only language to have done so four times as of 2020[177]).
The eval() vs. exec() built-in functions (in Python 2, exec is a statement); the former is for expressions, the latter is for statements
Python can serve as a scripting language for web applications, e.g., via mod_wsgi for the Apache webserver.[189] With Web Server Gateway Interface, a standard API has evolved to facilitate these applications. Web frameworks like Django, Pylons, Pyramid, TurboGears, web2py, Tornado, Flask, Bottle, and Zope support developers in the design and maintenance of complex applications. Pyjs and IronPython can be used to develop the client-side of Ajax-based applications. SQLAlchemy can be used as a data mapper to a relational database. Twisted is a framework to program communications between computers, and is used (for example) by Dropbox.
"Installers for GIMP for Windows – Frequently Asked Questions". 26 July 2013. Archived from the original on 17 July 2013. Retrieved 26 July 2013.
LibreOffice includes Python and intends to replace Java with Python. Its Python Scripting Provider is a core feature[209] since Version 4.0 from 7 February 2013.
The syntax :=, called the "walrus operator", was introduced in Python 3.8. It assigns values to variables as part of a larger expression.[94]
"The Genie Programming Language Tutorial". Archived from the original on 1 June 2020. Retrieved 28 February 2020.
Eby, Phillip J. (7 December 2003). "PEP 333 – Python Web Server Gateway Interface v1.0". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 14 June 2020. Retrieved 19 February 2012.
"Immunity: Knowing You're Secure". Archived from the original on 16 February 2009.
Ruby's creator, Yukihiro Matsumoto, has said: "I wanted a scripting language that was more powerful than Perl, and more object-oriented than Python. That's why I decided to design my own language."[217]
Langa, Łukasz (24 March 2022). "Python Insider: Python 3.10.4 and 3.9.12 are now available out of schedule". Python Insider. Retrieved 19 April 2022.
^ "Python 3.11.2, Python 3.10.10 and 3.12.0 alpha 5 are available". 8 February 2023. Retrieved 8 February 2023.
^ "Installers for GIMP for Windows – Frequently Asked Questions". 26 July 2013. Archived from the original on 17 July 2013. Retrieved 26 July 2013.
^ "PEP 380". Python.org. Archived from the original on 4 June 2020. Retrieved 3 December 2012.
"test — Regression tests package for Python — Python 3.7.13 documentation". docs.python.org. Retrieved 17 May 2022.
"Sunsetting Python 2". Python.org. Archived from the original on 12 January 2020. Retrieved 22 September 2019.
^ "Python : the holy grail of programming". CERN Bulletin. CERN Publications (31/2006). 31 July 2006. Archived from the original on 15 January 2013. Retrieved 11 February 2012.
"Highlights: Python 2.5". Python.org. Archived from the original on 4 August 2019. Retrieved 20 March 2018.
"platform — Access to underlying platform's identifying data — Python 3.10.4 documentation". docs.python.org. Retrieved 17 May 2022.
PyS60 was a Python 2 interpreter for Series 60 mobile phones released by Nokia in 2005. It implemented many of the modules from the standard library and some additional modules for integrating with the Symbian operating system. The Nokia N900 also supports Python with GTK widget libraries, enabling programs to be written and run on the target device.[141]
"15 Ways Python Is a Powerful Force on the Web". Archived from the original on 11 May 2019. Retrieved 3 July 2018.
^ a b "TIOBE Index". TIOBE. Retrieved 3 January 2023. The TIOBE Programming Community index is an indicator of the popularity of programming languages Updated as required.
^ Millman, K. Jarrod; Aivazis, Michael (2011). "Python for Scientists and Engineers". Computing in Science and Engineering. 13 (2): 9–12. Bibcode:2011CSE....13b...9M. doi:10.1109/MCSE.2011.36. Archived from the original on 19 February 2019. Retrieved 7 July 2014.
^ van Rossum, Guido (22 April 2009). "Tail Recursion Elimination". Neopythonic.blogspot.be. Archived from the original on 19 May 2018. Retrieved 3 December 2012.
^ a b Aahz; Baxter, Anthony (15 March 2001). "PEP 6 – Bug Fix Releases". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 5 June 2020. Retrieved 27 June 2009.
^ "PEP 13 – Python Language Governance". Python.org. Archived from the original on 27 May 2021. Retrieved 25 August 2021.
"The State of Developer Ecosystem in 2020 Infographic". JetBrains: Developer Tools for Professionals and Teams. Archived from the original on 1 March 2021. Retrieved 5 March 2021.
"In Python, should I use else after a return in an if block?". Stack Overflow. Stack Exchange. 17 February 2011. Archived from the original on 20 June 2019. Retrieved 6 May 2011.
^ "PEP 483 – The Theory of Type Hints". Python.org. Archived from the original on 14 June 2020. Retrieved 14 June 2018.
^ Yegulalp, Serdar (16 January 2017). "Nim language draws from best of Python, Rust, Go, and Lisp". InfoWorld. Archived from the original on 13 October 2018. Retrieved 7 June 2020. Nim's syntax is strongly reminiscent of Python's, as it uses indented code blocks and some of the same syntax (such as the way if/elif/then/else blocks are constructed).
A common neologism in the Python community is pythonic, which has a wide range of meanings related to program style. "Pythonic" code may use Python idioms well, be natural or show fluency in the language, or conform with Python's minimalist philosophy and emphasis on readability. Code that is difficult to understand or reads like a rough transcription from another programming language is called unpythonic.[79][80]
"test — Regression tests package for Python — Python 3.7.13 documentation"
The del statement, which removes a variable—deleting the reference from the name to the value, and producing an error if the variable is referred to before it is redefined
"Python Insider: Python releases 3.10.7, 3.9.14, 3.8.14, and 3.7.14 are now available"
The raise statement, used to raise a specified exception or re-raise a caught exception
^ van Rossum, Guido (29 August 2000). "SETL (was: Lukewarm about range literals)". Python-Dev (Mailing list). Archived from the original on 14 July 2018. Retrieved 13 March 2011.
Shell, Scott (17 June 2014). "An introduction to Python for scientific computing" (PDF). Archived (PDF) from the original on 4 February 2019. Retrieved 3 February 2019.
^ Sweigart, Al (2020). Beyond the Basic Stuff with Python: Best Practices for Writing Clean Code. No Starch Press. p. 322. ISBN 978-1-59327-966-0. Archived from the original on 13 August 2021. Retrieved 7 July 2021.
The if statement, which conditionally executes a block of code, along with else and elif (a contraction of else-if)
^ "Extending and Embedding the Python Interpreter: Reference Counts". Docs.python.org. Archived from the original on 18 October 2012. Retrieved 5 June 2020. Since Python makes heavy use of malloc() and free(), it needs a strategy to avoid memory leaks as well as the use of freed memory. The chosen method is called reference counting.
"Built-in Types". Archived from the original on 14 June 2020. Retrieved 3 October 2019.
Aahz; Baxter, Anthony (15 March 2001). "PEP 6 – Bug Fix Releases". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 5 June 2020. Retrieved 27 June 2009.
^ "PYPL PopularitY of Programming Language index". pypl.github.io. Archived from the original on 14 March 2017. Retrieved 26 March 2021.
"PEP 8 – Style Guide for Python Code". Python.org. Archived from the original on 17 April 2019. Retrieved 26 March 2019.
CCP porkbelly (24 August 2010). "Stackless Python 2.7". EVE Community Dev Blogs. CCP Games. Archived from the original on 11 January 2014. Retrieved 11 January 2014. As you may know, EVE has at its core the programming language known as Stackless Python.
^ "Quotes about Python". Python Software Foundation. Archived from the original on 3 June 2020. Retrieved 8 January 2012.
^ "PEP 8 – Style Guide for Python Code". Python.org. Archived from the original on 17 April 2019. Retrieved 26 March 2019.
^ Prechelt, Lutz (14 March 2000). "An empirical comparison of C, C++, Java, Perl, Python, Rexx, and Tcl" (PDF). Archived (PDF) from the original on 3 January 2020. Retrieved 30 August 2013.
"Python Setup and Usage". Python Software Foundation. Archived from the original on 17 June 2020. Retrieved 10 January 2020.
^ "Sunsetting Python 2". Python.org. Archived from the original on 12 January 2020. Retrieved 22 September 2019.
^ van Rossum, Guido; Hettinger, Raymond (7 February 2003). "PEP 308 – Conditional Expressions". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 13 March 2016. Retrieved 13 July 2011.
^ Piotrowski, Przemyslaw (July 2006). "Build a Rapid Web Development Environment for Python Server Pages and Oracle". Oracle Technology Network. Oracle. Archived from the original on 2 April 2019. Retrieved 12 March 2012.
"The Computer Language Benchmarks Game". Archived from the original on 14 June 2020. Retrieved 30 April 2020.
Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.[33]
"Changelog — Python 3.9.0 documentation". docs.python.org. Archived from the original on 7 February 2021. Retrieved 8 February 2021.
^ Esterbrook, Charles. "Acknowledgements". cobra-language.com. Cobra Language. Archived from the original on 8 February 2008. Retrieved 7 April 2010.
^ "Python Culture". ebeab. 21 January 2014. Archived from the original on 30 January 2014.
^ a b "Why was Python created in the first place?". General Python FAQ. Python Software Foundation. Archived from the original on 24 October 2012. Retrieved 22 March 2007. I had extensive experience with implementing an interpreted language in the ABC group at CWI, and from working with this group I had learned a lot about language design. This is the origin of many Python features, including the use of indentation for statement grouping and the inclusion of very high-level data types (although the details are all different in Python).
Python also comes with an Integrated development environment (IDE) called IDLE, which is more beginner-oriented.
Python supports gradual typing.[102] Python's syntax allows specifying static types, but they are not checked in the default implementation, CPython. An experimental optional static type-checker, mypy, supports compile-time type checking.[103]
"Python Insider: Python 2.7.18, the last release of Python 2"
^ "Modulecounts". Modulecounts. 14 November 2022. Archived from the original on 26 June 2022.
3Syntax and semantics											Toggle Syntax and semantics subsection																					3.1Indentation																											3.2Statements and control flow																											3.3Expressions																											3.4Methods																											3.5Typing																											3.6Arithmetic operations
^ "Red Hat Customer Portal – Access to 24x7 support and knowledge". access.redhat.com. Archived from the original on 6 March 2021. Retrieved 26 February 2021.
Python strives for a simpler, less-cluttered syntax and grammar while giving developers a choice in their coding methodology. In contrast to Perl's "there is more than one way to do it" motto, Python embraces a "there should be one—and preferably only one—obvious way to do it"philosophy.[73] Alex Martelli, a Fellow at the Python Software Foundation and Python book author, wrote: "To describe something as 'clever' is not considered a compliment in the Python culture."[74]
CPython's public releases come in three types, distinguished by which part of the version number is incremented:
^ "PyDatalog". Archived from the original on 13 June 2020. Retrieved 22 July 2012.
Pyrex (latest release in 2010) and Shed Skin (latest release in 2013) compile to C and C++ respectively.
Beazley, David M. (2009). Python Essential Reference (4th ed.). p. 66. ISBN 9780672329784.
"GitHub – IronLanguages/ironpython3: Implementation of Python 3.x for .NET Framework that is built on top of the Dynamic Language Runtime". GitHub. Archived from the original on 28 September 2021.
Fortenberry, Tim (17 January 2003). "Industrial Light & Magic Runs on Python". Python Software Foundation. Archived from the original on 6 June 2020. Retrieved 11 February 2012.
Google's Grumpy (latest release in 2017) transpiles Python 2 to Go.[150][151][152]
^ Shafer, Daniel G. (17 January 2003). "Python Streamlines Space Shuttle Mission Design". Python Software Foundation. Archived from the original on 5 June 2020. Retrieved 24 November 2008.
^ GitHub – reddit-archive/reddit: historical code from reddit.com., The Reddit Archives, archived from the original on 1 June 2020, retrieved 20 March 2019
"An Interview with the Creator of Ruby". Linuxdevcenter.com. Archived from the original on 28 April 2018. Retrieved 3 December 2012.
^ "PEP 8100". Python Software Foundation. Archived from the original on 4 June 2020. Retrieved 4 May 2019.
Cython compiles (a superset of) Pythonto C (while the resulting code is also usable with Python and also e.g. C++).
"Classes". The Python Tutorial. Python Software Foundation. Archived from the original on 23 October 2012. Retrieved 20 February 2012. It is a mixture of the class mechanisms found in C++ and Modula-3
^ van Rossum, Guido (9 February 2006). "Language Design Is Not Just Solving Puzzles". Artima forums. Artima. Archived from the original on 17 January 2020. Retrieved 21 March 2007.
"PEP 8100". Python Software Foundation. Archived from the original on 4 June 2020. Retrieved 4 May 2019.
"Ada 83 Reference Manual (raise statement)". Archived from the original on 22 October 2019. Retrieved 7 January 2020.
"4. Built-in Types — Python 3.6.3rc1 documentation". python.org. Archived from the original on 14 June 2020. Retrieved 1 October 2017.
The for statement, which iterates over an iterable object, capturing each element to a local variable for use by the attached block
^ "What's New in Python 2.6 — Python v2.6.9 documentation". docs.python.org. Archived from the original on 23 December 2019. Retrieved 26 September 2015.
"Why is Python a dynamic language and also a strongly typed language – Python Wiki"
Conditional expressions are written as x if c else y[95] (different in order of operands from the c ? x : y operator common to many other languages).
"An Interview with Guido van Rossum". Oreilly.com. Archived from the original on 16 July 2014. Retrieved 24 November 2008.
^ "Gotchas for Python Users". boo.codehaus.org. Codehaus Foundation. Archived from the original on 11 December 2008. Retrieved 24 November 2008.
"Python 3.11.2, Python 3.10.10 and 3.12.0 alpha 5 are available"
^ "15 Ways Python Is a Powerful Force on the Web". Archived from the original on 11 May 2019. Retrieved 3 July 2018.
Python has various string literals:Delimited by single or double quote marks; unlike in Unix shells, Perl, and Perl-influenced languages, single and double quote marks work the same. Both use the backslash (\) as an escape character. String interpolation became available in Python 3.6 as "formatted string literals".[98]Triple-quoted (beginning and ending with three single or double quote marks), which may span multiple lines and function like here documents in shells, Perl, and Ruby.Raw string varieties, denoted by prefixing the string literal with r. Escape sequences are not interpreted; hence raw strings are useful where literal backslashes are common, such as regular expressions and Windows-style paths. (Compare "@-quoting" in C#.)
^ Simionato, Michele. "The Python 2.3 Method Resolution Order". Python Software Foundation. Archived from the original on 20 August 2020. Retrieved 29 July 2014. The C3 method itself has nothing to do with Python, since it was invented by people working on Dylan and it is described in a paper intended for lispers
^ "What's New in Python 3.8". Archived from the original on 8 June 2020. Retrieved 14 October 2019.
^ "Python-for-EV3". LEGO Education. Archived from the original on 7 June 2020. Retrieved 17 April 2019.
Warsaw, Barry; Hylton, Jeremy; Goodger, David (13 June 2000). "PEP 1 – PEP Purpose and Guidelines". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 6 June 2020. Retrieved 19 April 2011.
Older projects (or not to be used with Python 3.x and latest syntax):
Methods on objects are functions attached to the object's class; the syntax instance.method(argument) is, for normal methods and functions, syntactic sugar for Class.method(instance, argument). Python methods have an explicit self parameter to access instance data, in contrast to the implicit self (or this) in some other object-oriented programming languages (e.g., C++, Java, Objective-C, Ruby).[99] Python also provides methods, often called dunder methods (due to their names beginning and ending with double-underscores), to allow user-defined classes to modify how they are handled by native operations including length, comparison, in arithmetic operations and type conversion.[100]
"PEP 7 – Style Guide for C Code | peps.python.org". peps.python.org. Retrieved 28 April 2022.
van Rossum, Guido (1993). "An Introduction to Python for UNIX/C Programmers". Proceedings of the NLUUG Najaarsconferentie (Dutch UNIX Users Group). CiteSeerX 10.1.1.38.2023. even though the design of C is far from ideal, its influence on Python is considerable.
^ Jouppi, Norm. "Google supercharges machine learning tasks with TPU custom chip". Google Cloud Platform Blog. Archived from the original on 18 May 2016. Retrieved 19 May 2016.
^ "What is Sugar?". Sugar Labs. Archived from the original on 9 January 2009. Retrieved 11 February 2012.
"Type hinting for Python". LWN.net. 24 December 2014. Archived from the original on 20 June 2019. Retrieved 5 May 2015.
^ "google/grumpy". 10 April 2020. Archived from the original on 15 April 2020. Retrieved 25 March 2020 – via GitHub.
Kupries, Andreas; Fellows, Donal K. (14 September 2000). "TIP #3: TIP Format". tcl.tk. Tcl Developer Xchange. Archived from the original on 13 July 2017. Retrieved 24 November 2008.
Delimited by single or double quote marks; unlike in Unix shells, Perl, and Perl-influenced languages, single and double quote marks work the same. Both use the backslash (\) as an escape character. String interpolation became available in Python 3.6 as "formatted string literals".[98]
As of 14 November 2022,[update] the Python Package Index (PyPI), the official repository for third-party Python software, contains over 415,000[121] packages with a wide range of functionality, including:
Its design offers some support for functional programming in the Lisp tradition. It has filter,mapandreduce functions; list comprehensions, dictionaries, sets, and generator expressions.[71] The standard library has two modules (itertools and functools) that implement functional tools borrowed from Haskell and Standard ML.[72]
Python's developers strive to avoid premature optimization and reject patches to non-critical parts of the CPython reference implementation that would offer marginal increases in speed at the cost of clarity.[75] When speed is important, a Python programmer can move time-critical functions to extension modules written in languages such as C; or use PyPy, a just-in-time compiler. Cython is also available, which translates a Python script into C and makes direct C-level API calls into the Python interpreter.
Statements cannot be a part of an expression—so list and other comprehensions or lambda expressions, all being expressions, cannot contain statements. A particular case is that an assignment statement such as a = 1 cannot form part of the conditional expression of a conditional statement. This has the advantage of avoiding a classic C error of mistaking an assignment operator = for an equality operator == in conditions: if (c = 1) { ... } is syntactically valid (but probably unintended) C code, but if c = 1: ... causes a syntax error in Python.
"Extending and Embedding the Python Interpreter: Reference Counts". Docs.python.org. Archived from the original on 18 October 2012. Retrieved 5 June 2020. Since Python makes heavy use of malloc() and free(), it needs a strategy to avoid memory leaks as well as the use of freed memory. The chosen method is called reference counting.
"PEP 465 – A dedicated infix operator for matrix multiplication"
^ "2to3 – Automated Python 2 to 3 code translation". docs.python.org. Archived from the original on 4 June 2020. Retrieved 2 February 2021.
Murri, Riccardo (2013). Performance of Python runtimes on a non-numeric scientific code. European Conference on Python in Science (EuroSciPy). arXiv:1404.6388. Bibcode:2014arXiv1404.6388M.
Python uses the + operator for string concatenation. Python uses the * operator for duplicating a string a specified number of times.
Peterson, Benjamin (20 April 2020). "Python Insider: Python 2.7.18, the last release of Python 2". Python Insider. Archived from the original on 26 April 2020. Retrieved 27 April 2020.
^ a b Peters, Tim (19 August 2004). "PEP 20 – The Zen of Python". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 26 December 2018. Retrieved 24 November 2008.
^ a b "PEP 498 – Literal String Interpolation". python.org. Archived from the original on 15 June 2020. Retrieved 8 March 2017.
"itertools — Functions creating iterators for efficient looping — Python 3.7.1 documentation". docs.python.org. Archived from the original on 14 June 2020. Retrieved 22 November 2016. This module implements a number of iterator building blocks inspired by constructs from APL, Haskell, and SML.
Millman, K. Jarrod; Aivazis, Michael (2011). "Python for Scientists and Engineers". Computing in Science and Engineering. 13 (2): 9–12. Bibcode:2011CSE....13b...9M. doi:10.1109/MCSE.2011.36. Archived from the original on 19 February 2019. Retrieved 7 July 2014.
The Python → 11l → C++ transpiler[149] compiles a subset of Python 3 to C++ (C++17).
^ a b Zadka, Moshe; van Rossum, Guido (11 March 2001). "PEP 238 – Changing the Division Operator". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 28 May 2020. Retrieved 23 October 2013.
Lattner, Chris (3 June 2014). "Chris Lattner's Homepage". Chris Lattner. Archived from the original on 22 December 2015. Retrieved 3 June 2014. I started work on the Swift Programming Language in July of 2010. I implemented much of the basic language structure, with only a few people knowing of its existence. A few other (amazing) people started contributing in earnest late in 2011, and it became a major focus for the Apple Developer Tools group in July 2013 [...] drawing ideas from Objective-C, Rust, Haskell, Ruby, Python, C#, CLU, and far too many others to list.
van Rossum, Guido (20 January 2009). "A Brief Timeline of Python". The History of Python. Archived from the original on 5 June 2020. Retrieved 20 January 2009.
Python's development practices have also been emulated by other languages. For example, the practice of requiring a document describing the rationale for, and issues surrounding, a change to the language (in Python, a PEP) is also used in Tcl,[219] Erlang,[220] and Swift.[221]
^ "Why is it called Python?". General Python FAQ. Docs.python.org. Archived from the original on 24 October 2012. Retrieved 3 January 2023.
^ "An Interview with the Creator of Ruby". Linuxdevcenter.com. Archived from the original on 28 April 2018. Retrieved 3 December 2012.
Raw string varieties, denoted by prefixing the string literal with r. Escape sequences are not interpreted; hence raw strings are useful where literal backslashes are common, such as regular expressions and Windows-style paths. (Compare "@-quoting" in C#.)
^ "speed comparison between CPython and Pypy". Speed.pypy.org. Archived from the original on 10 May 2021. Retrieved 3 December 2012.
^ Norwitz, Neal (8 April 2002). "[Python-Dev] Release Schedules (was Stability & change)". Archived from the original on 15 December 2018. Retrieved 27 June 2009.
The import and from statements, used to import modules whose functions or variables can be used in the current program
The while statement, which executes a block of code as long as its condition is true
CoffeeScript, a programming language that cross-compiles to JavaScript, has Python-inspired syntax.
^ "Changelog — Python 3.9.0 documentation". docs.python.org. Archived from the original on 7 February 2021. Retrieved 8 February 2021.
^ "PEP 206 – Python Advanced Library". Python.org. Archived from the original on 5 May 2021. Retrieved 11 October 2021.
"Modulecounts". Modulecounts. 14 November 2022. Archived from the original on 26 June 2022.
"Guido, Some Guys, and a Mailing List: How Python is Developed"
^ "re — Regular expression operations — Python 3.10.6 documentation". docs.python.org. Retrieved 6 September 2022. This module provides regular expression matching operations similar to those found in Perl.
^ "Natural Language Toolkit — NLTK 3.5b1 documentation". www.nltk.org. Archived from the original on 13 June 2020. Retrieved 10 April 2020.
"CPython byte code". Docs.python.org. Archived from the original on 5 June 2020. Retrieved 16 February 2016.
The Cain Gang Ltd. "Python Metaclasses: Who? Why? When?" (PDF). Archived from the original (PDF) on 30 May 2009. Retrieved 27 June 2009.
^ "Download Python". Python.org. Archived from the original on 8 December 2020. Retrieved 13 December 2020.
^ "Myths about indentation in Python". Secnetix.de. Archived from the original on 18 February 2018. Retrieved 19 April 2011.
MicroPython and CircuitPython are Python 3 variants optimized for microcontrollers, including Lego Mindstorms EV3.[137]
Boo uses indentation, a similar syntax, and a similar object model.[210]
The assert statement, used in debugging to check for conditions that should apply
"More Control Flow Tools". Python 3 documentation. Python Software Foundation. Archived from the original on 4 June 2016. Retrieved 24 July 2015. By popular demand, a few features commonly found in functional programming languages like Lisp have been added to Python. With the lambda keyword, small anonymous functions can be created.
Fehily, Chris (2002). Python. Peachpit Press. p. xv. ISBN 9780201748840. Archived from the original on 17 July 2017. Retrieved 9 May 2017.
^ "Changing the Python release cadence [LWN.net]". lwn.net. Archived from the original on 6 November 2019. Retrieved 6 November 2019.
Cinder is a performance-oriented fork of CPython 3.8 that contains a number of optimizations including bytecode inline caching, eager evaluation of coroutines, a method-at-a-time JIT, and an experimental bytecode compiler.[139]
CPython is the reference implementation of Python. It is written in C, meeting the C89 standard (Python 3.11 uses C11[123]) with several select C99 features. CPython includes its own C extensions, but third-party extensions are not limited to older C versions—e.g. they can be implemented with C11 or C++.[124][125]) It compiles Python programs into an intermediate bytecode[126] which is then executed by its virtual machine.[127] CPython is distributed with a large standard library written in a mixture of C and native Python, and is available for many platforms, including Windows (starting with Python 3.9, the Python installer deliberately fails to install on Windows 7 and 8;[128][129] Windows XP was supported until Python 3.5) and most modern Unix-like systems, including macOS (and Apple M1 Macs, since Python 3.9.1, with experimental installer) and unofficial support for e.g. VMS.[130] Platform portability was one of its earliest priorities.[131] (During Python 1 and 2 development, even OS/2 and Solaris were supported,[132] but support has since been dropped for many platforms.)
^ "PEP 466 – Network Security Enhancements for Python 2.7.x". python.org. Archived from the original on 4 June 2020. Retrieved 9 January 2017.
Python's development is conducted largely through the Python Enhancement Proposal (PEP) process, the primary mechanism for proposing major new features, collecting community input on issues, and documenting Python design decisions.[158] Python coding style is covered in PEP 8.[159] Outstanding PEPs are reviewed and commented on by the Python community and the steering council.[158]
Psyco is a discontinued just-in-time specializing compiler that integrates with CPython and transforms bytecode to machine code at runtime. The emitted code is specialized for certain data types and is faster than the standard Python code. Psyco does not support Python 2.7 or later.
"PEP 623 – Remove wstr from Unicode". Python.org. Archived from the original on 5 March 2021. Retrieved 14 February 2021.
"PEP 380". Python.org. Archived from the original on 4 June 2020. Retrieved 3 December 2012.
^ "Immunity: Knowing You're Secure". Archived from the original on 16 February 2009.
^ "pprint — Data pretty printer — Python 3.11.0 documentation". docs.python.org. Archived from the original on 22 January 2021. Retrieved 5 November 2022. stuff = ['spam', 'eggs', 'lumberjack', 'knights', 'ni']
^ "About Python". Python Software Foundation. Archived from the original on 20 April 2012. Retrieved 24 April 2012., second section "Fans of Python use the phrase "batteries included" to describe the standard library, which covers everything from asynchronous processing to zip files."
^ "Code Style — The Hitchhiker's Guide to Python". docs.python-guide.org. Archived from the original on 27 January 2021. Retrieved 20 January 2021.
^ "Proposals: iterators and generators [ES4 Wiki]". wiki.ecmascript.org. Archived from the original on 20 October 2007. Retrieved 24 November 2008.
van Rossum, Guido (5 April 2006). "PEP 3000 – Python 3000". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 3 March 2016. Retrieved 27 June 2009.
"OpenCV: OpenCV-Python Tutorials". docs.opencv.org. Archived from the original on 23 September 2020. Retrieved 14 September 2020.
"PyAIML 0.8.5 : Python Package Index". Pypi.python.org. Retrieved 17 July 2013.
"About Python". Python Software Foundation. Archived from the original on 20 April 2012. Retrieved 24 April 2012., second section "Fans of Python use the phrase "batteries included" to describe the standard library, which covers everything from asynchronous processing to zip files."
"Tornado: Facebook's Real-Time Web Framework for Python – Facebook for Developers". Facebook for Developers. Archived from the original on 19 February 2019. Retrieved 19 June 2018.
"15 Ways Python Is a Powerful Force on the Web"
"Python 3.12.0 alpha 6 released". 7 March 2023. Retrieved 8 March 2023.
^ Rossum, Guido Van (20 January 2009). "The History of Python: A Brief Timeline of Python". The History of Python. Archived from the original on 5 June 2020. Retrieved 5 March 2021.
"Download Python for Other Platforms". Python.org. Archived from the original on 27 November 2020. Retrieved 4 December 2020.
Kuchling, A. M.; Zadka, Moshe (16 October 2000). "What's New in Python 2.0". Python Software Foundation. Archived from the original on 23 October 2012. Retrieved 11 February 2012.
Strachan, James (29 August 2003). "Groovy – the birth of a new dynamic language for the Java platform". Archived from the original on 5 April 2007. Retrieved 11 June 2007.
Russell, Stuart J. & Norvig, Peter (2009). Artificial Intelligence: A Modern Approach (3rd ed.). Upper Saddle River, NJ: Prentice Hall. ISBN 978-0-13-604259-4.
Python's design and philosophy have influenced many other programming languages:
Python's name is derived from the British comedy group Monty Python, whom Python creator Guido van Rossum enjoyed while developing the language. Monty Python references appear frequently in Python code and culture;[173] for example, the metasyntactic variables often used in Python literature are spam and eggs instead of the traditional foo and bar.[173][174] The official Python documentation also contains various references to Monty Python routines.[175][176]
"Gotchas for Python Users". boo.codehaus.org. Codehaus Foundation. Archived from the original on 11 December 2008. Retrieved 24 November 2008.
Kuchling, A. M. "Functional Programming HOWTO". Python v2.7.2 documentation. Python Software Foundation. Archived from the original on 24 October 2012. Retrieved 9 February 2012. List comprehensions and generator expressions [...] are a concise notation for such operations, borrowed from the functional programming language Haskell.
Python is a multi-paradigm programming language. Object-oriented programming and structured programming are fully supported, and many of their features support functional programming and aspect-oriented programming (including metaprogramming[65] and metaobjects).[66] Many other paradigms are supported via extensions, including design by contract[67][68] and logic programming.[69]
Numba uses LLVM to compile a subset of Python to machine code.
^ a b Hettinger, Raymond (30 January 2002). "PEP 289 – Generator Expressions". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 14 June 2020. Retrieved 19 February 2012.
Schemenauer, Neil; Peters, Tim; Hetland, Magnus Lie (18 May 2001). "PEP 255 – Simple Generators". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 5 June 2020. Retrieved 9 February 2012.
^ Peterson, Benjamin (20 April 2020). "Python Insider: Python 2.7.18, the last release of Python 2". Python Insider. Archived from the original on 26 April 2020. Retrieved 27 April 2020.
Holth, Moore (30 March 2014). "PEP 0441 – Improving Python ZIP Application Support". Archived from the original on 26 December 2018. Retrieved 12 November 2015.
Anonymous functions are implemented using lambda expressions; however, there may be only one expression in each body.
^ "Download Python". Python.org. Archived from the original on 8 August 2018. Retrieved 24 May 2021.
The division between integers produces floating-point results. The behavior of division has changed significantly over time:[108]
"The Python Language Reference, section 3.3. New-style and classic classes, for release 2.7.1"
"PyDatalog". Archived from the original on 13 June 2020. Retrieved 22 July 2012.
However, at some point order needs to be introduced to the heap to achieve the desired running time. In particular, degrees of nodes (here degree means the number of direct children) are kept quite low: every node has degree at most log n and the size of a subtree rooted in a node of degree k is at least Fk+2, where Fk is the kth Fibonacci number. This is achieved by the rule that we can cut at most one child of each non-root node. When a second child is cut, the node itself needs to be cut from its parent and becomes the root of a new tree (see Proof of degree bounds, below). The number of trees is decreased in the operation delete minimum, where trees are linked together.
Operation find minimum is now trivial because we keep the pointer to the node containing it. It does not change the potential of the heap, therefore both actual and amortized cost are constant.
As mentioned above, merge is implemented simply by concatenating the lists of tree roots of the two heaps. This can be done in constant time and the potential does not change, leading again to constant amortized time.
^ Iacono, John (2000), "Improved upper bounds for pairing heaps", Proc. 7th Scandinavian Workshop on Algorithm Theory (PDF), Lecture Notes in Computer Science, vol. 1851, Springer-Verlag, pp. 63–77, arXiv:1110.4428, CiteSeerX 10.1.1.748.7812, doi:10.1007/3-540-44985-X_5, ISBN 3-540-67690-2
Pettie, Seth (2005). Towards a Final Analysis of Pairing Heaps (PDF). FOCS '05 Proceedings of the 46th Annual IEEE Symposium on Foundations of Computer Science. pp. 174–183. CiteSeerX 10.1.1.549.471. doi:10.1109/SFCS.2005.75. ISBN 0-7695-2468-0.
Here are time complexities[8] of various heap data structures. Function names assume a min-heap.For the meaning of "O(f)" and "Θ(f)" see Big O notation.
"Binomial Heap | Brilliant Math & Science Wiki". brilliant.org. Retrieved 2019-09-30.
Although the total running time of a sequence of operations starting with an empty structure is bounded by the bounds given above, some (very few) operations in the sequence can take very long to complete (in particular delete and delete minimum have linear running time in the worst case). For this reason Fibonacci heaps and other amortized data structures may not be appropriate for real-time systems. It is possible to create a data structure which has the same worst-case performance as the Fibonacci heap has amortized performance. One such structure, the Brodal queue,[4] is, in the words of the creator, "quite complicated" and "[not] applicable in practice." Created in 2012, the strict Fibonacci heap[5] is a simpler (compared to Brodal's) structure with the same worst-case bounds. Despite having simpler structure, experiments show that in practice the strict Fibonacci heap performs slower than more complicated Brodal queue and also slower than basic Fibonacci heap.[6][7] The run-relaxed heaps of Driscoll et al. give good worst-case performance for all Fibonacci heap operations except merge.
Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. (1990). Introduction to Algorithms (1st ed.). MIT Press and McGraw-Hill. ISBN 0-262-03141-8.
Using Fibonacci heaps for priority queues improves the asymptotic running time of important algorithms, such as Dijkstra's algorithm for computing the shortest path between two nodes in a graph, compared to the same algorithm using other slower priority queue data structures.
Brodal and Okasaki later describe a persistent variant with the same bounds except for decrease-key, which is not supported.Heaps with n elements can be constructed bottom-up in O(n).[14]
^ Fredman, Michael Lawrence (July 1999). "On the Efficiency of Pairing Heaps and Related Data Structures" (PDF). Journal of the Association for Computing Machinery. 46 (4): 473–501. doi:10.1145/320211.320214.
For the Fibonacci heap, the find-minimum operation takes constant (O(1)) amortized time.[1] The insert and decrease key operations also work in constant amortized time.[2] Deleting an element (most often used in the special case of deleting the minimum element) works in O(log n) amortized time, where n is the size of the heap.[2] This means that starting from an empty data structure, any sequence of a insert and decrease key operations and b delete operations would take O(a + b log n) worst case time, where n is the maximum heap size. In a binary or binomial heap, such a sequence of operations would take O((a + b) log n) time. A Fibonacci heap is thus better than a binary or binomial heap when b is smaller than a by a non-constant factor. It is also possible to merge two Fibonacci heaps in constant amortized time, improving on the logarithmic merge time of a binomial heap, and improving on binary heaps which cannot handle merges efficiently.
Fredman, Michael Lawrence (July 1999). "On the Efficiency of Pairing Heaps and Related Data Structures" (PDF). Journal of the Association for Computing Machinery. 46 (4): 473–501. doi:10.1145/320211.320214.
This page was last edited on 2 March 2023, at 16:33 (UTC).
Operation extract minimum (same as delete minimum) operates in three phases. First we take the root containing the minimum element and remove it. Its children will become roots of new trees. If the number of children was d, it takes time O(d) to process all new roots and the potential increases by d−1. Therefore, the amortized running time of this phase is O(d) = O(log n).
In the third phase we check each of the remaining roots and find the minimum. This takes O(log n) time and the potential does not change. The overall amortized running time of extract minimum is therefore O(log n).
^ Brodal and Okasaki later describe a persistent variant with the same bounds except for decrease-key, which is not supported.Heaps with n elements can be constructed bottom-up in O(n).[14]
^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001) [1990]. "Chapter 20: Fibonacci Heaps". Introduction to Algorithms (2nd ed.). MIT Press and McGraw-Hill. pp. 476–497. ISBN 0-262-03293-7. Third edition p. 518.
^ Brodal, Gerth S. (1996), "Worst-Case Efficient Priority Queues" (PDF), Proc. 7th Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 52–58
In computer science, a Fibonacci heap is a data structure for priority queue operations, consisting of a collection ofheap-ordered trees. It has a better amortized running time than many other priority queue data structures including the binary heap and binomial heap. Michael L. Fredman and Robert E. Tarjan developed Fibonacci heaps in 1984 and published them in a scientific journal in 1987. Fibonacci heaps are named after the Fibonacci numbers, which are used in their running time analysis.
Takaoka, Tadao (1999), Theory of 2–3 Heaps (PDF), p. 12
Fredman, Michael Lawrence; Tarjan, Robert E. (July 1987). "Fibonacci heaps and their uses in improved network optimization algorithms" (PDF). Journal of the Association for Computing Machinery. 34 (3): 596–615. CiteSeerX 10.1.1.309.8927. doi:10.1145/28869.28874.
^ Haeupler, Bernhard; Sen, Siddhartha; Tarjan, Robert E. (November 2011). "Rank-pairing heaps" (PDF). SIAM J. Computing. 40 (6): 1463–1485. doi:10.1137/100785351.
^ a b c d e f g h i
^ Fredman, Michael L.; Sedgewick, Robert; Sleator, Daniel D.; Tarjan, Robert E. (1986). "The pairing heap: a new form of self-adjusting heap" (PDF). Algorithmica. 1 (1–4): 111–129. doi:10.1007/BF01840439. S2CID 23664143.
^ Takaoka, Tadao (1999), Theory of 2–3 Heaps (PDF), p. 12
^ Gerth Stølting Brodal (1996), "Worst-Case Efficient Priority Queues", Proc. 7th ACM-SIAM Symposium on Discrete Algorithms, Society for Industrial and Applied Mathematics: 52–58, CiteSeerX 10.1.1.43.8133, ISBN 0-89871-366-8
A routine induction proves that 1+∑i=0dFi=Fd+2{\displaystyle 1+\sum _{i=0}^{d}F_{i}=F_{d+2}} for any d≥0{\displaystyle d\geq 0}, which gives the desired lower bound on size(x).
^ Brodal, G. S. L.; Lagogiannis, G.; Tarjan, R. E. (2012). Strict Fibonacci heaps (PDF). Proceedings of the 44th symposium on Theory of Computing - STOC '12. p. 1177. doi:10.1145/2213977.2214082. ISBN 978-1-4503-1245-5.
^ Goodrich, Michael T.; Tamassia, Roberto (2004). "7.3.6. Bottom-Up Heap Construction". Data Structures and Algorithms in Java (3rd ed.). pp. 338–341. ISBN 0-471-46983-1.
Brodal, G. S. L.; Lagogiannis, G.; Tarjan, R. E. (2012). Strict Fibonacci heaps (PDF). Proceedings of the 44th symposium on Theory of Computing - STOC '12. p. 1177. doi:10.1145/2213977.2214082. ISBN 978-1-4503-1245-5.
^ Lower bound of Ω(log⁡log⁡n),{\displaystyle \Omega (\log \log n),}[11] upper bound of O(22log⁡log⁡n).{\displaystyle O(2^{2{\sqrt {\log \log n}}}).}[12]
De-recursived and memory efficient C implementation of Fibonacci heap (free/libre software, CeCILL-B license)
Larkin, Daniel; Sen, Siddhartha; Tarjan, Robert (2014). "A Back-to-Basics Empirical Study of Priority Queues". Proceedings of the Sixteenth Workshop on Algorithm Engineering and Experiments: 61–72. arXiv:1403.0252. Bibcode:2014arXiv1403.0252L. doi:10.1137/1.9781611973198.7. ISBN 978-1-61197-319-8. S2CID 15216766.
Lower bound of Ω(log⁡log⁡n),{\displaystyle \Omega (\log \log n),}[11] upper bound of O(22log⁡log⁡n).{\displaystyle O(2^{2{\sqrt {\log \log n}}}).}[12]
^ Pettie, Seth (2005). Towards a Final Analysis of Pairing Heaps (PDF). FOCS '05 Proceedings of the 46th Annual IEEE Symposium on Foundations of Computer Science. pp. 174–183. CiteSeerX 10.1.1.549.471. doi:10.1109/SFCS.2005.75. ISBN 0-7695-2468-0.
^ a b c d e f g h i Amortized time.
A Fibonacci heap is a collection of trees satisfying the minimum-heap property, that is, the key of a child is always greater than or equal to the key of the parent. This implies that the minimum key is always at the root of one of the trees. Compared with binomial heaps, the structure of a Fibonacci heap is more flexible. The trees do not have a prescribed shape and in the extreme case the heap can have every element in a separate tree. This flexibility allows some operations to be executed in a lazy manner, postponing the work for later operations. For example, merging heaps is done simply by concatenating the two lists of trees, and operation decrease key sometimes cuts a node from its parent and forms a new tree.
Fredman, Michael L.; Sedgewick, Robert; Sleator, Daniel D.; Tarjan, Robert E. (1986). "The pairing heap: a new form of self-adjusting heap" (PDF). Algorithmica. 1 (1–4): 111–129. doi:10.1007/BF01840439. S2CID 23664143.
where t is the number of trees in the Fibonacci heap, and m is the number of marked nodes. A node is marked if at least one of its children was cut since this node was made a child of another node (all roots are unmarked).The amortized time for an operation is given by the sum of the actual time and c times the difference in potential, where c is a constant (chosen to match the constant factors in the O notation for the actual time).
Finally, operation delete can be implemented simply by decreasing the key of the element to be deleted to minus infinity, thus turning it into the minimum of the whole heap. Then we call extract minimum to remove it. The amortized running time of this operation is O(log n).
^ Mrena, Michal; Sedlacek, Peter; Kvassay, Miroslav (June 2019). "Practical Applicability of Advanced Implementations of Priority Queues in Finding Shortest Paths". 2019 International Conference on Information and Digital Technologies (IDT). Zilina, Slovakia: IEEE: 335–344. doi:10.1109/DT.2019.8813457. ISBN 9781728114019. S2CID 201812705.
^ a b Larkin, Daniel; Sen, Siddhartha; Tarjan, Robert (2014). "A Back-to-Basics Empirical Study of Priority Queues". Proceedings of the Sixteenth Workshop on Algorithm Engineering and Experiments: 61–72. arXiv:1403.0252. Bibcode:2014arXiv1403.0252L. doi:10.1137/1.9781611973198.7. ISBN 978-1-61197-319-8. S2CID 15216766.
However to complete the extract minimum operation, we need to update the pointer to the root with minimum key. Unfortunately there may be up to n roots we need to check. In the second phase we therefore decrease the number of roots by successively linking together roots of the same degree. When two roots u and v have the same degree, we make one of them a child of the other so that the one with the smaller key remains the root. Its degree will increase by one. This is repeated until every root has a different degree. To find trees of the same degree efficiently we use an array of length O(log n) in which we keep a pointer to one root of each degree. When a second root is found of the same degree, the two are linked and the array is updated. The actual running time is O(log n + m) where m is the number of roots at the beginning of the second phase. At the end we will have at most O(log n) roots (because each has a different degree). Therefore, the difference in the potential function from before this phase to after it is: O(log n) − m, and the amortized running time is then at most O(log n + m) + c(O(log n) − m). With a sufficiently large choice of c, this simplifies to O(log n).
"On the Efficiency of Pairing Heaps and Related Data Structures"
^ "Binomial Heap | Brilliant Math & Science Wiki". brilliant.org. Retrieved 2019-09-30.
As a result of a relaxed structure, some operations can take a long time while others are done very quickly. For the amortized running time analysis, we use the potential method, in that we pretend that very fast operations take a little bit longer than they actually do. This additional time is then later combined and subtracted from the actual running time of slow operations. The amount of time saved for later use is measured at any given moment by a potential function. The potential of a Fibonacci heap is given by
^ a b c d Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. (1990). Introduction to Algorithms (1st ed.). MIT Press and McGraw-Hill. ISBN 0-262-03141-8.
Haeupler, Bernhard; Sen, Siddhartha; Tarjan, Robert E. (November 2011). "Rank-pairing heaps" (PDF). SIAM J. Computing. 40 (6): 1463–1485. doi:10.1137/100785351.
Brodal, Gerth S. (1996), "Worst-Case Efficient Priority Queues" (PDF), Proc. 7th Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 52–58
To allow fast deletion and concatenation, the roots of all trees are linked using a circular doubly linked list. The children of each node are also linked using such a list. For each node, we maintain its number of children and whether the node is marked. Moreover, we maintain a pointer to the root containing the minimum key.
"Fibonacci heaps and their uses in improved network optimization algorithms"
Brodal, Gerth Stølting; Lagogiannis, George; Tarjan, Robert E. (2012). Strict Fibonacci heaps (PDF). Proceedings of the 44th symposium on Theory of Computing - STOC '12. pp. 1177–1184. CiteSeerX 10.1.1.233.1740. doi:10.1145/2213977.2214082. ISBN 978-1-4503-1245-5.
Gerth Stølting Brodal (1996), "Worst-Case Efficient Priority Queues", Proc. 7th ACM-SIAM Symposium on Discrete Algorithms, Society for Industrial and Applied Mathematics: 52–58, CiteSeerX 10.1.1.43.8133, ISBN 0-89871-366-8
Base case: If x has height 0, then d = 0, and size(x) = 1 = F2.
Inductive case:Suppose x has positive height and degree d > 0.Let y1, y2, ..., yd be the children of x, indexed in order of the times they were most recently made children of x (y1 being the earliest and yd the latest), and let c1, c2, ..., cd be their respective degrees.We claim that ci ≥ i-2 for each i with 2 ≤ i ≤ d: Just before yi was made a child of x, y1,...,yi−1 were already children of x, and so x had degree at least i−1 at that time.Since trees are combined only when the degrees of their roots are equal, it must have been that yi also had degree at least i-1 at the time it became a child of x.From that time to the present, yi can only have lost at most one child (as guaranteed by the marking process), and so its current degree ci is at least i−2.This proves the claim.
They are not as efficient in practice when compared with the theoretically less efficient forms of heaps. In their simplest version they require storage and manipulation of four pointers per node, whereas only two or three pointers per node are needed in other structures, such as Binary heap, Binomial heap, Pairing heap, Brodal queue and Rank pairing heap.
Mrena, Michal; Sedlacek, Peter; Kvassay, Miroslav (June 2019). "Practical Applicability of Advanced Implementations of Priority Queues in Finding Shortest Paths". 2019 International Conference on Information and Digital Technologies (IDT). Zilina, Slovakia: IEEE: 335–344. doi:10.1109/DT.2019.8813457. ISBN 9781728114019. S2CID 201812705.
Thus, the root of each tree in a heap has one unit of time stored. This unit of time can be used later to link this tree with another tree at amortized time 0. Also, each marked node has two units of time stored. One can be used to cut the node from its parent. If this happens, the node becomes a root and the second unit of time will remain stored in it as in any other root.
Although Fibonacci heaps look very efficient, they have the following two drawbacks:[3]
^ a b c Fredman, Michael Lawrence; Tarjan, Robert E. (July 1987). "Fibonacci heaps and their uses in improved network optimization algorithms" (PDF). Journal of the Association for Computing Machinery. 34 (3): 596–615. CiteSeerX 10.1.1.309.8927. doi:10.1145/28869.28874.
Operation decrease key will take the node, decrease the key and if the heap property becomes violated (the new key is smaller than the key of the parent), the node is cut from its parent. If the parent is not a root, it is marked. If it has been marked already, it is cut as well and its parent is marked. We continue upwards until we reach either the root or an unmarked node. Now we set the minimum pointer to the decreased value if it is the new minimum. In the process we create some number, say k, of new trees. Each of these new trees except possibly the first one was marked originally but as a root it will become unmarked. One node can become marked. Therefore, the number of marked nodes changes by −(k − 1) + 1 = − k + 2. Combining these 2 changes, the potential changes by 2(−k + 2) + k = −k + 4. The actual time to perform the cutting was O(k), therefore (again with a sufficiently large choice of c) the amortized running time is constant.
Since the heights of all the yi are strictly less than that of x, we can apply the inductive hypothesis to them to get size(yi) ≥ Fci+2 ≥ F(i−2)+2 = Fi.The nodes x and y1 each contribute at least 1 to size(x), and so we have
Goodrich, Michael T.; Tamassia, Roberto (2004). "7.3.6. Bottom-Up Heap Construction". Data Structures and Algorithms in Java (3rd ed.). pp. 338–341. ISBN 0-471-46983-1.
^ Brodal, Gerth Stølting; Lagogiannis, George; Tarjan, Robert E. (2012). Strict Fibonacci heaps (PDF). Proceedings of the 44th symposium on Theory of Computing - STOC '12. pp. 1177–1184. CiteSeerX 10.1.1.233.1740. doi:10.1145/2213977.2214082. ISBN 978-1-4503-1245-5.
.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001) [1990]. "Chapter 20: Fibonacci Heaps". Introduction to Algorithms (2nd ed.). MIT Press and McGraw-Hill. pp. 476–497. ISBN 0-262-03293-7. Third edition p. 518.
Consider any node x somewhere in the heap (x need not be the root of one of the main trees).Define size(x) to be the size of the tree rooted at x (the number of descendants of x, including x itself).We prove by induction on the height of x (the length of a longest simple path from x to a descendant leaf), that size(x) ≥ Fd+2, where d is the degree of x.
Operation insert works by creating a new heap with one element and doing merge. This takes constant time, and the potential increases by one, because the number of trees increases. The amortized cost is thus still constant.
Fibonacci heaps have a reputation for being slow in practice[18] due to large memory consumption per node and high constant factors on all operations.Recent experimental results suggest that Fibonacci heaps are more efficient in practice than most of its later derivatives, including quake heaps, violation heaps, strict Fibonacci heaps, rank pairing heaps, but less efficient than either pairing heaps or array-based heaps.[7]
The amortized performance of a Fibonacci heap depends on the degree (number of children) of any tree root being O(log n), where n is the size of the heap.Here we show that the size of the (sub)tree rooted at any node x of degree d in the heap must have size at least Fd+2, where Fk is the kth Fibonacci number.The degree bound follows from this and the fact (easily proved by induction) that Fd+2≥φd{\displaystyle F_{d+2}\geq \varphi ^{d}} for all integers d≥0{\displaystyle d\geq 0}, where φ=(1+5)/2≐1.618{\displaystyle \varphi =(1+{\sqrt {5}})/2\doteq 1.618}.(We then have n≥Fd+2≥φd{\displaystyle n\geq F_{d+2}\geq \varphi ^{d}}, and taking the log to base φ{\displaystyle \varphi } of both sides gives d≤logφ⁡n{\displaystyle d\leq \log _{\varphi }n} as required.)
Iacono, John (2000), "Improved upper bounds for pairing heaps", Proc. 7th Scandinavian Workshop on Algorithm Theory (PDF), Lecture Notes in Computer Science, vol. 1851, Springer-Verlag, pp. 63–77, arXiv:1110.4428, CiteSeerX 10.1.1.748.7812, doi:10.1007/3-540-44985-X_5, ISBN 3-540-67690-2
Prokash Sinha (Dec 1, 2004). "A Memory-Efficient Doubly Linked List". Linux Journal.
The XOR operation cancels addr(B) appearing twice in the equation and all we are left with is the addr(D).
The R2 register always contains the XOR of the address of current item C with the address of the predecessor item P: C⊕P. The Link fields in the records contain the XOR of the left and right successor addresses, say L⊕R. XOR of R2 (C⊕P) with the current link field (L⊕R) yields C⊕P⊕L⊕R.
Computer systems have increasingly cheap and plentiful memory, therefore storage overhead is not generally an overriding issue outside specialized embedded systems. Where it is still desirable to reduce the overhead of a linked list, unrolling provides a more practical approach (as well as other advantages, such as increasing cache performance and speeding random access).
General-purpose debugging tools cannot follow the XOR chain, making debugging more difficult;[2]
XOR linked lists do not provide some of the important advantages of doubly linked lists, such as the ability to delete a node from the list knowing only its address or the ability to insert a new node before or after an existing node when knowing only the address of the existing node.
When traversing the list from left to right: supposing the cursor is at C, the previous item, B, may be XORed with the value in the link field (B⊕D). The address for D will then be obtained and list traversal may resume. The same pattern applies in the other direction.
While traversing the list the address of the previously accessed node is needed to calculate the next node's address and the pointers will be unreadable if one isn't traversing the list—for example, if the pointer to a list item was contained in another data structure;
^ "c++ associative containers based on the XOR scapegoat tree". Retrieved 5 November 2021.
Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: "XOR linked list" – news · newspapers · books · scholar · JSTOR
The subtraction linked list is also special in that the entire list can be relocated in memory without needing any patching of pointer values, since adding a constant offset to each address in the list will not require any changes to the values stored in the link fields. (See also serialization.) This is an advantage over both XOR linked lists and traditional linked lists.
Most garbage collection schemes do not work with data structures that do not contain literal pointers;
Two XOR operations suffice to do the traversal from one item to the next, the same instructions sufficing in both cases. Consider a list with items {…B C D…} and with R1 and R2 being registers containing, respectively, the address of the current (say C) list item and a work register containing the XOR of the current address with the previous address (say C⊕D).Cast as System/360 instructions:
This page was last edited on 13 May 2022, at 20:22 (UTC).
End of list is signified by imagining a list item at address zero placed adjacent to an end point, as in {0 A B C…}. The link field at A would be 0⊕B. An additional instruction is needed in the above sequence after the two XOR operations to detect a zero result in developing the address of the current item,
Gadbois, David;et al. "GC [garbage collection] FAQ – draft". Retrieved 5 December 2018.
^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}"XOR Linked List - A Memory Efficient Doubly Linked List | Set 1 - GeeksforGeeks". GeeksforGeeks. 2011-05-23. Retrieved 2018-10-29.
An XOR linked list is a type of data structure used in computer programming. It takes advantage of the bitwise XOR operation to decrease storage requirements for doubly linked lists.
.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}"XOR Linked List - A Memory Efficient Doubly Linked List | Set 1 - GeeksforGeeks". GeeksforGeeks. 2011-05-23. Retrieved 2018-10-29.
^ Gadbois, David;et al. "GC [garbage collection] FAQ – draft". Retrieved 5 December 2018.
An XOR linked list compresses the same information into one address field by storing the bitwise XOR (here denoted by ⊕) of the address for previous and the address for next in one field:
The price for the decrease in memory usage is an increase in code complexity, making maintenance more expensive;
The key is the first operation, and the properties of XOR:
If the predecessor had been R, the P(=R) and Rcancel, leaving C⊕L.
"XOR Linked List - A Memory Efficient Doubly Linked List | Set 1 - GeeksforGeeks"
"c++ associative containers based on the XOR scapegoat tree". Retrieved 5 November 2021.
Find sources: "XOR linked list" – news · newspapers · books · scholar · JSTOR
A list end point can be made reflective by making the link pointer be zero. A zero pointer is a mirror. (The XOR of the left and right neighbor addresses, being the same, is zero.)
This kind of list has exactly the same properties as the XOR linked list, except that a zero link field is not a "mirror". The address of the next node in the list is given by subtracting the previous node's address from the current node's link field.
If the predecessor was L, the P(=L) and L cancel outleaving C⊕R.
4Variations											Toggle Variations subsection																					4.1Addition linked list																											4.2Subtraction linked list																											4.3Binary search tree
The XOR linked list concept can be generalized to XOR binary search trees.[3]
This kind of list differs from the standard "traditional" XOR linked list in that the instruction sequences needed to traverse the list forwards is different from the sequence needed to traverse the list in reverse. The address of the next node, going forwards, is given by adding the link field to the previous node's address; the address of the preceding node is given by subtracting the link field from the next node's address.
Not all languages support type conversion between pointers and integers, XOR on pointers is not defined in some contexts;
An ordinary doubly linked list stores addresses of the previous and next list items in each list node, requiring two address fields:
The underlying principle of the XOR linked list can be applied to any reversible binary operation. Replacing XOR by addition or subtraction gives slightly different, but largely equivalent, formulations:
David Kanter (September 25, 2010). "Intel's Sandy Bridge Microarchitecture - Instruction Decode and uop Cache". Real World Technologies.
"Inside Intel Core Microarchitecture and Smart Memory Access". Intel. 2006. p. 5. Archived from the original (PDF) on 2011-12-29. Retrieved 2012-01-26.
The placement policy decides where in the cache a copy of a particular entry of main memory will go. If the placement policy is free to choose any entry in the cache to hold the copy, the cache is called fully associative. At the other extreme, if each entry in the main memory can go in just one place in the cache, the cache is direct-mapped. Many caches implement a compromise in which each entry in the main memory can go to any one of N places in the cache, and are described as N-way set associative.[10] For example, the level-1 data cache in an AMD Athlon is two-way set associative, which means that any particular location in main memory can be cached in either of two locations in the level-1 data cache.
^ Leon Gu; Dipti Motiani (October 2003). "Trace Cache" (PDF). Retrieved 2013-10-06.
Cache reads are the most common CPU operation that takes more than a single cycle. Program execution time tends to be very sensitive to the latency of a level-1 data cache hit. A great deal of design effort, and often power and silicon area are expended making the caches as fast as possible.
Virtually indexed, virtually tagged (VIVT) caches use the virtual address for both the index and the tag. This caching scheme can result in much faster lookups, since the MMU does not need to be consulted first to determine the physical address for a given virtual address. However, VIVT suffers from aliasing problems, where several different virtual addresses may refer to the same physical address. The result is that such addresses would be cached separately despite referring to the same memory, causing coherency problems. Although solutions to this problem exist[26] they do not work for standard coherence protocols. Another problem is homonyms, where the same virtual address maps to several different physical addresses. It is not possible to distinguish these mappings merely by looking at the virtual index itself, though potential solutions include: flushing the cache after a context switch, forcing address spaces to be non-overlapping, tagging the virtual address with an address space ID (ASID). Additionally, there is a problem that virtual-to-physical mappings can change, which would require flushing cache lines, as the VAs would no longer be valid.All these issues are absent if tags use physical addresses (VIPT).
In a separate cache structure, instructions and data are cached separately, meaning that a cache line is used to cache either instructions or data, but not both; various benefits have been demonstrated with separate data and instruction translation lookaside buffers.[50]In a unified structure, this constraint is not present, and cache lines can be used to cache both instructions and data.
^ David Kanter (September 25, 2010). "Intel's Sandy Bridge Microarchitecture - Instruction Decode and uop Cache". Real World Technologies.
The original Pentium 4 processor had a four-way set associative L1 data cache of 8 KiB in size, with 64-byte cache blocks. Hence, there are 8 KiB / 64 = 128 cache blocks. The number of sets is equal to the number of cache blocks divided by the number of ways of associativity, what leads to 128 / 4 = 32 sets, and hence 25 = 32 different indices. There are 26 = 64 possible offsets. Since the CPU address is 32 bits wide, this implies 32 - 5 - 6 = 21 bits for the tag field.
Harvey G. Cragon."Memory systems and pipelined processors".1996. ISBN 0-86720-474-5, ISBN 978-0-86720-474-2."Chapter 4.1: Cache Addressing, Virtual or Real"p. 209[1]
Another advantage of inclusive caches is that the larger cache can use larger cache lines, which reduces the size of the secondary cache tags. (Exclusive caches require both caches to have the same size cache lines, so that cache lines can be swapped on a L1 miss, L2 hit.) If the secondary cache is an order of magnitude larger than the primary, and the cache data is an order of magnitude larger than the cache tags, this tag area saved can be comparable to the incremental area needed to store the L1 cache data in the L2.[53]
Sanjeev Jahagirdar; Varghese George; Inder Sodhi; Ryan Wells (2012). "Power Management of the Third Generation Intel Core Micro Architecture formerly codenamed Ivy Bridge" (PDF). hotchips.org. p. 18. Archived from the original (PDF) on 2020-07-29. Retrieved 2015-12-16.
^ Sumner, F. H.; Haley, G.; Chenh, E. C. Y. (1962). "The Central Control Unit of the 'Atlas' Computer". Information Processing 1962. IFIP Congress Proceedings. Vol. Proceedings of IFIP Congress 62. Spartan.
As the x86 microprocessors reached clock rates of 20 MHz and above in the 386, small amounts of fast cache memory began to be featured in systems to improve performance. This was because the DRAM used for main memory had significant latency, up to 120 ns, as well as refresh cycles. The cache was constructed from more expensive, but significantly faster, SRAM memory cells, which at the time had latencies around 10–25 ns. The early caches were external to the processor and typically located on the motherboard in the form of eight or nine DIP devices placed in sockets to enable the cache as an optional extra or upgrade feature.
^ a b "Products (Formerly Crystal Well)". Intel. Retrieved 2013-09-15.
For the purposes of the present discussion, there are three important features of address translation:
Chen, J. Bradley; Borg, Anita; Jouppi, Norman P. (1992). "A Simulation Based Study of TLB Performance". SIGARCH Computer Architecture News. 20 (2): 114–123. doi:10.1145/146628.139708.
^ a b C. Kozyrakis. "Lecture 3: Advanced Caching Techniques" (PDF). Archived from the original (PDF) on September 7, 2012.
8.1History																								8.1.1First TLB implementations																											8.1.2First instruction cache																											8.1.3First data cache																											8.1.4In 68k microprocessors																											8.1.5In x86 microprocessors																											8.1.6In ARM microprocessors																											8.1.7Current research
A CPU cache is a hardware cache used by the central processing unit (CPU) of a computer to reduce the average cost (time or energy) to access data from the main memory.[1] A cache is a smaller, faster memory, located closer to a processor core, which stores copies of the data from frequently used main memory locations.Most CPUs have a hierarchy of multiple cache levels (L1, L2, often L3, and rarely even L4), with different instruction-specific and data-specific caches at level 1.[2] The cache memory is typically implemented with static random-access memory (SRAM), in modern CPUs by far the largest part of them by chip area, but SRAM is not always used for all levels (of I- or D-cache), or even any level, sometimes some latter or all levels are implemented with eDRAM.
While all of the cache blocks in a particular cache are the same size and have the same associativity, typically the "lower-level" caches (called Level 1 cache) have a smaller number of blocks, smaller block size, and fewer blocks in a set, but have very short access times. "Higher-level" caches (i.e. Level 2 and above) have progressively larger numbers of blocks, larger block size, more blocks in a set, and relatively longer access times, but are still much faster than main memory.
The solution is to have the operating system attempt to assign different physical color pages to different virtual colors, a technique called page coloring. Although the actual mapping from virtual to physical color is irrelevant to system performance, odd mappings are difficult to keep track of and have little benefit, so most approaches to page coloring simply try to keep physical and virtual page colors the same.
^ Oded Lempel (2013-07-28). "2nd Generation Intel Core Processor Family: Intel Core i7, i5 and i3" (PDF). hotchips.org. p. 7–10,31–45. Archived from the original (PDF) on 2020-07-29. Retrieved 2014-01-21.
^ "Product Fact Sheet: Accelerating 5G Network Infrastructure, from the Core to the Edge". Intel Newsroom (Press release). Retrieved 2020-04-12. L1 cache of 32KB/core, L2 cache of 4.5MB per 4-core cluster and shared LLC cache up to 15MB.
"Survey of CPU Cache-Based Side-Channel Attacks: Systematic Analysis, Security Models, and Countermeasures"
C. Kozyrakis. "Lecture 3: Advanced Caching Techniques" (PDF). Archived from the original (PDF) on September 7, 2012.
^ James E. Thornton (October 1964), "Parallel operation in the control data 6600" (PDF), Proc. of the October 27-29, 1964, fall joint computer conference, part II: very high speed computer systems
Various techniques have been employed to keep the CPU busy during this time, including out-of-order execution in which the CPU attempts to execute independent instructions after the instruction that is waiting for the cache miss data.Another technology, used by many processors, is simultaneous multithreading (SMT), which allows an alternate thread to use the CPU core while the first thread waits for required CPU resources to become available.
One of the advantages of a direct-mapped cache is that it allows simple and fast speculation. Once the address has been computed, the one cache index which might have a copy of that location in memory is known. That cache entry can be read, and the processor can continue to work with that data before it finishes checking that the tag actually matches the requested address.
^ Kaxiras, Stefanos; Ros, Alberto (2013). A New Perspective for Efficient Virtual-Cache Coherence. 40th International Symposium on Computer Architecture (ISCA). pp. 535–547. CiteSeerX 10.1.1.307.9125. doi:10.1145/2485922.2485968. ISBN 9781450320795. S2CID 15434231.
Timothy Roscoe; Andrew Baumann (2009-03-03). "Advanced Operating Systems Caches and TLBs (263-3800-00L)" (PDF). systems.ethz.ch. Archived from the original (PDF) on 2011-10-07. Retrieved 2016-02-14.
Marking some memory ranges as non-cacheable can improve performance, by avoiding caching of memory regions that are rarely re-accessed. This avoids the overhead of loading something into the cache without having any reuse.Cache entries may also be disabled or locked depending on the context.
"Micro-Architecture]". Skewed-associative caches have been shown to have two major advantages over conventional set-associative caches.
The simplest cache is a virtually indexed direct-mapped cache. The virtual address is calculated with an adder, the relevant portion of the address extracted and used to index an SRAM, which returns the loaded data. The data is byte aligned in a byte shifter, and from there is bypassed to the next operation. There is no need for any tag checking in the inner loop –in fact, the tags need not even be read. Later in the pipeline, but before the load instruction is retired, the tag for the loaded data must be read, and checked against the virtual address to make sure there was a cache hit. On a miss, the cache is updated with the requested cache line and the pipeline is restarted.
The K8 keeps the instruction and data caches coherent in hardware, which means that a store into an instruction closely following the store instruction will change that following instruction. Other processors, like those in the Alpha and MIPS family, have relied on software to keep the instruction cache coherent. Stores are not guaranteed to show up in the instruction stream until a program calls an operating system facility to ensure coherency.
^ N.P.Jouppi. "Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers." - 17th Annual International Symposium on Computer Architecture, 1990. Proceedings., doi:10.1109/ISCA.1990.134547
^ "Explanation of the L1 and L2 Cache". amecomputers.com. Archived from the original on 2014-07-14. Retrieved 2014-06-09.
On power-up, the hardware sets all the valid bits in all the caches to "invalid".Some systems also set a valid bit to "invalid" at other times, such as when multi-master bus snooping hardware in the cache of one processor hears an address broadcast from some other processor, and realizes that certain data blocks in the local cache are now stale and should be marked invalid.
David Kanter (August 26, 2010). "AMD's Bulldozer Microarchitecture - Memory Subsystem Continued". Real World Technologies.
John L. Hennessy; David A. Patterson (2011). Computer Architecture: A Quantitative Approach. p. B-9. ISBN 978-0-12-383872-8.
Cache performance measurement has become important in recent times where the speed gap between the memory performance and the processor performance is increasing exponentially. The cache was introduced to reduce this speed gap. Thus knowing how well the cache is able to bridge the gap in the speed of processor and memory becomes important, especially in high-performance systems. The cache hit rate and the cache miss rate play an important role in determining this performance. To improve the cache performance, reducing the miss rate becomes one of the necessary steps among other steps. Decreasing the access time to the cache also gives a boost to its performance.
3Associativity											Toggle Associativity subsection																					3.1Direct-mapped cache																											3.2Two-way set associative cache																											3.3Speculative execution																											3.4Two-way skewed associative cache																											3.5Pseudo-associative cache																											3.6Multicolumn cache
Physically indexed, physically tagged (PIPT) caches use the physical address for both the index and the tag. While this is simple and avoids problems with aliasing, it is also slow, as the physical address must be looked up (which could involve a TLB miss and access to main memory) before that address can be looked up in the cache.
As the latency difference between main memory and the fastest cache has become larger, some processors have begun to utilize as many as three levels of on-chip cache.Price-sensitive designs used this to pull the entire cache hierarchy on-chip, but by the 2010s some of the highest-performance designs returned to having large off-chip caches, which is often implemented in eDRAM and mounted on a multi-chip module, as a fourth cache level. In rare cases, such as in the mainframe CPU IBM z15 (2019), all levels down to L1 are implemented by eDRAM, replacing SRAM entirely (for cache,SRAM is still used for registers). The ARM-based Apple M1 has a 192 KiB L1 cache for each of the four high-performance cores, an unusually large amount; however the four high-efficiency cores only have 128 KiB.
^ Chen, J. Bradley; Borg, Anita; Jouppi, Norman P. (1992). "A Simulation Based Study of TLB Performance". SIGARCH Computer Architecture News. 20 (2): 114–123. doi:10.1145/146628.139708.
Understanding CPU caching and performance –an article on Ars Technica by Jon Stokes
Alpha 21164 (1995) has 1 to 64 MiB off-chip L3 cache.
Sumner, F. H.; Haley, G.; Chenh, E. C. Y. (1962). "The Central Control Unit of the 'Atlas' Computer". Information Processing 1962. IFIP Congress Proceedings. Vol. Proceedings of IFIP Congress 62. Spartan.
This allows full-speed operation with a much smaller cache than a traditional full-time instruction cache.
Smith, Alan Jay (September 1982). "Cache Memories" (PDF). Computing Surveys. 14 (3): 473–530. doi:10.1145/356887.356892. S2CID 6023466.
The idea of having the processor use the cached data before the tag match completes can be applied to associative caches as well. A subset of the tag, called a hint, can be used to pick just one of the possible cache entries mapping to the requested address. The entry selected by the hint can then be used in parallel with checking the full tag. The hint technique works best when used in the context of address translation, as explained below.
The speed of this recurrence (the load latency) is crucial to CPU performance, and so most modern level-1 caches are virtually indexed, which at least allows the MMU's TLB lookup to proceed in parallel with fetching the data from the cache RAM.
"Intel Iris Pro 5200 Graphics Review: Core i7-4950HQ Tested". AnandTech. Retrieved 2013-09-16.
Tian Tian; Chiu-Pi Shih (2012-03-08). "Software Techniques for Shared-Cache Multi-Core Systems". Intel. Retrieved 2015-11-24.
The K8 also has multiple-level caches. There are second-level instruction and data TLBs, which store only PTEs mapping 4 KiB. Both instruction and data caches, and the various TLBs, can fill from the large unified L2 cache. This cache is exclusive to both the L1 instruction and data caches, which means that any 8-byte line can only be in one of the L1 instruction cache, the L1 data cache, or the L2 cache. It is, however, possible for a line in the data cache to have a PTE which is also in one of the TLBs—the operating system is responsible for keeping the TLBs coherent by flushing portions of them when the page tables in memory are updated.
An associative cache is more complicated, because some form of tag must be read to determine which entry of the cache to select. An N-way set-associative level-1 cache usually reads all N possible tags and N data in parallel, and then chooses the data associated with the matching tag. Level-2 caches sometimes save power by reading the tags first, so that only one data element is read from the data SRAM.
Large physically indexed caches (usually secondary caches) run into a problem: the operating system rather than the application controls which pages collide with one another in the cache. Differences in page allocation from one program run to the next lead to differences in the cache collision patterns, which can lead to very large differences in program performance. These differences can make it very difficult to get a consistent and repeatable timing for a benchmark run.
^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Gabriel Torres (September 12, 2007). "How The Cache Memory Works".
In the common case of finding a hit in the first way tested, a pseudo-associative cache is as fast as a direct-mapped cache, but it has a much lower conflict miss rate than a direct-mapped cache, closer to the miss rate of a fully associative cache.[14]
Anand Lal Shimpi (2012-10-05). "Intel's Haswell Architecture Analyzed". AnandTech. Retrieved 2013-10-20.
Virtual memory requires the processor to translate virtual addresses generated by the program into physical addresses in main memory. The portion of the processor that does this translation is known as the memory management unit (MMU). The fast path through the MMU can perform those translations stored in the translation lookaside buffer (TLB), which is a cache of mappings from the operating system's page table, segment table, or both.
^ Sally Adee (2009). "Chip Design Thwarts Sneak Attack on Data". {{cite journal}}: Cite journal requires |journal= (help)
Nathan N. Sadler; Daniel J. Sorin (2006). "Choosing an Error Protection Scheme for a Microprocessor's L1 Data Cache" (PDF). p. 4.
Ian Cutress (2016-08-18). "AMD Zen Microarchitecture: Dual Schedulers, Micro-Op Cache and Memory Hierarchy Revealed". AnandTech. Retrieved 2017-04-03.
^ a b Baruch Solomon; Avi Mendelson; Doron Orenstein; Yoav Almog; Ronny Ronen (August 2001). "Micro-Operation Cache: A Power Aware Frontend for Variable Instruction Length ISA" (PDF). ISLPED'01: Proceedings of the 2001 International Symposium on Low Power Electronics and Design (IEEE Cat. No.01TH8581). Intel. pp. 4–9. doi:10.1109/LPE.2001.945363. ISBN 978-1-58113-371-4. S2CID 195859085. Retrieved 2013-10-06.
Multicolumn cache remains a high hit ratio due to its high associativity, and has a comparable low latency to a direct-mapped cache due to its high percentage of hits in major locations. The concepts of major locations and selected locations in multicolumn cache have been used in several cache designs in ARM Cortex R chip, Intel’s way-predicting cache memory, IBM’s reconfigurable multi-way associative cache memory, and Oracle’s dynamic cache replacement way selection based on address tab bits.
If data is written to the cache, at some point it must also be written to main memory; the timing of this write is known as the write policy. In a write-through cache, every write to the cache causes a write to main memory.Alternatively, in a write-back or copy-back cache, writes are not immediately mirrored to the main memory, and the cache instead tracks which locations have been written over, marking them as dirty. The data in these locations is written back to the main memory only when that data is evicted from the cache. For this reason, a read miss in a write-back cache may sometimes require two memory accesses to service: one to first write the dirty location to main memory, and then another to read the new location from memory. Also, a write to a main memory location that is not yet mapped in a write-back cache may evict an already dirty location, thereby freeing that cache space for the new memory location.
The index describes which cache set that the data has been put in. The index length is ⌈log2⁡(s)⌉{\displaystyle \lceil \log _{2}(s)\rceil } bits for s cache sets.
Intel's Xeon MP product codenamed "Tulsa" (2006) features 16 MiB of on-die L3 cache shared between two processor cores.
"IBM z13 and IBM z13s Technical Introduction" (PDF). IBM. March 2016. p. 20.
^ Zhenghong Wang; Ruby B. Lee (November 8–12, 2008). A novel cache architecture with enhanced performance and security (PDF). 41st annual IEEE/ACM International Symposium on Microarchitecture. pp. 83–93. Archived (PDF) from the original on March 6, 2012.
To understand the problem, consider a CPU with a 1 MiB physically indexed direct-mapped level-2 cache and 4 KiB virtual memory pages. Sequential physical pages map to sequential locations in the cache until after 256 pages the pattern wraps around. We can label each physical page with a color of 0–255 to denote where in the cache it can go. Locations within physical pages with different colors cannot conflict in the cache.
When trying to read from or write to a location in the main memory, the processor checks whether the data from that location is already in the cache. If so, the processor will read from or write to the cache instead of the much slower main memory.
The three-level caches were used again first with the introduction of multiple processor cores, where the L3 cache was added to the CPU die.It became common for the total cache sizes to be increasingly larger in newer processor generations, and recently (as of 2011) it is not uncommon to find Level 3 cache sizes of tens of megabytes.[62]
A μop cache has many similarities with a trace cache, although a μop cache is much simpler thus providing better power efficiency; this makes it better suited for implementations on battery-powered devices. The main disadvantage of the trace cache, leading to its power inefficiency, is the hardware complexity required for its heuristic deciding on caching and reusing dynamically created instruction traces.[43]
In order of worse but simple to better but complex:
"The Intel Skylake Mobile and Desktop Launch, with Architecture Analysis"
James E. Thornton (October 1964), "Parallel operation in the control data 6600" (PDF), Proc. of the October 27-29, 1964, fall joint computer conference, part II: very high speed computer systems
Pipelined CPUs access memory from multiple points in the pipeline: instruction fetch, virtual-to-physical address translation, and data fetch (see classic RISC pipeline). The natural design is to use different physical caches for each of these points, so that no one physical resource has to be scheduled to service two points in the pipeline. Thus the pipeline naturally ends up with at least three separate caches (instruction, TLB, and data), each specialized to its particular role.
Smart Cache shares the actual cache memory between the cores of a multi-core processor. In comparison to a dedicated per-core cache, the overall cache miss rate decreases when not all cores need equal parts of the cache space. Consequently, a single core can use the full level 2 or level 3 cache, if the other cores are inactive.[45] Furthermore, the shared cache makes it faster to share memory among different execution cores.[46]
^ "Cortex-R4 and Cortex-R4F Technical Reference Manual". arm.com. Retrieved 2013-09-28.
An effective memory address which goes along with the cache line (memory block) is split (MSB to LSB) into the tag, the index and the block offset.[18][19]
6Address translation											Toggle Address translation subsection																					6.1Homonym and synonym problems																											6.2Virtual tags and vhints																											6.3Page coloring
In the early days of microcomputer technology, memory access was only slightly slower than register access. But since the 1980s[57] the performance gap between processor and memory has been growing. Microprocessors have advanced much faster than memory, especially in terms of their operating frequency, so memory became a performance bottleneck. While it was technically possible to have all the main memory as fast as the CPU, a more economically viable path has been taken: use plenty of low-speed memory, but also introduce a small high-speed cache memory to alleviate the performance gap. This provided an order of magnitude more capacity—for the same price—with only a slightly reduced combined performance.
The software page coloring technique has been used to effectively partition the shared Last level Cache (LLC) in multicore processors.[30] This operating system-based LLC management in multicore processors has been adopted by Intel.[31]
Mahapatra, Nihar R.; Venkatrao, Balakrishna (1999). "The processor-memory bottleneck: problems and solutions" (PDF). Crossroads. 5 (3es): 2–es. doi:10.1145/357783.331677. S2CID 11557476. Retrieved 2013-03-05.{{cite journal}}:CS1 maint: url-status (link)
Many modern desktop, server, and industrial CPUs have at least three independent caches:
Caches have historically used both virtual and physical addresses for the cache tags, although virtual tagging is now uncommon. If the TLB lookup can finish before the cache RAM lookup, then the physical address is available in time for tag compare, and there is no need for virtual tagging. Large caches, then, tend to be physically tagged, and only small, very low latency caches are virtually tagged. In recent general-purpose CPUs, virtual tagging has been superseded by vhints, as described below.
^ a b Chenxi Zhang; Xiaodong Zhang; Yong Yan (September–October 1997). "Two fast and high-associativity cache schemes" (PDF). IEEE Micro. 17 (5): 40–49. doi:10.1109/40.621212.
^ a b "Intel Iris Pro 5200 Graphics Review: Core i7-4950HQ Tested". AnandTech. Retrieved 2013-09-16.
Intel Haswell CPUs with integrated Intel Iris Pro Graphics have 128 MiB of eDRAM acting essentially as an L4 cache.[47]
"The microarchitecture of Intel, AMD and VIA CPUs: An optimization guide for assembly programmers and compiler makers"
"Micro-Operation Cache: A Power Aware Frontend for Variable Instruction Length ISA"
"2nd Generation Intel Core Processor Family: Intel Core i7, i5 and i3"
Smith, Ryan. "Intel Launches Atom P5900: A 10nm Atom for Radio Access Networks". www.anandtech.com. Retrieved 2020-04-12.
^ "Micro-Architecture]". Skewed-associative caches have been shown to have two major advantages over conventional set-associative caches.
Programmers attempting to make maximum use of the cache may arrange their programs' access patterns so that only 1 MiB of data need be cached at any given time, thus avoiding capacity misses. But they should also ensure that the access patterns do not have conflict misses. One way to think about this problem is to divide up the virtual pages the program uses and assign them virtual colors in the same way as physical colors were assigned to physical pages before. Programmers can then arrange the access patterns of their code so that no two pages with the same virtual color are in use at the same time. There is a wide literature on such optimizations (e.g. loop nest optimization), largely coming from the High Performance Computing (HPC) community.
"Choosing an Error Protection Scheme for a Microprocessor's L1 Data Cache"
Typically, sharing the L1 cache is undesirable because the resulting increase in latency would make each core run considerably slower than a single-core chip.However, for the highest-level cache, the last one called before accessing memory, having a global cache is desirable for several reasons, such as allowing a single core to use the whole cache, reducing data redundancy by making it possible for different processes or threads to share cached data, and reducing the complexity of utilized cache coherency protocols.[48]For example, an eight-core chip with three levels may include an L1 cache for each core, one intermediate L2 cache for each pair of cores, and one L3 cache shared between all cores.
Some authors refer to the block offset as simply the "offset"[20] or the "displacement".[21][22]
The early history of cache technology is closely tied to the invention and use of virtual memory.[citation needed]Because of scarcity and cost of semi-conductor memories, early mainframe computers in the 1960s used a complex hierarchy of physical memory, mapped onto a flat virtual memory space used by programs. The memory technologies would span semi-conductor, magnetic core, drum and disc. Virtual memory seen and used by programs would be flat and caching would be used to fetch data and instructions into the fastest memory ahead of processor access. Extensive studies were done to optimize the cache sizes. Optimal values were found to depend greatly on the programming language used with Algol needing the smallest and Fortran and Cobol needing the largest cache sizes.[disputed– discuss]
^ The very first paging machine, the Ferranti Atlas[24][25] had no page tables in main memory; there was an associative memory with one entry for every 512 word page frame of core.
With the 486 processor, an 8 KiB cache was integrated directly into the CPU die. This cache was termed Level 1 or L1 cache to differentiate it from the slower on-motherboard, or Level 2 (L2) cache. These on-motherboard caches were much larger, with the most common size being 256 KiB. The popularity of on-motherboard cache continued through the Pentium MMX era but was made obsolete by the introduction of SDRAM and the growing disparity between bus clock rates and CPU clock rates, which caused on-motherboard cache to be only slightly faster than main memory.
The block offset specifies the desired data within the stored data block within the cache row. Typically the effective address is in bytes, so the block offset length is ⌈log2⁡(b)⌉{\displaystyle \lceil \log _{2}(b)\rceil } bits, where b is the number of bytes per data block.The tag contains the most significant bits of the address, which are checked against all rows in the current set (the set has been retrieved by index) to see if this set contains the requested address. If it does, a cache hit occurs. The tag length in bits is as follows:
Fetching complete pre-decoded instructions eliminates the need to repeatedly decode variable length complex instructions into simpler fixed-length micro-operations, and simplifies the process of predicting, fetching, rotating and aligning fetched instructions. A μop cache effectively offloads the fetch and decode hardware, thus decreasing power consumption and improving the frontend supply of decoded micro-operations.The μop cache also increases performance by more consistently delivering decoded micro-operations to the backend and eliminating various bottlenecks in the CPU's fetch and decode logic.[40][41]
Early examples of CPU caches include the Atlas 2[3] and the IBM System/360 Model 85[4] in the 1960s. The first CPUs that used a cache had only one level of cache; unlike later level 1 cache, it was not split into L1d (for data) and L1i (for instructions). Split L1 cache started in 1976 with the IBM 801 CPU,[5][6] became mainstream in the late 1980s, and in 1997 entered the embedded CPU market with the ARMv5TE. In 2015, even sub-dollar SoCs split the L1 cache. They also have L2 caches and, for larger processors, L3 caches as well. The L2 cache is usually not split, and acts as a common repository for the already split L1 cache. Every core of a multi-core processor has a dedicated L1 cache and is usually not shared between the cores. The L2 cache, and higher-level caches, may be shared between the cores. L4 cache is currently uncommon, and is generally dynamic random-access memory (DRAM) on a separate die or chip, rather than static random-access memory (SRAM). An exception to this is when eDRAM is used for all levels of cache, down to L1. Historically L1 was also on a separate die, however bigger die sizes have allowed integration of it as well as other cache levels, with the possible exception of the last level. Each extra level of cache tends to be bigger and optimized differently.
Choosing the right value of associativity involves a trade-off. If there are ten places to which the placement policy could have mapped a memory location, then to check if that location is in the cache, ten cache entries must be searched. Checking more places takes more power and chip area, and potentially more time. On the other hand, caches with more associativity suffer fewer misses (see conflict misses), so that the CPU wastes less time reading from the slow main memory. The general guideline is that doubling the associativity, from direct mapped to two-way, or from two-way to four-way, has about the same effect on raising the hit rate as doubling the cache size. However, increasing associativity more than four does not improve hit rate as much,[11] and are generally done for other reasons (see virtual aliasing).Some CPUs can dynamically reduce the associativity of their caches in low-power states, which acts as a power-saving measure.[12]
Evaluating Associativity in CPU Caches – Hill and Smith (1989) – introduces capacity, conflict, and compulsory classification
The K8 has four specialized caches: an instruction cache, an instruction TLB, a data TLB, and a data cache. Each of these caches is specialized:
The first documented use of an instruction cache was on the CDC 6600.[60]
Intel introduced a Level 4 on-package cache with the Haswell microarchitecture. Crystalwell[33] Haswell CPUs, equipped with the GT3e variant of Intel's integrated Iris Pro graphics, effectively feature 128 MiB of embedded DRAM (eDRAM) on the same package.This L4 cache is shared dynamically between the on-die GPU and CPU, and serves as a victim cache to the CPU's L3 cache.[34]
Register files sometimes also have hierarchy: The Cray-1 (circa 1976) had eight address "A" and eight scalar data "S" registers that were generally usable. There was also a set of 64 address "B" and 64 scalar data "T" registers that took longer to access, but were faster than main memory. The "B" and "T" registers were provided because the Cray-1 did not have a data cache. (The Cray-1 did, however, have an instruction cache.)
Cache Performance for SPEC CPU2000 Benchmarks – Hill and Cantin (2003) – This reference paper has been updated several times. It has thorough and lucidly presented simulation results for a reasonably wide set of benchmarks and cache organizations.
Other types of caches exist (that are not counted towards the "cache size" of the most important caches mentioned above), such as the translation lookaside buffer (TLB) which is part of the memory management unit (MMU) which most CPUs have.
One advantage of strictly inclusive caches is that when external devices or other processors in a multiprocessor system wish to remove a cache line from the processor, they need only have the processor check the L2 cache. In cache hierarchies which do not enforce inclusion, the L1 cache must be checked as well. As a drawback, there is a correlation between the associativities of L1 and L2 caches: if the L2 cache does not have at least as many ways as all L1 caches together, the effective associativity of the L1 caches is restricted. Another disadvantage of inclusive cache is that whenever there is an eviction in L2 cache, the (possibly) corresponding lines in L1 also have to get evicted in order to maintain inclusiveness. This is quite a bit of work, and would result in a higher L1 miss rate.[52]
The "size" of the cache is the amount of main memory data it can hold.This size can be calculated as the number of bytes stored in each data block times the number of blocks stored in the cache.(The tag, flag and error correction code bits are not included in the size,[17] although they do affect the physical area of a cache.)
R. W. O'Neill. Experience using a time sharing multiprogramming system with dynamic address relocation hardware. Proc. AFIPS Computer Conference 30 (Spring Joint Computer Conference, 1967). pp. 611–621. doi:10.1145/1465482.1465581.
Intel Core i7 (2008) has an 8 MiB on-die unified L3 cache that is inclusive, shared by all cores.
^ Anand Lal Shimpi (2000-11-20). "The Pentium 4's Cache – Intel Pentium 4 1.4 GHz & 1.5 GHz". AnandTech. Retrieved 2015-11-30.
Perhaps the ultimate reduction of virtual hints can be found in the Pentium 4 (Willamette and Northwood cores). In these processors the virtual hint is effectively two bits, and the cache is four-way set associative. Effectively, the hardware maintains a simple permutation from virtual address to cache index, so that no content-addressable memory (CAM) is necessary to select the right one of the four ways fetched.
Most general purpose CPUs implement some form of virtual memory. To summarize, either each program running on the machine sees its own simplified address space, which contains code and data for that program only, or all programs run in a common virtual address space. A program executes by calculating, comparing, reading and writing to addresses of its virtual address space, rather than addresses of physical address space, making programs simpler and thus easier to write.
Latency: The physical address is available from the MMU some time, perhaps a few cycles, after the virtual address is available from the address generator.
Direct mapped cache –good best-case time, but unpredictable in the worst case
7.2Multi-level caches																								7.2.1Multi-core chips																											7.2.2Separate versus unified																											7.2.3Exclusive versus inclusive
A cache that relies on virtual indexing and tagging becomes inconsistent after the same virtual address is mapped into different physical addresses (homonym), which can be solved by using physical address for tagging, or by storing the address space identifier in the cache line. However, the latter approach does not help against the synonym problem, in which several cache lines end up storing data for the same physical address. Writing to such locations may update only one location in the cache, leaving the others with inconsistent data. This issue may be solved by using non-overlapping memory layouts for different address spaces, or otherwise the cache (or a part of it) must be flushed when the mapping changes.[29]
Baruch Solomon; Avi Mendelson; Doron Orenstein; Yoav Almog; Ronny Ronen (August 2001). "Micro-Operation Cache: A Power Aware Frontend for Variable Instruction Length ISA" (PDF). ISLPED'01: Proceedings of the 2001 International Symposium on Low Power Electronics and Design (IEEE Cat. No.01TH8581). Intel. pp. 4–9. doi:10.1109/LPE.2001.945363. ISBN 978-1-58113-371-4. S2CID 195859085. Retrieved 2013-10-06.
IBM (June 1968). IBM System/360 Model 85 Functional Characteristics (PDF). SECOND EDITION. A22-6916-1.
AMD Phenom II (2008) has up to 6 MiB on-die unified L3 cache.
A more modern cache might be 16 KiB, 4-way set-associative, virtually indexed, virtually hinted, and physically tagged, with 32 B lines, 32-bit read width and 36-bit physical addresses. The read path recurrence for such a cache looks very similar to the path above. Instead of tags, vhints are read, and matched against a subset of the virtual address. Later on in the pipeline, the virtual address is translated into a physical address by the TLB, and the physical tag is read (just one, as the vhint supplies which way of the cache to read). Finally the physical address is compared to the physical tag to determine if a hit has occurred.
^ "Atlas 2 at Cambridge Mathematical Laboratory (and Aldermaston and CAD Centre)". November 2012. Two tunnel diode stores were developed at Cambridge; one, which worked very well, speeded up the fetching of operands, the other was intended to speed up the fetching of instructions. The idea was that most instructions are obeyed in sequence, so when an instruction was fetched that word was placed in the slave store in the location given by the fetch address modulo 32; the remaining bits of the fetch address were also stored. If the wanted word was in the slave it was read from there instead of main memory. This would give a major speedup to instruction loops up to 32 instructions long, and reduced effect for loops up to 64 words.
"Power Management of the Third Generation Intel Core Micro Architecture formerly codenamed Ivy Bridge"
Intel's Crystalwell[33] variant of its Haswell processors introduced an on-package 128 MiB eDRAM Level 4 cache which serves as a victim cache to the processors' Level 3 cache.[34] In the Skylake microarchitecture the Level 4 cache no longer works as a victim cache.[35]
^ Su, Chao; Zeng, Qingkai (2021-06-10).Nicopolitidis, Petros (ed.). "Survey of CPU Cache-Based Side-Channel Attacks: Systematic Analysis, Security Models, and Countermeasures". Security and Communication Networks. 2021: 1–15. doi:10.1155/2021/5559552. ISSN 1939-0122.
^ Smith, Ryan. "Intel Launches Atom P5900: A 10nm Atom for Radio Access Networks". www.anandtech.com. Retrieved 2020-04-12.
^ Harvey G. Cragon."Memory systems and pipelined processors".1996. ISBN 0-86720-474-5, ISBN 978-0-86720-474-2."Chapter 4.1: Cache Addressing, Virtual or Real"p. 209[1]
A true set-associative cache tests all the possible ways simultaneously, using something like a content-addressable memory. A pseudo-associative cache tests each possible way one at a time. A hash-rehash cache and a column-associative cache are examples of a pseudo-associative cache.
The time taken to fetch one cache line from memory (read latency due to a cache miss) matters because the CPU will run out of things to do while waiting for the cache line. When a CPU reaches this state, it is called a stall.As CPUs become faster compared to main memory, stalls due to cache misses displace more potential computation; modern CPUs can execute hundreds of instructions in the time taken to fetch a single cache line from main memory.
Shared highest-level cache, which is called before accessing memory, is usually referred to as the last level cache (LLC).Additional techniques are used for increasing the level of parallelism when LLC is shared between multiple cores, including slicing it into multiple pieces which are addressing certain ranges of memory addresses, and can be accessed independently.[49]
Gaining insights into multicore cache partitioning: Bridging the gap between simulation and real systems
Other processors have other kinds of predictors (e.g., the store-to-load bypass predictor in the DEC Alpha 21264), and various specialized predictors are likely to flourish in future processors.
There are several tools available to computer architects to help explore tradeoffs between the cache cycle time, energy, and area; the CACTI cache simulator[65] and the SimpleScalar instruction set simulator are two open-source options.
A victim cache is a cache used to hold blocks evicted from a CPU cache upon replacement. The victim cache lies between the main cache and its refill path, and holds only those blocks of data that were evicted from the main cache. The victim cache is usually fully associative, and is intended to reduce the number of conflict misses. Many commonly used programs do not require an associative mapping for all the accesses. In fact, only a small fraction of the memory accesses of the program require high associativity. The victim cache exploits this property by providing high associativity to only these accesses. It was introduced by Norman Jouppi from DEC in 1990.[32]
"IBM System/360 Model 85 Functional Characteristics" (PDF). IBM. June 1968. A22-6916-1.
One of the more extreme examples of cache specialization is the trace cache (also known as execution trace cache) found in the Intel Pentium 4 microprocessors.A trace cache is a mechanism for increasing the instruction fetch bandwidth and decreasing power consumption (in the case of the Pentium 4) by storing traces of instructions that have already been fetched and decoded.[36]
^ John L. Hennessy; David A. Patterson (2011). Computer Architecture: A Quantitative Approach. p. B-9. ISBN 978-0-12-383872-8.
"AMD Zen Microarchitecture: Dual Schedulers, Micro-Op Cache and Memory Hierarchy Revealed"
Agner Fog (2014-02-19). "The microarchitecture of Intel, AMD and VIA CPUs: An optimization guide for assembly programmers and compiler makers" (PDF). agner.org. Retrieved 2014-03-21.
The advantage of exclusive caches is that they store more data. This advantage is larger when the exclusive L1 cache is comparable to the L2 cache, and diminishes if the L2 cache is many times larger than the L1 cache. When the L1 misses and the L2 hits on an access, the hitting cache line in the L2 is exchanged with a line in the L1. This exchange is quite a bit more work than just copying a line from L2 to L1, which is what an inclusive cache does.[52]
Apple M1 CPU has 128 or 192 KiB instruction L1 cache for each core (important for latency/single-thread performance), depending on core type. This is an unusually large L1 cache for any CPU type (not just for a laptop); the total cache memory size is not unusually large (the total is more important for throughput) for a laptop, and much larger total (e.g. L3 or L4) sizes are available in IBM's mainframes.
Write Coalescing Cache[38] is a special cache that is part of L2 cache in AMD's Bulldozer microarchitecture. Stores from both L1D caches in the module go through the WCC, where they are buffered and coalesced.The WCC's task is reducing number of writes to the L2 cache.
^ Kun Niu (28 May 2015). "How does the BTIC (branch target instruction cache) work?". Retrieved 7 April 2018.
"Explanation of the L1 and L2 Cache". amecomputers.com. Archived from the original on 2014-07-14. Retrieved 2014-06-09.
^ Jiang Lin; Qingda Lu; Xiaoning Ding; Zhao Zhang; Xiaodong Zhang; P. Sadayappan (2008). Gaining insights into multicore cache partitioning: Bridging the gap between simulation and real systems (PDF). IEEE 14th International Symposium on High Performance Computer Architecture. Salt Lake City, UT. pp. 367–378. doi:10.1109/HPCA.2008.4658653.
^ "L210 Cache Controller Technical Reference Manual". arm.com. Retrieved 2013-09-28.
^ IEEE Xplore - Phased set-associative cache design for reduced power consumption. Ieeexplore.ieee.org (2009-08-11). Retrieved on 2013-07-30.
The data TLB has two copies which keep identical entries. The two copies allow two data accesses per cycle to translate virtual addresses to physical addresses. Like the instruction TLB, this TLB is split into two kinds of entries.
The adjacent diagram is intended to clarify the manner in which the various fields of the address are used. Address bit 31 is most significant, bit 0 is least significant. The diagram shows the SRAMs, indexing, and multiplexing for a 4 KiB, 2-way set-associative, virtually indexed and virtually tagged cache with 64 byte (B) lines, a 32-bit read width and 32-bit virtual address.
Memory Hierarchy in Cache-Based Systems Archived 2009-09-15 at the Wayback Machine – by Ruud van der Pas, 2002, Sun Microsystems – a nice introductory article to CPU memory caching
^ Taylor, George; Davies, Peter; Farmwald, Michael (1990). "The TLB Slice - A Low-Cost High-Speed Address Translation Mechanism". CH2887-8/90/0000/0355$01.OO. {{cite journal}}: Cite journal requires |journal= (help)
N.P.Jouppi. "Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers." - 17th Annual International Symposium on Computer Architecture, 1990. Proceedings., doi:10.1109/ISCA.1990.134547
Aamer Jaleel; Eric Borch; Malini Bhandaru; Simon C. Steely Jr.; Joel Emer (2010-09-27). "Achieving Non-Inclusive Cache Performance with Inclusive Caches" (PDF). jaleels.org. Retrieved 2014-06-09.
IBM System/360 Model 67 Functional Characteristics (PDF). Third Edition. IBM. February 1972. GA27-2719-2.
A Cache Primer – by Paul Genua, P.E., 2004, Freescale Semiconductor, another introductory article
^ David Kanter (August 26, 2010). "AMD's Bulldozer Microarchitecture - Memory Subsystem Continued". Real World Technologies.
Some SPARC designs have improved the speed of their L1 caches by a few gate delays by collapsing the virtual address adder into the SRAM decoders. See sum-addressed decoder.
Multi-level caches introduce new design decisions.For instance, in some processors, all data in the L1 cache must also be somewhere in the L2 cache.These caches are called strictly inclusive. Other processors (like the AMD Athlon) have exclusive caches: data is guaranteed to be in at most one of the L1 and L2 caches, never in both.Still other processors (like the Intel Pentium II, III, and 4) do not require that data in the L1 cache also reside in the L2 cache, although it may often do so. There is no universally accepted name for this intermediate policy;[51][52]two common names are "non-exclusive" and "partially-inclusive".
Ying Zheng; Brian T. Davis; Matthew Jordan (10–12 March 2004). Performance Evaluation of Exclusive Cache Hierarchies (PDF). IEEE International Symposium on Performance Analysis of Systems and Software. Austin, TX, USA. pp. 89–96. doi:10.1109/ISPASS.2004.1291359. ISBN 0-7803-8385-0. Archived from the original (PDF) on 2012-08-13. Retrieved 2014-06-09.{{cite conference}}:CS1 maint: date format (link)
Granularity: The virtual address space is broken up into pages. For instance, a 4 GiB virtual address space might be cut up into 1,048,576 pages of 4 KiB size, each of which can be independently mapped. There may be multiple page sizes supported; see virtual memory for elaboration.
In this cache organization, each location in the main memory can go in only one entry in the cache.Therefore, a direct-mapped cache can also be called a "one-way set associative" cache.It does not have a placement policy as such, since there is no choice of which cache entry's contents to evict.This means that if two locations map to the same entry, they may continually knock each other out.Although simpler, a direct-mapped cache needs to be much larger than an associative one to give comparable performance, and it is more unpredictable.Let x be block number in cache, y be block number of memory, and n be number of blocks in cache, then mapping is done with the help of the equation x = y mod n.
8Implementation											Toggle Implementation subsection																					8.1History																								8.1.1First TLB implementations																											8.1.2First instruction cache																											8.1.3First data cache																											8.1.4In 68k microprocessors																											8.1.5In x86 microprocessors																											8.1.6In ARM microprocessors																											8.1.7Current research																														8.2Multi-ported cache
^ David A. Patterson; John L. Hennessy (2009). Computer Organization and Design: The Hardware/Software Interface. p. 484. ISBN 978-0-12-374493-7.
David A. Patterson; John L. Hennessy (2009). Computer Organization and Design: The Hardware/Software Interface. p. 484. ISBN 978-0-12-374493-7.
Some versions of the Intel 386 processor could support 16 to 256 KiB of external cache.
"Product Fact Sheet: Accelerating 5G Network Infrastructure, from the Core to the Edge"
Su, Chao; Zeng, Qingkai (2021-06-10).Nicopolitidis, Petros (ed.). "Survey of CPU Cache-Based Side-Channel Attacks: Systematic Analysis, Security Models, and Countermeasures". Security and Communication Networks. 2021: 1–15. doi:10.1155/2021/5559552. ISSN 1939-0122.
^ "Inside Intel Core Microarchitecture and Smart Memory Access". Intel. 2006. p. 5. Archived from the original (PDF) on 2011-12-29. Retrieved 2012-01-26.
One early virtual memory system, the IBM M44/44X, required an access to a mapping table held in core memory before every programmed access to main memory.[23][NB 1] With no caches, and with the mapping table memory running at the same speed as main memory this effectively cut the speed of memory access in half. Two early machines that used a page table in main memory for mapping, the IBM System/360 Model 67 and the GE 645, both had a small associative memory as a cache for accesses to the in-memory page table.Both machines predated the first machine with a cache for main memory, the IBM System/360 Model 85, so the first hardware cache used in a computer system was not a data or instruction cache, but rather a TLB.
Taylor, George; Davies, Peter; Farmwald, Michael (1990). "The TLB Slice - A Low-Cost High-Speed Address Translation Mechanism". CH2887-8/90/0000/0355$01.OO. {{cite journal}}: Cite journal requires |journal= (help)
The original Pentium 4 processor also had an eight-way set associative L2 integrated cache 256 KiB in size, with 128-byte cache blocks. This implies 32 - 8 - 7 = 17 bits for the tag field.[20]
"Atlas 2 at Cambridge Mathematical Laboratory (and Aldermaston and CAD Centre)". November 2012. Two tunnel diode stores were developed at Cambridge; one, which worked very well, speeded up the fetching of operands, the other was intended to speed up the fetching of instructions. The idea was that most instructions are obeyed in sequence, so when an instruction was fetched that word was placed in the slave store in the location given by the fetch address modulo 32; the remaining bits of the fetch address were also stored. If the wanted word was in the slave it was read from there instead of main memory. This would give a major speedup to instruction loops up to 32 instructions long, and reduced effect for loops up to 64 words.
This page was last edited on 16 March 2023, at 04:51 (UTC).
"The Pentium 4's Cache – Intel Pentium 4 1.4 GHz & 1.5 GHz"
Caches (like for RAM historically) have generally been sized in powers of: 2, 4, 8, 16 etc. KiB; when up to MiB sizes (i.e. for larger non-L1), very early on the pattern broke down, to allow for larger caches without being forced into the doubling-in-size paradigm, with e.g. Intel Core 2 Duo with 3 MiB L2 cache in April 2008. This happened much later for L1 caches, as their size is generally still a small number of KiB. The IBM zEC12 from 2012 is an exception however, to gain unusually large 96 KiB L1 data cache for its time, and e.g. the IBM z13 having a 96 KiB L1 instruction cache (and 128 KiB L1 data cache),[7] and Intel Ice Lake-based processors from 2018, having 48 KiB L1 data cache and 48 KiB L1 instruction cache. In 2020, some Intel Atom CPUs (with up to 24 cores) have (multiple of) 4.5 MiB and 15 MiB cache sizes.[8][9]
^ Kilburn, T.; Payne, R. B.; Howarth, D. J. (December 1961). "The Atlas Supervisor". Computers - Key to Total Systems Control. Conferences Proceedings. Vol. 20 Proceedings of the Eastern Joint Computer Conference Washington, D.C. Macmillan. pp. 279–294.
IBM POWER4 (2001) has off-chip L3 caches of 32 MiB per processor, shared among several processors.
The snag is that while all the pages in use at any given moment may have different virtual colors, some may have the same physical colors. In fact, if the operating system assigns physical pages to virtual pages randomly and uniformly, it is extremely likely that some pages will have the same physical color, and then locations from those pages will collide in the cache (this is the birthday paradox).
Fully associative cache –the best miss rates, but practical only for a small number of entries
It can be useful to distinguish the two functions of tags in an associative cache: they are used to determine which way of the entry set to select, and they are used to determine if the cache hit or missed. The second function must always be correct, but it is permissible for the first function to guess, and get the wrong answer occasionally.
Anand Lal Shimpi (2000-11-20). "The Pentium 4's Cache – Intel Pentium 4 1.4 GHz & 1.5 GHz". AnandTech. Retrieved 2015-11-30.
If each location in the main memory can be cached in either of two locations in the cache, one logical question is: which one of the two? The simplest and most commonly used scheme, shown in the right-hand diagram above, is to use the least significant bits of the memory location's index as the index for the cache memory, and to have two entries for each index. One benefit of this scheme is that the tags stored in the cache do not have to include that part of the main memory address which is implied by the cache memory's index. Since the cache tags have fewer bits, they require fewer transistors, take less space on the processor circuit board or on the microprocessor chip, and can be read and compared faster. Also LRU is especially simple since only one bit needs to be stored for each pair.
"Intel Iris Pro 5200 Graphics Review: Core i7-4950HQ Tested". AnandTech. Retrieved 2014-02-25.
The benefits of L3 and L4 caches depend on the application's access patterns.Examples of products incorporating L3 and L4 caches include the following:
A data cache typically requires two flag bits per cache line –a valid bit and a dirty bit.Having a dirty bit set indicates that the associated cache line has been changed since it was read from main memory ("dirty"), meaning that the processor has written data to that line and the new value has not propagated all the way to main memory.
Comparing with a direct-mapped cache, a set associative cache has a reduced number of bits for itscache set index that maps to a cache set, where multiple ways or blocks stays, such as 2 blocks for a 2-way set associative cache and 4 blocks for a 4-way set associative cache. Comparing with a direct mapped cache, the unused cache index bits become a part of the tag bits. For example, a 2-way set associative cache contributes 1 bit to the tag and a 4-way set associative cache contributes 2 bits to the tag. The basic idea of the multicolumn cache[16] is to use the set index to map to a cache set as a conventional set associative cache does, and to use the added tag bits to index a way in the set. For example, in a 4-way set associative cache, the two bits are used to index way 00, way 01, way 10, and way 11, respectively. This double cache indexing is called a “major location mapping”, and its latency is equivalent to a direct-mapped access.Extensive experiments in multicolumn cache design[16] shows that the hit ratio to major locations is as high as 90%. If cache mapping conflicts with a cache block in the major location, the existing cache block will be moved to another cache way in the same set, which is called “selected location”. Because the newly indexed cache block is a most recently used (MRU) block, it is placed in the major location in multicolumn cache with a consideration of temporal locality. Since multicolumn cache is designed for a cache with a high associativity, the number of ways in each set is high; thus, it is easy find a selected location in the set. A selected location index by an additional hardware is maintained for the major location in a cache block.
^ a b Anand Lal Shimpi (2012-10-05). "Intel's Haswell Architecture Analyzed". AnandTech. Retrieved 2013-10-20.
There are intermediate policies as well. The cache may be write-through, but the writes may be held in a store data queue temporarily, usually so multiple stores can be processed together (which can reduce bus turnarounds and improve bus utilization).
A micro-operation cache (μop cache, uop cache or UC)[39] is a specialized cache that stores micro-operations of decoded instructions, as received directly from the instruction decoders or from the instruction cache.When an instruction needs to be decoded, the μop cache is checked for its decoded form which is re-used if cached; if it is not available, the instruction is decoded and then cached.
Oded Lempel (2013-07-28). "2nd Generation Intel Core Processor Family: Intel Core i7, i5 and i3" (PDF). hotchips.org. p. 7–10,31–45. Archived from the original (PDF) on 2020-07-29. Retrieved 2014-01-21.
4Cache entry structure											Toggle Cache entry structure subsection																					4.1Example																											4.2Flag bits
A branch target cache or branch target instruction cache, the name used on ARM microprocessors,[44] is a specialized cache which holds the first few instructions at the destination of a taken branch.This is used by low-powered processors which do not need a normal instruction cache because the memory system is capable of delivering instructions fast enough to satisfy the CPU without one.However, this only applies to consecutive instructions in sequence; it still takes several cycles of latency to restart instruction fetch at a new address, causing a few cycles of pipeline bubble after a control transfer.A branch target cache provides instructions for those few cycles avoiding a delay after most taken branches.
Jiang Lin; Qingda Lu; Xiaoning Ding; Zhao Zhang; Xiaodong Zhang; P. Sadayappan (2008). Gaining insights into multicore cache partitioning: Bridging the gap between simulation and real systems (PDF). IEEE 14th International Symposium on High Performance Computer Architecture. Salt Lake City, UT. pp. 367–378. doi:10.1109/HPCA.2008.4658653.
^ Aamer Jaleel; Eric Borch; Malini Bhandaru; Simon C. Steely Jr.; Joel Emer (2010-09-27). "Achieving Non-Inclusive Cache Performance with Inclusive Caches" (PDF). jaleels.org. Retrieved 2014-06-09.
^ GE-645 System Manual (PDF). General Electric. January 1968. Retrieved 2020-07-10.
^ Sanjeev Jahagirdar; Varghese George; Inder Sodhi; Ryan Wells (2012). "Power Management of the Third Generation Intel Core Micro Architecture formerly codenamed Ivy Bridge" (PDF). hotchips.org. p. 18. Archived from the original (PDF) on 2020-07-29. Retrieved 2015-12-16.
Ian Cutress (September 2, 2015). "The Intel Skylake Mobile and Desktop Launch, with Architecture Analysis". AnandTech.
Because the cache is 4 KiB and has 64 B lines, there are just 64 lines in the cache, and we read two at a time from a Tag SRAM which has 32 rows, each with a pair of 21 bit tags. Although any function of virtual address bits 31 through 6 could be used to index the tag and data SRAMs, it is simplest to use the least significant bits.
Some processors (e.g. early SPARCs) have caches with both virtual and physical tags. The virtual tags are used for way selection, and the physical tags are used for determining hit or miss. This kind of cache enjoys the latency advantage of a virtually tagged cache, and the simple software interface of a physically tagged cache. It bears the added cost of duplicated tags, however. Also, during miss processing, the alternate ways of the cache line indexed have to be probed for virtual aliases and any matches evicted.
The 68030, released in 1987, is basically a 68020 core with an additional 256-byte data cache, an on-chip memory management unit (MMU), a process shrink, and added burst mode for the caches. The 68040, released in 1990, has split instruction and data caches of four kilobytes each. The 68060, released in 1994, has the following: 8 KiB data cache (four-way associative), 8 KiB instruction cache (four-way associative), 96-byte FIFO instruction buffer, 256-entry branch cache, and 64-entry address translation cache MMU buffer (four-way associative).
Scratchpad memory (SPM), also known as scratchpad, scratchpad RAM or local store in computer terminology, is a high-speed internal memory used for temporary storage of calculations, data, and other work in progress.
^ "IBM System/360 Model 85 Functional Characteristics" (PDF). IBM. June 1968. A22-6916-1.
^ "AMD K8". Sandpile.org. Archived from the original on 2007-05-15. Retrieved 2007-06-02.
André Seznec (1993). "A Case for Two-Way Skewed-Associative Caches". ACM SIGARCH Computer Architecture News. 21 (2): 169–178. doi:10.1145/173682.165152.
Chenxi Zhang; Xiaodong Zhang; Yong Yan (September–October 1997). "Two fast and high-associativity cache schemes" (PDF). IEEE Micro. 17 (5): 40–49. doi:10.1109/40.621212.
Similarly, because the cache is 4 KiB and has a 4 B read path, and reads two ways for each access, the Data SRAM is 512 rows by 8 bytes wide.
2History											Toggle History subsection																					2.1Cache entries																											2.2Policies																								2.2.1Replacement policies																											2.2.2Write policies																														2.3Cache performance																								2.3.1CPU stalls
^ a b Agner Fog (2014-02-19). "The microarchitecture of Intel, AMD and VIA CPUs: An optimization guide for assembly programmers and compiler makers" (PDF). agner.org. Retrieved 2014-03-21.
To illustrate both specialization and multi-level caching, here is the cache hierarchy of the K8 core in the AMD Athlon 64 CPU.[54]
"AMD K8". Sandpile.org. Archived from the original on 2007-05-15. Retrieved 2007-06-02.
A cache miss is a failed attempt to read or write a piece of data in the cache, which results in a main memory access with much longer latency. There are three kinds of cache misses: instruction read miss, data read miss, and data write miss.
When the processor needs to read or write a location in memory, it first checks for a corresponding entry in the cache. The cache checks for the contents of the requested memory location in any cache lines that might contain that address. If the processor finds that the memory location is in the cache, a cache hit has occurred. However, if the processor does not find the memory location in the cache, a cache miss has occurred. In the case of a cache hit, the processor immediately reads or writes the data in the cache line.For a cache miss, the cache allocates a new entry and copies data from main memory, then the request is fulfilled from the contents of the cache.
The K8 also caches information that is never stored in memory—prediction information. These caches are not shown in the above diagram. As is usual for this class of CPU, the K8 has fairly complexbranch prediction, with tables that help predict whether branches are taken and other tables which predict the targets of branches and jumps. Some of this information is associated with instructions, in both the level 1 instruction cache and the unified secondary cache.
^ "Intel Iris Pro 5200 Graphics Review: Core i7-4950HQ Tested". AnandTech. Retrieved 2014-02-25.
Kaxiras, Stefanos; Ros, Alberto (2013). A New Perspective for Efficient Virtual-Cache Coherence. 40th International Symposium on Computer Architecture (ISCA). pp. 535–547. CiteSeerX 10.1.1.307.9125. doi:10.1145/2485922.2485968. ISBN 9781450320795. S2CID 15434231.
.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Gabriel Torres (September 12, 2007). "How The Cache Memory Works".
The instruction TLB keeps copies of page table entries (PTEs). Each cycle's instruction fetch has its virtual address translated through this TLB into a physical address. Each entry is either four or eight bytes in memory. Because the K8 has a variable page size, each of the TLBs is split into two sections, one to keep PTEs that map 4 KiB pages, and one to keep PTEs that map 4 MiB or 2 MiB pages. The split allows the fully associative match circuitry in each section to be simpler. The operating system maps different sections of the virtual address space with different size PTEs.
In computer engineering, a tag RAM is used to specify which of the possible memory locations is currently stored in a CPU cache.[55][56]For a simple, direct-mapped design fast SRAM can be used. Higher associative caches usually employ content-addressable memory.
GE-645 System Manual (PDF). General Electric. January 1968. Retrieved 2020-07-10.
Data is transferred between memory and cache in blocks of fixed size, called cache lines or cache blocks. When a cache line is copied from memory into the cache, a cache entry is created. The cache entry will include the copied data as well as the requested memory location (called a tag).
The extra area (and some latency) can be mitigated by keeping virtual hints with each cache entry instead of virtual tags. These hints are a subset or hash of the virtual tag, and are used for selecting the way of the cache from which to get data and a physical tag. Like a virtually tagged cache, there may be a virtual hint match but physical tag mismatch, in which case the cache entry with the matching hint must be evicted so that cache accesses after the cache fill at this address will have just one hint match. Since virtual hints have fewer bits than virtual tags distinguishing them from one another, a virtually hinted cache suffers more conflict misses than a virtually tagged cache.
Smart cache is a level 2 or level 3 caching method for multiple execution cores, developed by Intel.
7.1Specialized caches																								7.1.1Victim cache																											7.1.2Trace cache																											7.1.3Write Coalescing Cache (WCC)																											7.1.4Micro-operation (μop or uop) cache																											7.1.5Branch target instruction cache																											7.1.6Smart cache
The next development in cache implementation in the x86 microprocessors began with the Pentium Pro, which brought the secondary cache onto the same package as the microprocessor, clocked at the same frequency as the microprocessor.
An instruction cache requires only one flag bit per cache row entry: a valid bit.The valid bit indicates whether or not a cache block has been loaded with valid data.
Early cache designs focused entirely on the direct cost of cache and RAM and average execution speed.More recent cache designs also consider energy efficiency, fault tolerance, and other goals.[63][64]
The very first paging machine, the Ferranti Atlas[24][25] had no page tables in main memory; there was an associative memory with one entry for every 512 word page frame of core.
^ Ian Cutress (2016-08-18). "AMD Zen Microarchitecture: Dual Schedulers, Micro-Op Cache and Memory Hierarchy Revealed". AnandTech. Retrieved 2017-04-03.
Sally Adee (2009). "Chip Design Thwarts Sneak Attack on Data". {{cite journal}}: Cite journal requires |journal= (help)
^ IBM (June 1968). IBM System/360 Model 85 Functional Characteristics (PDF). SECOND EDITION. A22-6916-1.
To make room for the new entry on a cache miss, the cache may have to evict one of the existing entries. The heuristic it uses to choose the entry to evict is called the replacement policy. The fundamental problem with any replacement policy is that it must predict which existing cache entry is least likely to be used in the future. Predicting the future is difficult, so there is no perfect method to choose among the variety of replacement policies available. One popular replacement policy, least-recently used (LRU), replaces the least recently accessed entry.
^ a b c Ying Zheng; Brian T. Davis; Matthew Jordan (10–12 March 2004). Performance Evaluation of Exclusive Cache Hierarchies (PDF). IEEE International Symposium on Performance Analysis of Systems and Software. Austin, TX, USA. pp. 89–96. doi:10.1109/ISPASS.2004.1291359. ISBN 0-7803-8385-0. Archived from the original (PDF) on 2012-08-13. Retrieved 2014-06-09.{{cite conference}}:CS1 maint: date format (link)
Leon Gu; Dipti Motiani (October 2003). "Trace Cache" (PDF). Retrieved 2013-10-06.
"Atlas 2 at Cambridge Mathematical Laboratory (and Aldermaston and CAD Centre)"
Kilburn, T.; Payne, R. B.; Howarth, D. J. (December 1961). "The Atlas Supervisor". Computers - Key to Total Systems Control. Conferences Proceedings. Vol. 20 Proceedings of the Eastern Joint Computer Conference Washington, D.C. Macmillan. pp. 279–294.
Cache entry replacement policy is determined by a cache algorithm selected to be implemented by the processor designers.In some cases, multiple algorithms are provided for different kinds of work loads.
If the operating system can guarantee that each physical page maps to only one virtual color, then there are no virtual aliases, and the processor can use virtually indexed caches with no need for extra virtual alias probes during miss handling. Alternatively, the OS can flush a page from the cache whenever it changes from one virtual color to another. As mentioned above, this approach was used for some early SPARC and RS/6000 designs.
Another issue is the fundamental tradeoff between cache latency and hit rate. Larger caches have better hit rates but longer latency. To address this tradeoff, many computers use multiple levels of cache, with small fast caches backed up by larger, slower caches.Multi-level caches generally operate by checking the fastest cache, level 1 (L1), first; if it hits, the processor proceeds at high speed. If that smaller cache misses, the next fastest cache, level 2 (L2), is checked, and so on, before accessing external memory.
The first documented uses of a TLB were on the GE 645[58] and the IBM 360/67,[59] both of which used an associative memory as a TLB.
A trace cache stores instructions either after they have been decoded, or as they are retired. Generally, instructions are added to trace caches in groups representing either individual basic blocks or dynamic instruction traces. The Pentium 4's trace cache stores micro-operations resulting from decoding x86 instructions, providing also the functionality of a micro-operation cache.Having this, the next time an instruction is needed, it does not have to be decoded into micro-ops again.[37]: 63–68
Virtually indexed, physically tagged (VIPT) caches use the virtual address for the index and the physical address in the tag. The advantage over PIPT is lower latency, as the cache line can be looked up in parallel with the TLB translation, however the tag cannot be compared until the physical address is available. The advantage over VIVT is that since the tag has the physical address, the cache can detect homonyms.Theoretically, VIPT requires more tags bits because some of the index bits could differ between the virtual and physical addresses (for example bit 12 and above for 4 KiB pages) and would have to be included both in the virtual index and in the physical tag. In practice this is not an issue because, in order to avoid coherency problems, VIPT caches are designed to have no such index bits (e.g., by limiting the total number of bits for the index and the block offset to 12 for 4 KiB pages); this limits the size of VIPT caches to the page size times the associativity of the cache.
^ a b André Seznec (1993). "A Case for Two-Way Skewed-Associative Caches". ACM SIGARCH Computer Architecture News. 21 (2): 169–178. doi:10.1145/173682.165152.
^ "IBM z13 and IBM z13s Technical Introduction" (PDF). IBM. March 2016. p. 20.
^ Mahapatra, Nihar R.; Venkatrao, Balakrishna (1999). "The processor-memory bottleneck: problems and solutions" (PDF). Crossroads. 5 (3es): 2–es. doi:10.1145/357783.331677. S2CID 11557476. Retrieved 2013-03-05.{{cite journal}}:CS1 maint: url-status (link)
^ Smith, Alan Jay (September 1982). "Cache Memories" (PDF). Computing Surveys. 14 (3): 473–530. doi:10.1145/356887.356892. S2CID 6023466.
IBM POWER4 processor review –an article on ixbtlabs by Pavel Danilov
On-motherboard caches enjoyed prolonged popularity thanks to the AMD K6-2 and AMD K6-III processors that still used Socket 7, which was previously used by Intel with on-motherboard caches. K6-III included 256 KiB on-die L2 cache and took advantage of the on-board cache as a third level cache, named L3 (motherboards with up to 2 MiB of on-board cache were produced). After the Socket 7 became obsolete, on-motherboard cache disappeared from the x86 systems.
^ R. W. O'Neill. Experience using a time sharing multiprogramming system with dynamic address relocation hardware. Proc. AFIPS Computer Conference 30 (Spring Joint Computer Conference, 1967). pp. 611–621. doi:10.1145/1465482.1465581.
The first documented use of a data cache was on the IBM System/360 Model 85.[61]
Caches can be divided into four types, based on whether the index or tag correspond to physical or virtual addresses:
The instruction cache keeps copies of 64-byte lines of memory, and fetches 16 bytes each cycle. Each byte in this cache is stored in ten bits rather than eight, with the extra bits marking the boundaries of instructions (this is an example of predecoding). The cache has only parity protection rather than ECC, because parity is smaller and any damaged data can be replaced by fresh data fetched from memory (which always has an up-to-date copy of instructions).
The K8 uses an interesting trick to store prediction information with instructions in the secondary cache. Lines in the secondary cache are protected from accidental data corruption (e.g. by an alpha particle strike) by either ECC or parity, depending on whether those lines were evicted from the data or instruction primary caches. Since the parity code takes fewer bits than the ECC code, lines from the instruction cache have a few spare bits. These bits are used to cache branch prediction information associated with those instructions. The net result is that the branch predictor has a larger effective history table, and so has better accuracy.
^ "Cache design" (PDF). ucsd.edu. 2010-12-02. p. 10–15. Retrieved 2023-01-29.
A multi-ported cache is a cache which can serve more than one request at a time. When accessing a traditional cache we normally use a single memory address, whereas in a multi-ported cache we may request N addresses at a time –where N is the number of ports that connected through the processor and the cache. The benefit of this is that a pipelined processor may access memory from different phases in its pipeline. Another benefit is that it allows the concept of super-scalar processors through different cache levels.
^ Tian Tian; Chiu-Pi Shih (2012-03-08). "Software Techniques for Shared-Cache Multi-Core Systems". Intel. Retrieved 2015-11-24.
Memory part 2: CPU caches –an article on lwn.net by Ulrich Drepper describing CPU caches in detail
"Intel's Sandy Bridge Microarchitecture - Instruction Decode and uop Cache"
Kun Niu (28 May 2015). "How does the BTIC (branch target instruction cache) work?". Retrieved 7 April 2018.
"Intel Launches Atom P5900: A 10nm Atom for Radio Access Networks"
Cached data from the main memory may be changed by other entities (e.g., peripherals using direct memory access (DMA) or another core in a multi-core processor), in which case the copy in the cache may become out-of-date or stale. Alternatively, when a CPU in a multiprocessor system updates data in the cache, copies of data in caches associated with other CPUs become stale. Communication protocols between the cache managers that keep the data consistent are known as cache coherence protocols.
But virtual indexing is not the best choice for all cache levels. The cost of dealing with virtual aliases grows with cache size, and as a result most level-2 and larger caches are physically indexed.
7Cache hierarchy in a modern processor											Toggle Cache hierarchy in a modern processor subsection																					7.1Specialized caches																								7.1.1Victim cache																											7.1.2Trace cache																											7.1.3Write Coalescing Cache (WCC)																											7.1.4Micro-operation (μop or uop) cache																											7.1.5Branch target instruction cache																											7.1.6Smart cache																														7.2Multi-level caches																								7.2.1Multi-core chips																											7.2.2Separate versus unified																											7.2.3Exclusive versus inclusive																														7.3Scratchpad memory																											7.4Example: the K8																											7.5More hierarchies																											7.6Tag RAM
Aliasing: Multiple virtual addresses can map to a single physical address. Most processors guarantee that all updates to that single physical address will happen in program order. To deliver on that guarantee, the processor must ensure that only one copy of a physical address resides in the cache at any given time.
It is also possible for the operating system to ensure that no virtual aliases are simultaneously resident in the cache. The operating system makes this guarantee by enforcing page coloring, which is described below. Some early RISC processors (SPARC, RS/6000) took this approach. It has not been used recently, as the hardware cost of detecting and evicting virtual aliases has fallen and the software complexity and performance penalty of perfect page coloring has risen.
Other schemes have been suggested, such as the skewed cache,[13] where the index for way 0 is direct, as above, but the index for way 1 is formed with a hash function. A good hash function has the property that addresses which conflict with the direct mapping tend not to conflict when mapped with the hash function, and so it is less likely that a program will suffer from an unexpectedly large number of conflict misses due to a pathological access pattern. The downside is extra latency from computing the hash function.[14] Additionally, when it comes time to load a new line and evict an old line, it may be difficult to determine which existing line was least recently used, because the new line conflicts with data at different indexes in each way; LRU tracking for non-skewed caches is usually done on a per-set basis. Nevertheless, skewed-associative caches have major advantages over conventional set-associative ones.[15]
IEEE Xplore - Phased set-associative cache design for reduced power consumption
^ Nathan N. Sadler; Daniel J. Sorin (2006). "Choosing an Error Protection Scheme for a Microprocessor's L1 Data Cache" (PDF). p. 4.
These predictors are caches in that they store information that is costly to compute. Some of the terminology used when discussing predictors is the same as that for caches (one speaks of a hit in a branch predictor), but predictors are not generally thought of as part of the cache hierarchy.
One of the early works describing μop cache as an alternative frontend for the Intel P6 processor family is the 2001 paper "Micro-Operation Cache: A Power Aware Frontend for Variable Instruction Length ISA".[40]Later, Intel included μop caches in its Sandy Bridge processors and in successive microarchitectures like Ivy Bridge and Haswell.[37]: 121–123 [41] AMD implemented a μop cache in their Zen microarchitecture.[42]
Itanium 2 (2003) has a 6 MiB unified level 3 (L3) cache on-die; the Itanium 2 (2003) MX 2 module incorporates two Itanium 2 processors along with a shared 64 MiB L4 cache on a multi-chip module that was pin compatible with a Madison processor.
The great advantage of virtual tags is that, for associative caches, they allow the tag match to proceed before the virtual to physical translation is done. However, coherence probes and evictions present a physical address for action. The hardware must have some means of converting the physical addresses into a cache index, generally by storing physical tags as well as virtual tags. For comparison, a physically tagged cache does not need to keep virtual tags, which is simpler. When a virtual to physical mapping is deleted from the TLB, cache entries with those virtual addresses will have to be flushed somehow. Alternatively, if cache entries are allowed on pages not mapped by the TLB, then those entries will have to be flushed when the access rights on those pages are changed in the page table.
When considering a chip with multiple cores, there is a question of whether the caches should be shared or local to each core. Implementing shared cache inevitably introduces more wiring and complexity. But then, having one cache per chip, rather than core, greatly reduces the amount of space needed, and thus one can include a larger cache.
"Altering Computer Architecture is Way to Raise Throughput, Suggests IBM Researchers". Electronics. 49 (25): 30–31. December 23, 1976.
"Product Fact Sheet: Accelerating 5G Network Infrastructure, from the Core to the Edge". Intel Newsroom (Press release). Retrieved 2020-04-12. L1 cache of 32KB/core, L2 cache of 4.5MB per 4-core cluster and shared LLC cache up to 15MB.
The 68010, released in 1982, has a "loop mode" which can be considered a tiny and special-case instruction cache that accelerates loops that consist of only two instructions. The 68020, released in 1984, replaced that with a typical instruction cache of 256 bytes, being the first 68k series processor to feature true on-chip cache memory.
^ Timothy Roscoe; Andrew Baumann (2009-03-03). "Advanced Operating Systems Caches and TLBs (263-3800-00L)" (PDF). systems.ethz.ch. Archived from the original (PDF) on 2011-10-07. Retrieved 2016-02-14.
^ IBM System/360 Model 67 Functional Characteristics (PDF). Third Edition. IBM. February 1972. GA27-2719-2.
IEEE Xplore - Phased set-associative cache design for reduced power consumption. Ieeexplore.ieee.org (2009-08-11). Retrieved on 2013-07-30.
Cache read misses from an instruction cache generally cause the largest delay, because the processor, or at least the thread of execution, has to wait (stall) until the instruction is fetched from main memory.Cache read misses from a data cache usually cause a smaller delay, because instructions not dependent on the cache read can be issued and continue execution until the data is returned from main memory, and the dependent instructions can resume execution.Cache write misses to a data cache generally cause the shortest delay, because the write can be queued and there are few limitations on the execution of subsequent instructions; the processor can continue until the queue is full. For a detailed introduction to the types of misses, see cache performance measurement and metric.
Physically indexed, virtually tagged (PIVT) caches are often claimed in literature to be useless and non-existing.[27]However, the MIPS R6000 uses this cache type as the sole known implementation.[28]The R6000 is implemented in emitter-coupled logic, which is an extremely fast technology not suitable for large memories such as a TLB.The R6000 solves the issue by putting the TLB memory into a reserved part of the second-level cache having a tiny, high-speed TLB "slice" on chip.The cache is indexed by the physical address obtained from the TLB slice.However, since the TLB slice only translates those virtual address bits that are necessary to index the cache and does not use any tags, false cache hits may occur, which is solved by tagging with the virtual address.
Modern processors have multiple interacting on-chip caches.The operation of a particular cache can be completely specified by the cache size, the cache block size, the number of blocks in a set, the cache set replacement policy, and the cache write policy (write-through or write-back).[20]
^ Ian Cutress (September 2, 2015). "The Intel Skylake Mobile and Desktop Launch, with Architecture Analysis". AnandTech.
The data block (cache line) contains the actual data fetched from the main memory.The tag contains (part of) the address of the actual data fetched from the main memory.The flag bits are discussed below.
The data cache keeps copies of 64-byte lines of memory. It is split into 8 banks (each storing 8 KiB of data), and can fetch two 8-byte data each cycle so long as those data are in different banks. There are two copies of the tags, because each 64-byte line is spread among all eight banks. Each tag copy handles one of the two accesses per cycle.
Zhenghong Wang; Ruby B. Lee (November 8–12, 2008). A novel cache architecture with enhanced performance and security (PDF). 41st annual IEEE/ACM International Symposium on Microarchitecture. pp. 83–93. Archived (PDF) from the original on March 6, 2012.
^ "Altering Computer Architecture is Way to Raise Throughput, Suggests IBM Researchers". Electronics. 49 (25): 30–31. December 23, 1976.
Finally, at the other end of the memory hierarchy, the CPU register file itself can be considered the smallest, fastest cache in the system, with the special characteristic that it is scheduled in software—typically by a compiler, as it allocates registers to hold values retrieved from main memory for, as an example, loop nest optimization. However, with register renaming most compiler register assignments are reallocated dynamically by hardware at runtime into a register bank, allowing the CPU to break false data dependencies and thus easing pipeline hazards.
