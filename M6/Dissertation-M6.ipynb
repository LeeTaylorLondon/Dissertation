{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM4074lwpxo/2XMoy84ekZn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Dissertation Set Expansion Paper\n","\n","This notebook contains my coded attempt at reproducing the best method from the paper: \n","\n","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999141."],"metadata":{"id":"iVkM328T7hE_"}},{"cell_type":"markdown","source":["## Download Files from my Dissertation Git Repo"],"metadata":{"id":"_usreLNk7Nln"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekKu9dEF2TbZ","executionInfo":{"status":"ok","timestamp":1678794044281,"user_tz":0,"elapsed":10531,"user":{"displayName":"L T","userId":"08720799606501677075"}},"outputId":"28dbfdbb-8293-4892-c7ba-140da5601a78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Dissertation'...\n","remote: Enumerating objects: 1402, done.\u001b[K\n","remote: Counting objects: 100% (109/109), done.\u001b[K\n","remote: Compressing objects: 100% (67/67), done.\u001b[K\n","remote: Total 1402 (delta 37), reused 100 (delta 31), pack-reused 1293\u001b[K\n","Receiving objects: 100% (1402/1402), 122.28 MiB | 24.20 MiB/s, done.\n","Resolving deltas: 100% (58/58), done.\n","Updating files: 100% (1990/1990), done.\n"]}],"source":["!git clone https://github.com/LeeTaylorNewcastle/Dissertation"]},{"cell_type":"markdown","source":["## Imports & Installations"],"metadata":{"id":"oeiig8AU81D6"}},{"cell_type":"code","source":["!pip install bs4\n","!pip install html5lib\n","!pip install gensim\n","!pip install scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RI6_KzlZIoc0","executionInfo":{"status":"ok","timestamp":1678810859510,"user_tz":0,"elapsed":16408,"user":{"displayName":"L T","userId":"08720799606501677075"}},"outputId":"c64eabb3-5dbb-4f9f-a7f4-d103750a5e1f"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: bs4 in /usr/local/lib/python3.9/dist-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from bs4) (4.9.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->bs4) (2.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.9/dist-packages (1.0.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from html5lib) (0.5.1)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.9/dist-packages (from html5lib) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (3.6.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.1.1)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","from bs4 import BeautifulSoup as bs\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","import numpy as np\n","import requests\n","import pickle\n","import string\n","import gensim\n","import os\n","import re"],"metadata":{"id":"vfQaiq5i8zaT","executionInfo":{"status":"ok","timestamp":1678813175668,"user_tz":0,"elapsed":219,"user":{"displayName":"L T","userId":"08720799606501677075"}}},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":["## Web Crawling and Scraping Functions\n","\n","The authors of the paper (MISSING_PAPER_NAME) scraped web pages related to their category, to reproduce their method I have also scraped web pages related to my category in a similar fashion. "],"metadata":{"id":"2w0J_T47_bdI"}},{"cell_type":"code","source":["INTERVAL = 3600\n","HEADERS = {\n","    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n","                  \"(KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\"\n","}\n","\n","def url_to_soup_obj(url: str):\n","    try:\n","        page = requests.get(url, headers=HEADERS)\n","    except:\n","        print(f\"<ERROR: {url}>\")\n","        return\n","    return bs(page.content, 'html5lib')\n","\n","def extract_elements(html_text, elm='p'):\n","    soup = bs(html_text, 'html.parser')\n","    paragraphs = []\n","    for p in soup.find_all(elm):\n","        paragraphs.append(p.text.strip())\n","    return paragraphs\n","\n","def google_search_wikipedia(search_str: str, debug: bool = True):\n","    # Convert search to google search URL\n","    gsearch_url = f\"https://www.google.com/search?q={'+'.join(search_str.lower().split())}\"\n","    # Generate 'soup' object of google search\n","    gsearch_soup = url_to_soup_obj(gsearch_url) # Todo: error\n","    # Extract URLs\n","    elms = extract_elements(str(gsearch_soup), elm='cite')\n","    href = extract_elements(str(gsearch_soup), elm='a')\n","    # Store 'cite' and 'a' elements in elms list\n","    for item in href:\n","        elms.append(item)\n","    # For-loop extracts URLs into list\n","    urls_ = []\n","    for string in elms:\n","        # Skip blank strings\n","        if string == '':\n","            continue\n","        # String must contain 'https:' but not '...' and 'category'\n","        if string.split()[0].__contains__(\"https:\") and \\\n","                not string.split()[-1].__contains__('...') and \\\n","                not string.lower().__contains__('category'):\n","            # Convert arrows to slashes for URL functionality\n","            string = string.replace(' ', '')\n","            urls_.append(string.replace('â€º', '/'))\n","    # Return list of URLs\n","    return list(set(urls_))\n","\n","def read_words_from_file(file_path, \n","                         encoding='utf-8-sig',\n","                         rtype='set'):\n","    with open(file_path, 'r', encoding=encoding) as f:\n","        if rtype == 'set':\n","            words = set(f.read().splitlines())\n","        elif rtype == 'list':\n","            words = f.read().splitlines()\n","        else:\n","            words = f.read().splitlines()\n","    return words\n","\n","def remove_code(text):\n","    # Use a regular expression to find any instances of code and scripts\n","    code = re.findall(r'<.*?>', text)\n","    # Remove all instances of code and scripts from the text\n","    clean_text = re.sub(r'<.*?>', '', text)\n","    # Return the resulting text\n","    return clean_text\n","\n","def extract_text(soup, headers=True, debug=True):\n","    # Find all HTML elements that contain the main content\n","    if headers:\n","        content_elements = soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\",\n","                                          \"h6\", \"a\", \"li\", \"span\", \"strong\", \"em\"])\n","    elif not headers:\n","        content_elements = soup.find_all([\"p\", \"a\", \"li\", \"span\", \"strong\", \"em\"])\n","    # Concatenate the text from all content elements\n","    content = [element.text.strip() for element in content_elements]\n","    for i, v in enumerate(content):\n","        content[i] = v.replace('\\n', '')\n","        content[i] = content[i].replace('  ', '')\n","    # Remove keywords\n","    content = [element for element in content if element.strip() != '']  # Remove blanks\n","    content = [element for element in content if len(element.split()) > 9]  # Remove lines with less than X words\n","    content = [element for element in content if not element.lower().__contains__('site')]\n","    content = [element for element in content if not element.lower().__contains__('cookie')]\n","    content = [element for element in content if not element.lower().__contains__('sign in')]\n","    content = [element for element in content if not element.lower().__contains__('instagram')]\n","    content = [element for element in content if not element.lower().__contains__('contact us')]\n","    # Combine content into a string\n","    content = '\\n'.join(set(content))\n","    content = remove_code(content)\n","    # Return the resulting text\n","    return content\n","\n","def query_to_text(search_str, page_limit=3, debug=True):\n","    # Define storage for extracted text\n","    rv = []\n","    # Extract URLs to scrape from\n","    urls = google_search_wikipedia(search_str=search_str)\n","    # For each URL append extracted readable text\n","    for url in urls[:page_limit]:\n","        if debug:\n","            print(url)\n","        soup_url = url_to_soup_obj(url)\n","        if soup_url is None:\n","            return ['']\n","        rv.append(extract_text(soup_url))\n","    # Mark EOF\n","    return rv\n","\n","def write_to_file(fn, text):\n","    with open(fn, \"w\", encoding='utf-8-sig') as f:\n","        for elm in text:\n","            f.write(str(elm) + '\\n')\n","\n","def main():\n","    # Read list of words to search\n","    entity_set = read_words_from_file(\n","      f'Dissertation/M6/'\n","      f'data_prep/entity_set.txt')\n","    # Entity set info\n","    print(\n","        f\"Set of words describing personality traits info.\\n\"\n","        f\"Number of words:  {len(entity_set)}\\n\"\n","        f\"First five words: {list(entity_set)[:5]}\\n\"\n","        f\"Source: http://ideonomy.mit.edu/essays/traits.html\\n\"\n","    )\n","    # Parse webapage for text\n","    counter, e_counter = 0, 0\n","    for word in list(entity_set)[:]:\n","        counter += 1\n","        try:\n","            search_term = f'wikipedia {word}'\n","            parsed_webpage_text = query_to_text(search_term, debug=False)\n","            if not os.path.exists(\"wcs\"):\n","                os.makedirs(\"wcs\")\n","            write_to_file(f\"wcs/{search_term}.txt\", parsed_webpage_text)\n","        except:\n","            counter -= 1\n","            e_counter += 1\n","            # print(f\"'{word}' could not be written to a file!\")\n","    print(f\"\\nSuccessfully scraped {counter} terms. {e_counter} terms failed.\")\n","\n","\n","main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"exHqNmAQ7KAX","executionInfo":{"status":"ok","timestamp":1678807870572,"user_tz":0,"elapsed":1553275,"user":{"displayName":"L T","userId":"08720799606501677075"}},"outputId":"4f862188-ec08-4fe5-c415-41573e2ef558"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Set of words describing personality traits info.\n","Number of words:  637\n","First five words: ['Impressive', 'Charmless', 'Cynical', 'Dull', 'Offhand']\n","Source: http://ideonomy.mit.edu/essays/traits.html\n","\n","<ERROR: Estheticianhttps://en.m.wikipedia.org/wiki/Esthetician>\n","<ERROR: Wikipedia:VaguenessWikipediahttps://en.wikipedia.org/wiki/Wikipedia:Vagueness>\n","<ERROR: Old-fashionedhttps://en.wikipedia.org/wiki/Old-fashioned>\n","<ERROR: Wikipedia:RudeWikipediahttps://en.wikipedia.org/wiki/Wikipedia:Rude>\n","<ERROR: contemplativeWiktionaryhttps://en.wiktionary.org/wiki/contemplative>\n","<ERROR: pedanticWiktionaryhttps://en.wiktionary.org/wiki/pedantic>\n","<ERROR: WiseWikipediahttps://en.wikipedia.org/wiki/Wise>\n","<ERROR: ImpatienceWikiquotehttps://en.wikiquote.org/wiki/Impatience>\n","<ERROR: flamboyancehttps://en.wiktionary.org/wiki/flamboyance>\n","<ERROR: Curiosityhttps://en.wikipedia.org/wiki/Curiosity>\n","<ERROR: Wikipedia:SUBJECTIVEWikipediahttps://en.wikipedia.org/wikipedia/Special:Search>\n","<ERROR: DrollWikipediahttps://en.wikipedia.org/wiki/Droll>\n","<ERROR: Wikipedia:PurposeWikipediahttps://en.wikipedia.org/wiki/Wikipedia:Purpose>\n","<ERROR: WinnersWikipediahttps://en.wikipedia.org/wiki/Winners>\n","<ERROR: uncharitablenesshttps://en.wiktionary.org/wiki/uncharitableness>\n","<ERROR: Wikipedia:Conciliationhttps://en.wikipedia.org/wiki/Wikipedia:Conciliation>\n","<ERROR: FakeWikipediahttps://en.wikipedia.org/wiki/Fake>\n","<ERROR: OptimismOptimismhttps://www.optimism.io>\n","<ERROR: dignityhttps://en.wiktionary.org/wiki/dignity>\n","<ERROR: Wikipedia:OffensiveWikipediahttps://en.wikipedia.org/wiki/Wikipedia:Offensive>\n","<ERROR: Wikipedia:HonestyWikipediahttps://en.wikipedia.org/wiki/Wikipedia:Honesty>\n","<ERROR: Wikipedia:ChallengesWikipediahttps://en.wikipedia.org/wiki/Wikipedia:Challenges>\n","<ERROR: LumanityLumanityhttps://lumanity.com>\n","<ERROR: idiosyncrasyhttps://en.wiktionary.org/wiki/idiosyncrasy>\n","\n","Successfully scraped 637 terms. 0 terms failed.\n"]}]},{"cell_type":"markdown","source":["The aim of this code is to collect and store information about personality traits from Wikipedia pages in a structured and organized manner. The information collected is used for further analysis and set expansion.\n","\n","The code starts by reading a list of words that describe personality traits from a file and printing information about the list. Then, for each word in the list, the program performs a Google search to find relevant Wikipedia pages, extracts the readable text from those pages, and writes the extracted text to a separate text file.\n","\n","This code performs web scraping by retrieving the text from Wikipedia pages relevant to words that describe personality traits. This code uses the Google search engine to find Wikipedia pages that match the given words, retrieves the HTML content of those pages, and extracts the readable text from the HTML content. The extracted text is then written to a text file."],"metadata":{"id":"ROcXGENWL-i-"}},{"cell_type":"markdown","source":["## Explore Scraped Text"],"metadata":{"id":"5Z5mV0cXL7im"}},{"cell_type":"code","source":["# Todo: Write code here\n","..."],"metadata":{"id":"cHS9nxPFL673"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Explain the code here!"],"metadata":{"id":"JHoLbwxdMh2K"}},{"cell_type":"markdown","source":["## Converted Scraped Text into Corpus"],"metadata":{"id":"VaHv7cvbMy5c"}},{"cell_type":"code","source":["def store_pickle_file(obj, filename):\n","    with open(filename, 'wb') as f:\n","        pickle.dump(obj, f)\n","\n","def load_pickle_file(filename):\n","    with open(filename, 'rb') as f:\n","        obj = pickle.load(f)\n","    return obj\n","\n","def remove_punctuation(text):\n","    \"\"\" \n","    Use a regular expression to match all \n","    characters that are not letters or numbers.\n","    \"\"\" \n","    return re.sub(r'[^\\w\\s]', '', text)\n","\n","def smart_replace(big_string_):\n","    \"\"\" Replace fullstops not used to end a sentence. \"\"\"\n","    # Type check\n","    if not isinstance(big_string_, str):\n","        raise TypeError(\"Param: 'big_string' should be a string!\")\n","    # Convert big_string to a list of chars\n","    chars = list(big_string_)\n","    # Remove fullstops not used to end sentences\n","    #   and remove newline chars\n","    new_chars = []\n","    for i, c in enumerate(chars):\n","        # Fullstop used to end sentence\n","        if c == '.' and chars[i+1] == ' ':\n","            new_chars.append(c)\n","        # Fullstop not used to end sentence\n","        elif chars[i-1] != ' ' and c == '.' and chars[i+1] != ' ':\n","            new_chars.append(',')\n","        elif c == '\\n':\n","            new_chars.append(' ')\n","        else:\n","            new_chars.append(c)\n","    new_big_string_ = ''.join(new_chars)\n","    return new_big_string_\n","\n","def extract_sentences(big_string_):\n","    \"\"\" Removing casing and seperate into sentences on fullstop chars. \"\"\"\n","    rv = []\n","    sentences_ = big_string_.split('.')\n","    for sentence in sentences_:\n","        sentence = sentence.lower()\n","        sentence = remove_punctuation(sentence)\n","        if len(sentence.split()) >= 10:\n","            rv.append(sentence.split())\n","    return rv\n","\n","def find_filenames(directory: str):\n","    filenames = []\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            filenames.append(os.path.join(root, file))\n","    return filenames\n","\n","def main():\n","    # Create list of file names\n","    filenames = find_filenames(\"wcs\")\n","    # Extract sentences from .txt files\n","    all_sentences = []\n","    for fn in filenames:\n","        words = read_words_from_file(fn, rtype='list')\n","        sentences = extract_sentences(' '.join(words))\n","        all_sentences.extend(sentences)\n","    # Create dir for corpus.pkl file\n","    if not os.path.exists(\"wcs_pkl\"):\n","        os.makedirs(\"wcs_pkl\")\n","    store_pickle_file(all_sentences, \"wcs_pkl/corpus.pkl\")\n","    # Mark EOF\n","    pass\n","\n","\n","main()"],"metadata":{"id":"ChdHXnNHM1nZ","executionInfo":{"status":"ok","timestamp":1678808573172,"user_tz":0,"elapsed":4219,"user":{"displayName":"L T","userId":"08720799606501677075"}}},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":["The aim of this code is to create a pickle file containing a list of lists, each list containing a sentence from webcrawling and scraping on wikipedia. The code uses two main functions: `main`.\n","\n","The main function uses the `find_filenames` function to search a directory named \"wcs\" and get a list of all the filenames. Then, it reads the contents of each file, removes punctuation, and splits the text into sentences. The resulting list of sentences is stored in a pickle file named \"corpus.pkl\".\n","\n","The other functions in the code are helper functions used by the main functions to process the text data. The `store_pickle_file` and `load_pickle_file` functions are used to store and load the data from the pickle files. The `remove_punctuation` function removes all characters that are not letters or numbers from a given text. The `smart_replace` function replaces full stops not used to end a sentence with a comma. The `extract_sentences` function removes casing and separates text into sentences on full stop characters."],"metadata":{"id":"gBohhaZPM4sJ"}},{"cell_type":"markdown","source":["## Explore Sentences"],"metadata":{"id":"6lgYIOK7Whbe"}},{"cell_type":"code","source":["# Todo: Write code here\n","..."],"metadata":{"id":"n2Jk1LOBWnx2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Explain the code here!"],"metadata":{"id":"UAJduAreWpPs"}},{"cell_type":"markdown","source":["## Train W2V "],"metadata":{"id":"q_E3CoQLW4d1"}},{"cell_type":"code","source":["def load_w2v(fn):\n","    print(f\"Loading Word2Vec model...\\n\")\n","    rv = None\n","    if fn.__contains__('.bin.gz'):\n","        rv = KeyedVectors.load_word2vec_format(fn, binary=True)\n","    elif fn.__contains__('.model'):\n","        rv = Word2Vec.load(fn)\n","    print(f\"Loaded Word2Vec model!\\n\")\n","    return rv\n","\n","def train_model(corpus_):\n","    # Train the word2vec model on the corpus\n","    model_ = gensim.models.Word2Vec(corpus_)  # vector_size=300\n","    return model_\n","\n","def save_model(model_, file_path):\n","    \"\"\" \n","    Save the weights of the trained model\n","\n","    :param model_: pass the model weights\n","    :param file_path: string - location and name to save to \n","    \"\"\"\n","    model_.save(file_path)\n","\n","def main():\n","    corpus = load_pickle_file(\"wcs_pkl/corpus.pkl\")\n","    trained_model = train_model(corpus)\n","    if not os.path.exists(\"model_weights\"):\n","        os.makedirs(\"model_weights\")\n","    save_model(trained_model, \"model_weights/trained_model.model\")\n","    # Mark EOF\n","    pass\n","\n","\n","main()"],"metadata":{"id":"lZMZ2LUmW621","executionInfo":{"status":"ok","timestamp":1678808611206,"user_tz":0,"elapsed":31527,"user":{"displayName":"L T","userId":"08720799606501677075"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["This code is performing three main tasks:\n","\n","1. Loading a pre-trained Word2Vec model. The function `load_w2v` takes a file name as an argument and loads a Word2Vec model stored in that file. The model is either in binary format ('.bin.gz') or in text format ('.model').\n","\n","2. Training a new Word2Vec model. The `train_model` function takes a corpus as an argument and trains a new Word2Vec model on that corpus.\n","\n","3. Saving the weights of the trained model. The `save_model` function takes a model and a file path as arguments, and saves the weights of the model in the specified file.\n","\n","The main function ties all of these tasks together by first loading the corpus from a pickle file, then training a new Word2Vec model on the corpus, and finally saving the weights of the trained model."],"metadata":{"id":"QcNSNLBIekls"}},{"cell_type":"markdown","source":["## Perform Set Expansion"],"metadata":{"id":"uB3Dg8lHeqXM"}},{"cell_type":"code","source":["def list_info(arr):\n","    \"\"\" Out information about the passed list. \"\"\"\n","    if isinstance(arr, list):\n","        print(f\"List contains {len(arr)} items.\")\n","    elif isinstance(arr, set):\n","        print(f\"Set contains {len(arr)} items.\")\n","    print(f\"First 5 items:[\")\n","    for item in list(arr)[:5]:\n","        print(f\"'{item}',\")\n","    print(f\"]\\n\")\n","    # Mark EOF\n","    pass\n","\n","def write_words_to_file(file_path, arr):\n","    # Open and write to file path passed\n","    with open(file_path, 'w', encoding='utf-8-sig') as f:\n","        string = '\\n'.join(arr)\n","        f.write(string)\n","    # Mark EOF\n","    pass\n","\n","def expand_set(model_fp='google_w2v_weights/GoogleNews-'\n","                        'vectors-negative300.bin.gz',\n","               entity_set_fp='Dissertation/M6/'\\\n","                        'data_prep/entity_set.txt',\n","               entity_set=None,\n","               out=True,\n","               output_fn='expansion_set_2'):\n","    \"\"\"\n","    This function loads a specified W2V model, reads and loads an entity set\n","    from a pickle file and convert it to a list.\n","    Then for each word in the entity set it appends the word and most similar\n","    word returned from the W2V model.\n","\n","    :param entity_set: set - the set of words which to expand on.\n","    :param model_fp: str - file path pointing to weights to be loaded.\n","    :param entity_set_fp: (str||None) - file path pointing to set to be loaded.\n","    :param out: bool - if true then the function will print info otherwise nothing\n","    will be outted to the console.\n","    :param output_fn: str - file name of the output_set_expansion to be stored and written to\n","    :return: a list of pairs where each pair contains the original word and\n","    the most similar word generated by the W2V model.\n","    \"\"\"\n","    # Load a pre-trained Word2Vec model\n","    model = load_w2v(fn=model_fp)\n","\n","    # Set state representing model type\n","    pre_trained = False\n","    if model_fp.__contains__('.model'):\n","        pre_trained = True\n","\n","    # Define the initial entity set\n","    if entity_set_fp is not None:\n","        entity_set = read_words_from_file(entity_set_fp)\n","    if out:\n","        print(f\"Entity set as a Python Set info: \")\n","        list_info(entity_set)\n","\n","    # Convert set to array\n","    entity_arr = list(entity_set)\n","    if out:\n","        print(f\"Entity set as a Python List info: \")\n","        list_info(entity_arr)\n","        \n","    # Expand the entity set\n","    expansion_arr = []\n","    e_counter = 0\n","    for word in entity_set:\n","        try:\n","            if not pre_trained:\n","                similar_words_list = model.most_similar(word.lower())\n","            else:\n","                similar_words_list = [word for word in model.wv.most_similar(word.lower())]\n","        except KeyError:\n","            if out:\n","                e_counter += 1\n","                # print(f\"'{word}' not present in W2V vocabulary.\")\n","            continue\n","        # similar_words = [word[0] for word in model.most_similar(word, topn=num_similar_words)]\n","        # expansion_arr.append(f\"{word}, {similar_words_list[0][0]}\")\n","        similar_words_added = 0\n","        for i, v in enumerate(similar_words_list):\n","            if similar_words_added == 3:\n","                continue\n","            if word.lower() != v[0].lower():\n","                expansion_arr.append(f\"{word}, {similar_words_list[i][0]}\")\n","                similar_words_added += 1\n","    # The expanded entity set\n","    if out:\n","        print(f\"Expanded list info: \")\n","        list_info(expansion_arr)\n","    write_words_to_file(f\"{output_fn}\", expansion_arr)\n","    print(\n","        f\"{e_counter} words were not present in the W2V-model's vocabulary.\\n\"\n","        f\"Written expanded set to the following dir: '{output_fn}'\"\n","    )\n","    return expansion_arr\n","\n","def main():\n","    expand_set(\n","        model_fp='model_weights/trained_model.model',\n","        output_fn='expanded_set_1.txt'\n","    )\n","    # Mark EOF\n","    pass\n","\n","\n","main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4dpCt6Vder-T","executionInfo":{"status":"ok","timestamp":1678809267789,"user_tz":0,"elapsed":1807,"user":{"displayName":"L T","userId":"08720799606501677075"}},"outputId":"b9a22833-ae85-4976-e1b9-513403cbbc66"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Word2Vec model...\n","\n","Loaded Word2Vec model!\n","\n","Entity set as a Python Set info: \n","Set contains 637 items.\n","First 5 items:[\n","'Impressive',\n","'Charmless',\n","'Cynical',\n","'Dull',\n","'Offhand',\n","]\n","\n","Entity set as a Python List info: \n","List contains 637 items.\n","First 5 items:[\n","'Impressive',\n","'Charmless',\n","'Cynical',\n","'Dull',\n","'Offhand',\n","]\n","\n","Expanded list info: \n","List contains 1530 items.\n","First 5 items:[\n","'Impressive, 181920',\n","'Impressive, adjective',\n","'Impressive, impersonal',\n","'Charmless, midfielder',\n","'Charmless, jotaro',\n","]\n","\n","127 words were not present in the W2V-model's vocabulary.\n","Written expanded set to the following dir: 'expanded_set_1.txt'\n"]}]},{"cell_type":"markdown","source":["The aim of this code is to expand a set of words (referred to as the entity set) by finding the most similar words for each word in the entity set using our trained Word2Vec model. \n","\n","The code first loads the Word2Vec model and reads in the entity set from a file. It then converts the set to a list and for each word in the entity set, it finds the most similar words using the pre-trained model. \n","\n","Finally, it writes the results to a file, which is a list of pairs where each pair contains the original word and the most similar word generated by the Word2Vec model."],"metadata":{"id":"J5ghdtj9e727"}},{"cell_type":"markdown","source":["## Expanded Set Manual Labelling \n","\n","In my GitHub repository files, which are cloned and downloaded into this project at the start of this notebook, I have manually labelled whether a word generated by W2V is a synonym, antonym, or related for the first 120 outputs. I have added a '1' to the end of the line if the word is a synonym, antonym, or related and a '0' otherwise.\n","\n","This labelling can be found in 'Dissertation/M6/output_set_expansion/labelled/eset_mit_wiki.txt'."],"metadata":{"id":"hUoJxZvGA1o8"}},{"cell_type":"code","source":["# /content/Dissertation/M6/output_set_expansion/labelled/eset_mit_wiki_.txt\n","with open('Dissertation/M6/output_set_expansion/labelled/eset_mit_wiki_.txt',\n","          \"r\") as f:\n","          lines = f.readlines()\n","\n","list_info(lines)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"927UFwmnA0ai","executionInfo":{"status":"ok","timestamp":1678812883933,"user_tz":0,"elapsed":226,"user":{"displayName":"L T","userId":"08720799606501677075"}},"outputId":"82dfb21a-9208-45b9-fde9-ccf44058dccb"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["List contains 120 items.\n","First 5 items:[\n","'ï»¿Sociable, radians, 0\n","',\n","'Sociable, modulo, 0\n","',\n","'Sociable, triplelevel, 0\n","',\n","'Energetic, dopamine, 1\n","',\n","'Energetic, legumes, 0\n","',\n","]\n","\n"]}]},{"cell_type":"markdown","source":["## Evaluation Metrics\n","#### Recall, Precision, F1-Score."],"metadata":{"id":"rPBRhPxe4Zly"}},{"cell_type":"code","source":["def load_y():\n","    # Read lines from file\n","    words = read_words_from_file(\n","        f\"Dissertation/M6/\"\n","        f\"output_set_expansion/labelled/\"\n","        f\"eset_mit_wiki_.txt\",\n","        rtype='list')\n","    y_pred_ = []\n","    # Extract y prediction value from each line\n","    for word in words:\n","        y = word.split()[-1]\n","        y_pred_.append(int(y))\n","    # Return y predictions read from file\n","    return y_pred_\n","\n","def main():\n","    y_pred = load_y()\n","    y_true = [1 for _ in range(len(y_pred))]\n","\n","    if not(len(y_pred) == len(y_true)):\n","        raise ValueError(\"Lists `y_pred` must equal `y_true`.\")\n","\n","    # Calculate precision\n","    precision = precision_score(y_true, y_pred)\n","\n","    # Calculate recall\n","    recall = recall_score(y_true, y_pred)\n","\n","    # Calculate F1 score\n","    f1 = f1_score(y_true, y_pred)\n","\n","    print(\"Recall:\", recall)\n","    print(\"Precision:\", precision)\n","    print(\"F1 Score:\", f1)\n","\n","\n","main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VAMGBztJ4j4F","executionInfo":{"status":"ok","timestamp":1678810988870,"user_tz":0,"elapsed":234,"user":{"displayName":"L T","userId":"08720799606501677075"}},"outputId":"d84abf0c-b225-488f-f846-b61624dad1e3"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Recall at k: 0.36666666666666664\n","Precision at k: 1.0\n","F1 Score at k: 0.5365853658536585\n"]}]},{"cell_type":"markdown","source":["This code evaluates the results of the expanded set of words by calculating the precision, recall, and F1 score.\n","\n","The code reads the predicted binary values from a file and stores it in the list `y_pred`. The actual binary values are stored in the list `y_true`, which is created as an array of ones with the same length as `y_pred`.\n","\n","The code then uses the `precision_score`, `recall_score`, and `f1_score` functions from the scikit-learn library to calculate precision, recall, and F1 score respectively. These metrics are commonly used to evaluate the performance of a binary classifier.\n","\n","Finally, the calculated values of precision, recall, and F1 score are printed to the console."],"metadata":{"id":"XjzIyeaz8R9q"}}]}