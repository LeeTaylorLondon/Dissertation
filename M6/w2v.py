# Author: Lee Taylor
import gensim
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from data_prep.sentences import list_info, load_pickle_file
import numpy as np
import gensim
import gzip
import shutil


def train_model(corpus_):
    """

    :param corpus_:
    :return:
    """
    # Train the word2vec model on the corpus
    model_ = gensim.models.Word2Vec(corpus_, vector_size=300)
    return model_


def save_model(model_, file_path):
    """

    :param model_:
    :param file_path:
    """
    # Save the weights of the trained model
    model_.save(file_path)


def read_words_from_file(file_path):
    """

    :param file_path:
    :return:
    """
    with open(file_path, 'r') as f:
        words = set(f.read().splitlines())
    return words


def write_words_to_file(file_path, arr):
    """

    :param file_path:
    :param arr:
    """
    # Open and write to file path passed
    with open(file_path, 'w') as f:
        string = '\n'.join(arr)
        f.write(string)
    # Mark EOF
    pass


def load_w2v(fn='google_w2v_weights/GoogleNews-vectors-negative300.bin.gz'):
    """

    :param fn:
    :return:
    """
    print(f"Loading Word2Vec model...")
    rv = None
    if fn.__contains__('.bin.gz'):
        rv = KeyedVectors.load_word2vec_format(fn, binary=True)
    elif fn.__contains__('.model'):
        rv = Word2Vec.load(fn)
    print(f"Loaded Word2Vec model!\n")
    return rv


def expand_set(model_fp='google_w2v_weights/GoogleNews-'
                        'vectors-negative300.bin.gz',
               entity_set_fp='data_prep/entity_set.txt',
               entity_set=None,
               out=True,
               output_fn='expansion_set_2'):
    """
    This function loads a specified W2V model, reads and loads an entity set
    from a pickle file and convert it to a list.
    Then for each word in the entity set it appends the word and most similar
    word returned from the W2V model.
    :param entity_set: set - the set of words which to expand on.
    :param model_fp: str - file path pointing to weights to be loaded.
    :param entity_set_fp: (str||None) - file path pointing to set to be loaded.
    :param out: bool - if true then the function will print info otherwise nothing
    will be outted to the console.
    :param output_fn: str - file name of the output to be stored and written to
    :return: a list of pairs where each pair contains the original word and
    the most similar word generated by the W2V model.
    """
    # Load a pre-trained Word2Vec model
    model = load_w2v(fn=model_fp)
    # Set state representing model type
    pre_trained = False
    if model_fp.__contains__('.model'):
        pre_trained = True
    # Define the initial entity set
    if entity_set_fp is not None:
        entity_set = read_words_from_file(entity_set_fp)
    if out:
        list_info(entity_set)
    # Convert set to array
    entity_arr = list(entity_set)
    if out:
        list_info(entity_arr)
    # Expand the entity set
    expansion_arr = []
    for word in entity_set:
        try:
            if not pre_trained:
                similar_words_list = model.most_similar(word.lower())
            else:
                similar_words_list = [word for word in model.wv.most_similar(word.lower())]
        except KeyError:
            if out:
                print(f"'{word}' not present in W2V vocabulary.")
            continue
        # similar_words = [word[0] for word in model.most_similar(word, topn=num_similar_words)]
        # expansion_arr.append(f"{word}, {similar_words_list[0][0]}")
        similar_words_added = 0
        for i, v in enumerate(similar_words_list):
            if similar_words_added == 3:
                continue
            if word.lower() != v[0].lower():
                expansion_arr.append(f"{word}, {similar_words_list[i][0]}")
                similar_words_added += 1
    # The expanded entity set
    if out:
        list_info(expansion_arr)
    write_words_to_file(f"{output_fn}.txt", expansion_arr)
    return expansion_arr


def w2v_transfer_learning(corpus_fp='data_prep/sentences_processed.pkl',
                          model_fn="google_trained_2.model"):
    """
    Loads the google pre-trained model, trains the existing
    model on our corpus.
    :param corpus_fp: string - point to the filepath of the corpus
    :param model_fn: string - specify the save location and name of
        the saved weights for the transfer-learning model
    """
    # Example corpus Todo: process my corpus to be like this
    # corpus = [["dog", "cat", "fish"],\
    #           ["car", "bus", "train"],\
    #           ["apple", "banana", "cherry"]]

    # Load sentences from .pkl file
    corpus = load_pickle_file(corpus_fp)
    list_info(corpus)
    # Create the Word2Vec model
    model = Word2Vec(vector_size=300, min_count=1)
    model.build_vocab(corpus)
    # Prevent error # Todo: this might be wrong
    model.wv.vectors_lockf = np.ones((100, 100), dtype=np.float32)
    # Use experimental function to perform TL
    model.wv.intersect_word2vec_format('google_w2v_weights/GoogleNews'
                                       '-vectors-negative300.bin.gz', binary=True)
    model.train(corpus, total_examples=len(corpus), epochs=10)
    # Save the model as pre-trained weights to load later
    save_model(model, model_fn)
    # Mark EOF
    pass


if __name__ == '__main__':
    # # Load sentences from .pkl file and reduce sentences
    # sentences_p = load_pickle_file('data_prep/sentences_processed.pkl')
    # list_info(sentences_p)  # 167.5K Sentences

    # # Train W2V and save model
    # corpus = sentences_p
    # model = train_model(corpus)
    # save_model(model, "word2vec_2.model")

    # Load google model and train it on our corpus
    # w2v_transfer_learning()

    # # Expand an entity set
    # expand_set()

    """
    def expand_set(
            model_fp='word2vec.model', 
            entity_set_fp='data_prep/entity_set.txt',
            out=True, 
            output_fn='expansion_set_2'
        ):
    """

    # expand_set(model_fp='google_trained_2.model',
    #            output_fn='expansion_set_TLM',
    #            entity_set_fp=None,
    #            entity_set={'happy', 'sad', 'optimistic'})

    # expand_set(output_fn='expansion_set_google')

    # Todo: use .pkl files in 'scrapings_sentences'  to train a W2V
    #  model then generate synonyms

    # Mark end of if-name-main-section
    pass


